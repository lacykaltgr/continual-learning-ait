{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/experiment/generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-05-10 13:48:43--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment [following]\n",
      "--2023-05-10 13:48:43--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment\n",
      "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
      "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘experiment.zip’\n",
      "\n",
      "experiment.zip          [       <=>          ]   1.58M  1.18MB/s    in 1.3s    \n",
      "\n",
      "2023-05-10 13:48:45 (1.18 MB/s) - ‘experiment.zip’ saved [1657860]\n",
      "\n",
      "Archive:  experiment.zip\n",
      "e7e6f0b439c19f52204ca7f0b7b16f0dfdb98b06\n",
      "   creating: continual-learning-ait-experiment/\n",
      "  inflating: continual-learning-ait-experiment/README.md  \n",
      "  inflating: continual-learning-ait-experiment/classifier.ipynb  \n",
      "  inflating: continual-learning-ait-experiment/classifier.py  \n",
      "  inflating: continual-learning-ait-experiment/data_preparation.ipynb  \n",
      "  inflating: continual-learning-ait-experiment/data_preparation.py  \n",
      "  inflating: continual-learning-ait-experiment/generator.ipynb  \n",
      "  inflating: continual-learning-ait-experiment/img.png  \n",
      "  inflating: continual-learning-ait-experiment/main.ipynb  \n",
      "   creating: continual-learning-ait-experiment/models/\n",
      "  inflating: continual-learning-ait-experiment/models/classifier.h5  \n",
      "  inflating: continual-learning-ait-experiment/models/encoder.h5  \n",
      "   creating: continual-learning-ait-experiment/stable_diffusion/\n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/autoencoder_kl.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/constants.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/diffusion_model.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/layers.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/stable_diffusion.py  \n",
      "  inflating: continual-learning-ait-experiment/utils.py  \n",
      "rm: cannot remove 'stable_diffusion': No such file or directory\n",
      "rm: cannot remove 'models': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "'''Download the files '''\n",
    "'''Only for colab'''\n",
    "\n",
    "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
    "!unzip experiment.zip\n",
    "!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
    "\n",
    "!rm -r stable_diffusion\n",
    "!rm -r models\n",
    "!mkdir stable_diffusion\n",
    "!mkdir models\n",
    "!mv diffusion_model.py stable_diffusion/\n",
    "!mv autoencoder_kl.py stable_diffusion/\n",
    "!mv layers.py stable_diffusion/\n",
    "!mv stable_diffusion.py stable_diffusion/\n",
    "!mv constants.py stable_diffusion/\n",
    "!mv encoder.h5 models/\n",
    "!mv classifier.h5 models/"
   ],
   "metadata": {
    "id": "FBqbqiHlwppa",
    "outputId": "7fccd050-1e1f-43d3-ea17-83af0798a7ef",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_typing\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/__init__.py:42\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:21\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03m@@UNKNOWN_CARDINALITY\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:419\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"API for using the tf.data service.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;124;03m  job of ParameterServerStrategy).\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 419\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data_service_pb2\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_server_lib\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:22\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:34\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m   arrays.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:37\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trace_type\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m doc_controls\n\u001B[0;32m---> 37\u001B[0m _np_bfloat16 \u001B[38;5;241m=\u001B[39m \u001B[43m_pywrap_bfloat16\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_bfloat16_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m _np_float8_e4m3fn \u001B[38;5;241m=\u001B[39m _pywrap_float8\u001B[38;5;241m.\u001B[39mTF_float8_e4m3fn_type()\n\u001B[1;32m     39\u001B[0m _np_float8_e5m2 \u001B[38;5;241m=\u001B[39m _pywrap_float8\u001B[38;5;241m.\u001B[39mTF_float8_e5m2_type()\n",
      "\u001B[0;31mTypeError\u001B[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D\n",
    "import math\n",
    "from stable_diffusion.constants import _ALPHAS_CUMPROD"
   ],
   "metadata": {
    "id": "qxtHOYA5wppb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_cifar_10():\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    n_classes = 10\n",
    "    X_train = (X_train / 127.5) -1\n",
    "    X_test = (X_test / 127.5) -1\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ],
   "metadata": {
    "id": "9cDXrVOHwppb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "0MPVDCr_wppc",
    "outputId": "ed07a447-b007-48f0-d15f-fab485aa3bcd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder = load_model(\"models/encoder.h5\")\n",
    "classifier = load_model(\"models/classifier.h5\")\n",
    "\n",
    "encoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_seq(x: object, layers: object) -> object:\n",
    "    for l in layers:\n",
    "        x = l(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResBlock(keras.layers.Layer):\n",
    "    def __init__(self, channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_layers = [\n",
    "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
    "            keras.activations.swish,\n",
    "            Conv2D(out_channels, 3, strides=(1, 1), padding='same'),\n",
    "        ]\n",
    "        self.emb_layers = [\n",
    "            keras.activations.swish,\n",
    "            keras.layers.Dense(out_channels),\n",
    "        ]\n",
    "        self.out_layers = [\n",
    "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
    "            keras.activations.swish,\n",
    "            Conv2D(out_channels, 3, strides=(1, 1), padding='same'),\n",
    "        ]\n",
    "        self.skip_connection = (\n",
    "            Conv2D(out_channels, 3, strides=(1, 1), padding='same') if channels != out_channels else lambda x: x\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, emb = inputs\n",
    "        h = apply_seq(x, self.in_layers)\n",
    "        emb_out = apply_seq(emb, self.emb_layers)\n",
    "        h = h + emb_out[:, None, None]\n",
    "        h = apply_seq(h, self.out_layers)\n",
    "        ret = self.skip_connection(x) + h\n",
    "        return ret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class UNetModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        print(\"UNetModel init\")\n",
    "        super().__init__()\n",
    "        self.img_height = 32\n",
    "        self.img_width = 32\n",
    "        self.ntype = tf.float32\n",
    "        self.time_embed = [\n",
    "            keras.layers.Dense(128),\n",
    "            keras.activations.swish,\n",
    "            keras.layers.Dense(128),\n",
    "        ]\n",
    "        self.input_blocks = [\n",
    "            [Conv2D(\n",
    "                32, 3, strides=(1, 1), padding='same'\n",
    "            )],\n",
    "            [ResBlock(32, 32)],\n",
    "            [ResBlock(32, 32)],\n",
    "            [Conv2D(32, 3, strides=(2, 2), padding='same')], #downsample\n",
    "            [ResBlock(32, 64)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(64, 64)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            [Conv2D(64, 3, strides=(2, 2), padding='same')], #downsample\n",
    "            [ResBlock(64, 128)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(128, 128)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            [Conv2D(128, 3, strides=(2, 2), padding='same')], #downsample\n",
    "            [ResBlock(128, 128)],\n",
    "            [ResBlock(128, 128)],\n",
    "        ]\n",
    "        self.middle_block = [\n",
    "            ResBlock(64, 64),\n",
    "            #Conv2D(1280, kernel_size=8, padding=\"same\"),\n",
    "            ResBlock(64, 64),\n",
    "        ]\n",
    "        self.output_blocks = [\n",
    "            [ResBlock(256, 128)],\n",
    "            [ResBlock(256, 128)],\n",
    "            [\n",
    "                ResBlock(256, 128),\n",
    "                keras.layers.UpSampling2D(size=(2, 2)),\n",
    "                Conv2D(128, 3, strides=(1,1), padding=1)\n",
    "            ],\n",
    "            [ResBlock(256, 128)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(256, 128)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            [\n",
    "                ResBlock(192, 128),\n",
    "                #Conv2D(1280, kernel_size=8, padding=\"same\"),\n",
    "                keras.layers.UpSampling2D(size=(2, 2)),\n",
    "                Conv2D(128, 3, strides=(1,1), padding=1)\n",
    "            ],\n",
    "            [ResBlock(192, 64)], #, Conv2D(640, kernel_size=8, padding=\"same\")],  # 6\n",
    "            [ResBlock(128, 64)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            [\n",
    "                ResBlock(96, 64),\n",
    "                #Conv2D(640, kernel_size=8, padding=\"same\"),\n",
    "                keras.layers.UpSampling2D(size=(2, 2)),\n",
    "                Conv2D(64, 3, strides=(1,1), padding=1)\n",
    "            ],\n",
    "            [ResBlock(96, 32)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(64, 32)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(64, 32)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "        ]\n",
    "        self.out = [\n",
    "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
    "            keras.activations.swish,\n",
    "            Conv2D(8, 3, strides=(1,1), padding=1),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, t_emb = inputs\n",
    "        emb = apply_seq(t_emb, self.time_embed)\n",
    "\n",
    "        def apply(x, layer):\n",
    "            return layer([x, emb]) if isinstance(layer, ResBlock) else layer(x)\n",
    "\n",
    "        saved_inputs = []\n",
    "        for b in self.input_blocks:\n",
    "            for layer in b:\n",
    "                x = apply(x, layer)\n",
    "            saved_inputs.append(x)\n",
    "\n",
    "        for layer in self.middle_block:\n",
    "            x = apply(x, layer)\n",
    "\n",
    "        for b in self.output_blocks:\n",
    "            x = tf.concat([x, saved_inputs.pop()], axis=-1)\n",
    "            for layer in b:\n",
    "                x = apply(x, layer)\n",
    "\n",
    "        return apply_seq(x, self.out)\n",
    "\n",
    "    def initialize(self, params, input_latent=None, batch_size=64):\n",
    "        timesteps = np.arange(1, params['num_steps']+ 1)\n",
    "        input_lat_noise_t = timesteps[int(len(timesteps)* params[\"input_latent_strength\"])]\n",
    "        latent, alphas, alphas_prev = self.get_starting_parameters(\n",
    "            timesteps, batch_size, input_latent=input_latent, input_lat_noise_t=input_lat_noise_t\n",
    "        )\n",
    "        timesteps = timesteps[: int(len(timesteps)*params[\"input_latent_strength\"])]\n",
    "        return latent, alphas, alphas_prev, timesteps\n",
    "\n",
    "\n",
    "    def get_x_prev(self, x, e_t, a_t, a_prev, temperature):\n",
    "        sigma_t = 0\n",
    "        sqrt_one_minus_at = math.sqrt(1 - a_t)\n",
    "        pred_x0 = x - sqrt_one_minus_at * e_t / math.sqrt(a_t)\n",
    "\n",
    "        # Direction pointing to x_t\n",
    "        dir_xt = math.sqrt(1.0 - a_prev - sigma_t**2) * e_t\n",
    "        #noise = sigma_t * tf.random.normal(x.shape, seed=seed) * temperature\n",
    "        x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt\n",
    "        return x_prev\n",
    "\n",
    "\n",
    "    def get_model_output(self, latent, timestep, batch_size):\n",
    "        timesteps = tf.convert_to_tensor([timestep], dtype=tf.float32)\n",
    "        t_emb = self.timestep_embedding(timesteps)\n",
    "        t_emb = tf.repeat(t_emb, repeats=batch_size, axis=0)\n",
    "        latent = self.call([latent, t_emb])\n",
    "        return latent\n",
    "\n",
    "\n",
    "    def timestep_embedding(self, timesteps, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = np.exp(\n",
    "            -math.log(max_period) * np.arange(0, half, dtype=\"float32\") / half\n",
    "        )\n",
    "        args = np.array(timesteps) * freqs\n",
    "        embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
    "        return tf.convert_to_tensor(embedding.reshape(1, -1), dtype=self.ntype)\n",
    "\n",
    "\n",
    "\n",
    "    # for model with input latent\n",
    "\n",
    "    def add_noise(self, x, t, noise=None):\n",
    "        if len(x.shape) == 3:\n",
    "            x = tf.expand_dims(x, axis=0)\n",
    "        batch_size, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "        if noise is None:\n",
    "            noise = tf.random.normal((batch_size, w, h, c), dtype=tf.float32)\n",
    "        sqrt_alpha_prod = tf.cast(_ALPHAS_CUMPROD[t] ** 0.5, tf.float32)\n",
    "        sqrt_one_minus_alpha_prod = (1 - _ALPHAS_CUMPROD[t]) ** 0.5\n",
    "\n",
    "        return sqrt_alpha_prod * x + sqrt_one_minus_alpha_prod * noise\n",
    "\n",
    "    def get_starting_parameters(self, timesteps, batch_size,  input_latent=None, input_lat_noise_t=None):\n",
    "        n_h = self.img_height // 8\n",
    "        n_w = self.img_width // 8\n",
    "        alphas = [_ALPHAS_CUMPROD[t] for t in timesteps]\n",
    "        alphas_prev = [1.0] + alphas[:-1]\n",
    "        if input_latent is None:\n",
    "            latent = tf.random.normal((batch_size, n_h, n_w, 8))\n",
    "        else:\n",
    "            input_latent = tf.cast(input_latent, self.ntype)\n",
    "            #latent = tf.repeat(input_latent , batch_size , axis=0)\n",
    "            latent = self.add_noise(input_latent, input_lat_noise_t)\n",
    "        return latent, alphas, alphas_prev\n"
   ],
   "metadata": {
    "id": "TY9yVQKwwppc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_one_hot_predictions(mem_pred):\n",
    "    max_indices = np.argmax(mem_pred, axis=1)  # Find the indices of the maximum probabilities along axis 1\n",
    "    num_classes = mem_pred.shape[1]  # Get the number of classes\n",
    "\n",
    "    # Create an array of zeros with the same number of rows as mem_pred and num_classes columns\n",
    "    mem_true = np.zeros_like(mem_pred)\n",
    "\n",
    "    # Set the value at the corresponding max_indices positions to 1\n",
    "    mem_true[np.arange(len(max_indices)), max_indices] = 1\n",
    "\n",
    "    return mem_true"
   ],
   "metadata": {
    "id": "OvZ3J9SG1kj0"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model at the same time'''\n",
    "\n",
    "def generate(cls=classifier, input_latent=None, train=True, coeff=1.0):\n",
    "\n",
    "    batch_size = params['batch_size'] if train else 64\n",
    "    latent, alphas, alphas_prev, timesteps = model.initialize(params, input_latent, batch_size)\n",
    "\n",
    "\n",
    "    for index, timestep in reversed(list(enumerate(timesteps))):\n",
    "        if train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                e_t = model.get_model_output(\n",
    "                    latent,\n",
    "                    timestep,\n",
    "                    batch_size,\n",
    "                )\n",
    "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "                latent = model.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
    "\n",
    "                pred = cls(latent)\n",
    "                #loss based on confidence\n",
    "                #ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
    "                loss = coeff*tf.keras.losses.categorical_crossentropy(pred, pred)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"]).apply_gradients(zip(grads, model.trainable_variables))\n",
    "        else:\n",
    "            e_t = model.get_model_output(\n",
    "                latent,\n",
    "                timestep,\n",
    "                batch_size,\n",
    "            )\n",
    "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "            latent = model.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
    "\n",
    "    return latent"
   ],
   "metadata": {
    "id": "n0nileU3wppd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef run_gen_epoch():\\n\\n\\n    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\\n        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\\n        if data.shape[0] != batch_size: break\\n        agent.state[\"sample_amt\"] += data.shape[0]\\n\\n        agent.state[\"data\"] = data\\n        agent.state[\"target\"] = target\\n        agent.state[\"i_example\"] = i\\n\\n        data = agent.state[\"data\"]\\n        latent = agent.gen.encoder(data)\\n        mem_x = None\\n\\n        for it in range(agent.params[\"gen_iters\"]):\\n            generate(agent, input_latent=latent)\\n    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\\n\\n        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\\n        loss = []\\n        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\\n\\n            mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\\n            mem_pred = agent.cls(mem_x)\\n            mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\\n            loss.append(np.mean(mem_loss))\\n\\n        print(\"Loss on generate: \",  np.mean(mem_loss))\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "'''\n",
    "def run_gen_epoch():\n",
    "\n",
    "\n",
    "    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\n",
    "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
    "        if data.shape[0] != batch_size: break\n",
    "        agent.state[\"sample_amt\"] += data.shape[0]\n",
    "\n",
    "        agent.state[\"data\"] = data\n",
    "        agent.state[\"target\"] = target\n",
    "        agent.state[\"i_example\"] = i\n",
    "\n",
    "        data = agent.state[\"data\"]\n",
    "        latent = agent.gen.encoder(data)\n",
    "        mem_x = None\n",
    "\n",
    "        for it in range(agent.params[\"gen_iters\"]):\n",
    "            generate(agent, input_latent=latent)\n",
    "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "\n",
    "        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\n",
    "        loss = []\n",
    "        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\n",
    "\n",
    "            mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\n",
    "            mem_pred = agent.cls(mem_x)\n",
    "            mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
    "            loss.append(np.mean(mem_loss))\n",
    "\n",
    "        print(\"Loss on generate: \",  np.mean(mem_loss))\n",
    "'''"
   ],
   "metadata": {
    "id": "1jelThWIwppd",
    "outputId": "eb4d266c-deaf-4260-8e1e-803ac04f1313",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNetModel init\n"
     ]
    }
   ],
   "source": [
    "model = UNetModel()"
   ],
   "metadata": {
    "id": "8IXX3Srqwppe",
    "outputId": "63f71ea3-f522-407f-ae92-9763c71c4443",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_steps\": 2,\n",
    "    \"input_latent_strength\":0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"batch_size\": 256,\n",
    "    \"gen_lr\": 2e-5,\n",
    "    \"n_epoch\": 3,\n",
    "}"
   ],
   "metadata": {
    "id": "xI76KdNbwppe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_cifar_10()"
   ],
   "metadata": {
    "id": "hAdxt2bYwppe",
    "outputId": "0dbf7e81-6290-456b-e8ef-1a525320d10b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-279800e0dba5>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0my_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"batch_size\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mlatent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mmem_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_latent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlatent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0mmem_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmem_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mmem_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcategorical_crossentropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmem_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmem_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-15-d0392c4c2fbf>\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(cls, input_latent, train, coeff)\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m                 e_t = model.get_model_output(\n\u001B[0m\u001B[1;32m     13\u001B[0m                     \u001B[0mlatent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m                     \u001B[0mtimestep\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-f53390578d52>\u001B[0m in \u001B[0;36mget_model_output\u001B[0;34m(self, latent, timestep, batch_size)\u001B[0m\n\u001B[1;32m    107\u001B[0m         \u001B[0mt_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimestep_embedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimesteps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0mt_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt_emb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrepeats\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m         \u001B[0mlatent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlatent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt_emb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlatent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-f53390578d52>\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmiddle_block\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mb\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_blocks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-f53390578d52>\u001B[0m in \u001B[0;36mapply\u001B[0;34m(x, layer)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mResBlock\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0msaved_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/stable_diffusion/diffusion_model.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0memb_out\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mskip_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Exception encountered when calling layer 'res_block_76' (type ResBlock).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer 'res_block_76' (type ResBlock):\n  • inputs=['tf.Tensor(shape=(256, 1, 1, 128), dtype=float32)', 'tf.Tensor(shape=(256, 128), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "for epoch in range(params[\"n_epoch\"]):\n",
    "    loss = []\n",
    "    for i in range(0, X_train.shape[0], params[\"batch_size\"]):\n",
    "        X_batch = X_train[i:i+params[\"batch_size\"]]\n",
    "        y_batch = y_train[i:i+params[\"batch_size\"]]\n",
    "        latent = encoder(X_batch)\n",
    "        mem_x = generate(input_latent=latent, train=True)\n",
    "        mem_pred = classifier(mem_x)\n",
    "        mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
    "        loss.append(np.mean(mem_loss))\n",
    "    print(\"Loss on generate: \",  np.mean(loss))"
   ],
   "metadata": {
    "id": "-o1U9ShTwppe",
    "outputId": "ad1aaa39-fb04-4757-e4fe-2fdb5d8391e2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "RXsGiESrBsK-"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
