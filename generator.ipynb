{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/experiment/generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 19:38:14--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment [following]\n",
            "--2023-05-10 19:38:14--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘experiment.zip’\n",
            "\n",
            "experiment.zip          [ <=>                ]   1.58M  8.79MB/s    in 0.2s    \n",
            "\n",
            "2023-05-10 19:38:14 (8.79 MB/s) - ‘experiment.zip’ saved [1658748]\n",
            "\n",
            "Archive:  experiment.zip\n",
            "4109c9f82042ac07feaff0e04a069c5d0a3dffbe\n",
            "   creating: continual-learning-ait-experiment/\n",
            "  inflating: continual-learning-ait-experiment/README.md  \n",
            "  inflating: continual-learning-ait-experiment/classifier.ipynb  \n",
            "  inflating: continual-learning-ait-experiment/classifier.py  \n",
            "  inflating: continual-learning-ait-experiment/data_preparation.ipynb  \n",
            "  inflating: continual-learning-ait-experiment/data_preparation.py  \n",
            "  inflating: continual-learning-ait-experiment/generator.ipynb  \n",
            "  inflating: continual-learning-ait-experiment/img.png  \n",
            "  inflating: continual-learning-ait-experiment/main.ipynb  \n",
            "   creating: continual-learning-ait-experiment/models/\n",
            "  inflating: continual-learning-ait-experiment/models/classifier.h5  \n",
            "  inflating: continual-learning-ait-experiment/models/encoder.h5  \n",
            "   creating: continual-learning-ait-experiment/stable_diffusion/\n",
            "  inflating: continual-learning-ait-experiment/stable_diffusion/autoencoder_kl.py  \n",
            "  inflating: continual-learning-ait-experiment/stable_diffusion/constants.py  \n",
            "  inflating: continual-learning-ait-experiment/stable_diffusion/diffusion_model.py  \n",
            "  inflating: continual-learning-ait-experiment/stable_diffusion/layers.py  \n",
            "  inflating: continual-learning-ait-experiment/stable_diffusion/stable_diffusion.py  \n",
            "  inflating: continual-learning-ait-experiment/utils.py  \n",
            "rm: cannot remove 'stable_diffusion': No such file or directory\n",
            "rm: cannot remove 'models': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "'''Download the files '''\n",
        "'''Only for colab'''\n",
        "\n",
        "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
        "!unzip experiment.zip\n",
        "!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
        "\n",
        "!rm -r stable_diffusion\n",
        "!rm -r models\n",
        "!mkdir stable_diffusion\n",
        "!mkdir models\n",
        "!mv diffusion_model.py stable_diffusion/\n",
        "!mv autoencoder_kl.py stable_diffusion/\n",
        "!mv layers.py stable_diffusion/\n",
        "!mv stable_diffusion.py stable_diffusion/\n",
        "!mv constants.py stable_diffusion/\n",
        "!mv encoder.h5 models/\n",
        "!mv classifier.h5 models/"
      ],
      "metadata": {
        "id": "FBqbqiHlwppa",
        "outputId": "b8857720-6fcb-45e7-d955-f5843abd19b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Conv2DTranspose\n",
        "import math\n",
        "from stable_diffusion.constants import _ALPHAS_CUMPROD"
      ],
      "metadata": {
        "id": "qxtHOYA5wppb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "def load_cifar_10():\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    n_classes = 10\n",
        "    X_train = (X_train / 127.5) -1\n",
        "    X_test = (X_test / 127.5) -1\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "metadata": {
        "id": "9cDXrVOHwppb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "0MPVDCr_wppc",
        "outputId": "4b181d9d-4b57-48de-9dba-4308b864167b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "encoder = load_model(\"models/encoder.h5\")\n",
        "classifier = load_model(\"models/classifier.h5\")\n",
        "\n",
        "encoder.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=5e-3),\n",
        "    loss=keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=5e-3),\n",
        "    loss=keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def apply_seq(x: object, layers: object) -> object:\n",
        "    for l in layers:\n",
        "        x = l(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "kBhHGv9jK6bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "class ResBlock(keras.layers.Layer):\n",
        "    def __init__(self, channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_layers = [\n",
        "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            Conv2D(out_channels, 3, strides=(1, 1), padding='same'),\n",
        "        ]\n",
        "        self.emb_layers = [\n",
        "            keras.activations.swish,\n",
        "            keras.layers.Dense(out_channels),\n",
        "        ]\n",
        "        self.out_layers = [\n",
        "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            Conv2D(out_channels, 3, strides=(1, 1), padding='same'),\n",
        "        ]\n",
        "        self.skip_connection = (\n",
        "            Conv2D(out_channels, 3, strides=(1, 1), padding='same') if channels != out_channels else lambda x: x\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, emb = inputs\n",
        "        h = apply_seq(x, self.in_layers)\n",
        "        emb_out = apply_seq(emb, self.emb_layers)\n",
        "        h = h + emb_out[:, None, None]\n",
        "        h = apply_seq(h, self.out_layers)\n",
        "        skip_x = self.skip_connection(x)\n",
        "        ret = skip_x + h\n",
        "        return ret"
      ],
      "metadata": {
        "id": "4Z_JKL-kK6bc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "outputs": [],
      "source": [
        "class UNetModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        print(\"UNetModel init\")\n",
        "        super().__init__()\n",
        "        self.img_height = 32\n",
        "        self.img_width = 32\n",
        "        self.ntype = tf.float32\n",
        "        self.time_embed = [\n",
        "            keras.layers.Dense(128),\n",
        "            keras.activations.swish,\n",
        "            keras.layers.Dense(128),\n",
        "        ]\n",
        "        self.input_blocks = [\n",
        "            [Conv2D( 32, 3, strides=(1, 1), padding='same')],\n",
        "\n",
        "            [ResBlock(32, 32)],\n",
        "            [ResBlock(32, 32)],\n",
        "            [Conv2D(64, 3, strides=(2, 2), padding='same'),], #downsample\n",
        "\n",
        "            [ResBlock(32, 64)], \n",
        "            [ResBlock(64, 64)], \n",
        "            [Conv2D(128, 3, strides=(2, 2), padding='same'),], #downsample\n",
        "\n",
        "            [ResBlock(64, 128)],\n",
        "            [ResBlock(128, 128)],\n",
        "            [Conv2D(128, 3, strides=(2, 2), padding='same')], #downsample\n",
        "\n",
        "            [ResBlock(128, 128)],\n",
        "            [ResBlock(128, 128)],\n",
        "        ]\n",
        "        self.middle_block = [\n",
        "            ResBlock(128, 128),\n",
        "            ResBlock(128, 128),\n",
        "        ]\n",
        "        self.output_blocks = [\n",
        "            [ResBlock(256, 128)],\n",
        "            [ResBlock(256, 128)],\n",
        "\n",
        "            [\n",
        "                ResBlock(256, 128),\n",
        "                Conv2DTranspose(128, 2, strides=(2,2)),\n",
        "                Conv2D(128, 3, strides=(1,1), padding='same')\n",
        "            ],\n",
        "            [ResBlock(256, 128)], \n",
        "            [ResBlock(256, 128)],\n",
        "\n",
        "            [\n",
        "                ResBlock(192, 128),\n",
        "                Conv2DTranspose(128, 2, strides=(2,2)),\n",
        "                Conv2D(64, 2, strides=(1,1), padding='valid')\n",
        "            ],\n",
        "            [ResBlock(192, 64)], \n",
        "            [ResBlock(128, 64)], \n",
        "\n",
        "            [\n",
        "                ResBlock(96, 64),\n",
        "                Conv2DTranspose(64, 3, strides=(2,2)),\n",
        "                Conv2D(64, 2, strides=(1,1), padding='valid')\n",
        "            ],\n",
        "            [ResBlock(96, 32)], \n",
        "            [ResBlock(64, 32)],\n",
        "\n",
        "            [ResBlock(64, 32)],\n",
        "        ]\n",
        "        self.out = [\n",
        "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            Conv2D(8, 3, strides=(1,1), padding='same'),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, t_emb = inputs\n",
        "        emb = apply_seq(t_emb, self.time_embed)\n",
        "\n",
        "        def apply(x, layer):\n",
        "            return layer([x, emb]) if isinstance(layer, ResBlock) else layer(x)\n",
        "\n",
        "        saved_inputs = []\n",
        "        for b in self.input_blocks:\n",
        "            for layer in b:\n",
        "                x = apply(x, layer)\n",
        "            saved_inputs.append(x)\n",
        "\n",
        "        for layer in self.middle_block:\n",
        "            x = apply(x, layer)\n",
        "\n",
        "        for b in self.output_blocks:\n",
        "            skip = saved_inputs.pop()\n",
        "            x = tf.concat([x, skip], axis=-1)\n",
        "            for layer in b:\n",
        "                x = apply(x, layer)\n",
        "\n",
        "        return apply_seq(x, self.out)\n",
        "\n",
        "    def initialize(self, params, input_latent=None, batch_size=64):\n",
        "        timesteps = np.arange(1, params['num_steps']+ 1)\n",
        "        input_lat_noise_t = timesteps[int(len(timesteps)* params[\"input_latent_strength\"])]\n",
        "        latent, alphas, alphas_prev = self.get_starting_parameters(\n",
        "            timesteps, batch_size, input_latent=input_latent, input_lat_noise_t=input_lat_noise_t\n",
        "        )\n",
        "        timesteps = timesteps[: int(len(timesteps)*params[\"input_latent_strength\"])]\n",
        "        return latent, alphas, alphas_prev, timesteps\n",
        "\n",
        "\n",
        "    def get_x_prev(self, x, e_t, a_t, a_prev, temperature):\n",
        "        sigma_t = 0\n",
        "        sqrt_one_minus_at = math.sqrt(1 - a_t)\n",
        "        pred_x0 = x - sqrt_one_minus_at * e_t / math.sqrt(a_t)\n",
        "\n",
        "        # Direction pointing to x_t\n",
        "        dir_xt = math.sqrt(1.0 - a_prev - sigma_t**2) * e_t\n",
        "        #noise = sigma_t * tf.random.normal(x.shape, seed=seed) * temperature\n",
        "        x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt\n",
        "        return x_prev\n",
        "\n",
        "\n",
        "    def get_model_output(self, latent, timestep, batch_size):\n",
        "        timesteps = tf.convert_to_tensor([timestep], dtype=tf.float32)\n",
        "        t_emb = self.timestep_embedding(timesteps)\n",
        "        t_emb = tf.repeat(t_emb, repeats=batch_size, axis=0)\n",
        "        latent = self.call([latent, t_emb])\n",
        "        return latent\n",
        "\n",
        "\n",
        "    def timestep_embedding(self, timesteps, dim=320, max_period=10000):\n",
        "        half = dim // 2\n",
        "        freqs = np.exp(\n",
        "            -math.log(max_period) * np.arange(0, half, dtype=\"float32\") / half\n",
        "        )\n",
        "        args = np.array(timesteps) * freqs\n",
        "        embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
        "        return tf.convert_to_tensor(embedding.reshape(1, -1), dtype=self.ntype)\n",
        "\n",
        "\n",
        "\n",
        "    # for model with input latent\n",
        "\n",
        "    def add_noise(self, x, t, noise=None):\n",
        "        if len(x.shape) == 3:\n",
        "            x = tf.expand_dims(x, axis=0)\n",
        "        batch_size, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
        "        if noise is None:\n",
        "            noise = tf.random.normal((batch_size, w, h, c), dtype=tf.float32)\n",
        "        sqrt_alpha_prod = tf.cast(_ALPHAS_CUMPROD[t] ** 0.5, tf.float32)\n",
        "        sqrt_one_minus_alpha_prod = (1 - _ALPHAS_CUMPROD[t]) ** 0.5\n",
        "\n",
        "        return sqrt_alpha_prod * x + sqrt_one_minus_alpha_prod * noise\n",
        "\n",
        "    def get_starting_parameters(self, timesteps, batch_size,  input_latent=None, input_lat_noise_t=None):\n",
        "        n_h = self.img_height // 8\n",
        "        n_w = self.img_width // 8\n",
        "        alphas = [_ALPHAS_CUMPROD[t] for t in timesteps]\n",
        "        alphas_prev = [1.0] + alphas[:-1]\n",
        "        if input_latent is None:\n",
        "            latent = tf.random.normal((batch_size, n_h, n_w, 8))\n",
        "        else:\n",
        "            input_latent = tf.cast(input_latent, self.ntype)\n",
        "            #latent = tf.repeat(input_latent , batch_size , axis=0)\n",
        "            latent = self.add_noise(input_latent, input_lat_noise_t)\n",
        "        return latent, alphas, alphas_prev\n"
      ],
      "metadata": {
        "id": "TY9yVQKwwppc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_predictions(mem_pred):\n",
        "    max_indices = np.argmax(mem_pred, axis=1)  # Find the indices of the maximum probabilities along axis 1\n",
        "    num_classes = mem_pred.shape[1]  # Get the number of classes\n",
        "\n",
        "    # Create an array of zeros with the same number of rows as mem_pred and num_classes columns\n",
        "    mem_true = np.zeros_like(mem_pred)\n",
        "\n",
        "    # Set the value at the corresponding max_indices positions to 1\n",
        "    mem_true[np.arange(len(max_indices)), max_indices] = 1\n",
        "\n",
        "    return mem_true"
      ],
      "metadata": {
        "id": "OvZ3J9SG1kj0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "'''Generate samples and train the diffusion model at the same time'''\n",
        "\n",
        "def generate(cls=classifier, input_latent=None, train=True, coeff=1.0):\n",
        "\n",
        "    batch_size = params['batch_size'] if train else 64\n",
        "    latent, alphas, alphas_prev, timesteps = model.initialize(params, input_latent, batch_size)\n",
        "\n",
        "\n",
        "    for index, timestep in reversed(list(enumerate(timesteps))):\n",
        "        if train:\n",
        "            with tf.GradientTape() as tape:\n",
        "                e_t = model.get_model_output(\n",
        "                    latent,\n",
        "                    timestep,\n",
        "                    batch_size,\n",
        "                )\n",
        "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "                latent = model.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
        "\n",
        "                pred = cls(latent)\n",
        "                #loss based on confidence\n",
        "                #ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
        "                loss = coeff*tf.keras.losses.categorical_crossentropy(pred, pred)\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"]).apply_gradients(zip(grads, model.trainable_variables))\n",
        "        else:\n",
        "            e_t = model.get_model_output(\n",
        "                latent,\n",
        "                timestep,\n",
        "                batch_size,\n",
        "            )\n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            latent = model.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
        "\n",
        "    return latent"
      ],
      "metadata": {
        "id": "n0nileU3wppd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef run_gen_epoch():\\n\\n\\n    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\\n        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\\n        if data.shape[0] != batch_size: break\\n        agent.state[\"sample_amt\"] += data.shape[0]\\n\\n        agent.state[\"data\"] = data\\n        agent.state[\"target\"] = target\\n        agent.state[\"i_example\"] = i\\n\\n        data = agent.state[\"data\"]\\n        latent = agent.gen.encoder(data)\\n        mem_x = None\\n\\n        for it in range(agent.params[\"gen_iters\"]):\\n            generate(agent, input_latent=latent)\\n    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\\n\\n        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\\n        loss = []\\n        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\\n\\n            mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\\n            mem_pred = agent.cls(mem_x)\\n            mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\\n            loss.append(np.mean(mem_loss))\\n\\n        print(\"Loss on generate: \",  np.mean(mem_loss))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "'''\n",
        "def run_gen_epoch():\n",
        "\n",
        "\n",
        "    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\n",
        "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
        "        if data.shape[0] != batch_size: break\n",
        "        agent.state[\"sample_amt\"] += data.shape[0]\n",
        "\n",
        "        agent.state[\"data\"] = data\n",
        "        agent.state[\"target\"] = target\n",
        "        agent.state[\"i_example\"] = i\n",
        "\n",
        "        data = agent.state[\"data\"]\n",
        "        latent = agent.gen.encoder(data)\n",
        "        mem_x = None\n",
        "\n",
        "        for it in range(agent.params[\"gen_iters\"]):\n",
        "            generate(agent, input_latent=latent)\n",
        "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
        "\n",
        "        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\n",
        "        loss = []\n",
        "        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\n",
        "\n",
        "            mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\n",
        "            mem_pred = agent.cls(mem_x)\n",
        "            mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
        "            loss.append(np.mean(mem_loss))\n",
        "\n",
        "        print(\"Loss on generate: \",  np.mean(mem_loss))\n",
        "'''"
      ],
      "metadata": {
        "id": "1jelThWIwppd",
        "outputId": "a4e50bce-4b53-457a-f4e8-1f0b231c3fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNetModel init\n"
          ]
        }
      ],
      "source": [
        "model = UNetModel()"
      ],
      "metadata": {
        "id": "8IXX3Srqwppe",
        "outputId": "01027298-2a54-4b67-bbd1-9869268bbb5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"num_steps\": 4,\n",
        "    \"input_latent_strength\":0.8,\n",
        "    \"temperature\": 0.9,\n",
        "    \"batch_size\": 256,\n",
        "    \"gen_lr\": 2e-5,\n",
        "    \"n_epoch\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "xI76KdNbwppe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_cifar_10()"
      ],
      "metadata": {
        "id": "hAdxt2bYwppe",
        "outputId": "8726f289-f7d3-44b7-8535-e17a8da40565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102\n",
            "0.044593334\n",
            "87\n",
            "0.06451675\n",
            "77\n",
            "0.060081493\n",
            "81\n",
            "0.050755266\n",
            "93\n",
            "0.04241792\n",
            "71\n",
            "0.04478211\n",
            "88\n",
            "0.046797242\n",
            "80\n",
            "0.052080985\n",
            "84\n",
            "0.047288146\n",
            "95\n",
            "0.038073584\n",
            "97\n",
            "0.041063264\n",
            "94\n",
            "0.051144887\n",
            "86\n",
            "0.047615964\n",
            "86\n",
            "0.045544498\n",
            "91\n",
            "0.036473818\n",
            "77\n",
            "0.052576117\n",
            "83\n",
            "0.051702604\n",
            "86\n",
            "0.053602077\n",
            "82\n",
            "0.04250906\n",
            "90\n",
            "0.061209902\n",
            "82\n",
            "0.05542068\n",
            "84\n",
            "0.048580483\n",
            "87\n",
            "0.07233008\n",
            "92\n",
            "0.050964113\n",
            "84\n",
            "0.032917783\n",
            "92\n",
            "0.0520269\n",
            "102\n",
            "0.05595901\n",
            "99\n",
            "0.054773174\n",
            "91\n",
            "0.05716739\n",
            "86\n",
            "0.047338016\n",
            "90\n",
            "0.036495984\n",
            "91\n",
            "0.039627954\n",
            "94\n",
            "0.049440607\n",
            "100\n",
            "0.062190853\n",
            "85\n",
            "0.061088603\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-7be7080bffeb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmem_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_latent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmem_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmem_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d0392c4c2fbf>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(cls, input_latent, train, coeff)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m#ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gen_lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1383\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m     gy = array_ops.reshape(\n\u001b[0;32m-> 1385\u001b[0;31m         math_ops.reduce_sum(gen_math_ops.mul(x, grad), ry), sy)\n\u001b[0m\u001b[1;32m   1386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2362\u001b[0m   \"\"\"\n\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2365\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2374\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2375\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11406\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11408\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  11409\u001b[0m         _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[1;32m  11410\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(params[\"n_epoch\"]):\n",
        "    loss = []\n",
        "    for i in range(0, X_train.shape[0], params[\"batch_size\"]):\n",
        "        X_batch = X_train[i:i+params[\"batch_size\"]]\n",
        "        y_batch = y_train[i:i+params[\"batch_size\"]]\n",
        "        latent = encoder(X_batch)\n",
        "        mem_x = generate(input_latent=latent, train=True)\n",
        "        mem_pred = classifier(mem_x)\n",
        "        print(len(y_batch[y_batch == mem_pred]))\n",
        "        mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
        "        loss.append(np.mean(mem_loss))\n",
        "        print(np.mean(mem_loss))\n",
        "    print(\"Loss on generate: \",  np.mean(loss))"
      ],
      "metadata": {
        "id": "-o1U9ShTwppe",
        "outputId": "1f60302d-29e5-4521-e91c-788fa24f0087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXsGiESrBsK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}