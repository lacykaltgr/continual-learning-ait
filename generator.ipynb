{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Download the files '''\n",
    "'''Only for colab'''\n",
    "\n",
    "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
    "!unzip experiment.zip\n",
    "!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
    "\n",
    "!rm -r stable_diffusion\n",
    "!rm -r models\n",
    "!mkdir stable_diffusion\n",
    "!mkdir models\n",
    "!mv diffusion_model.py stable_diffusion/\n",
    "!mv autoencoder_kl.py stable_diffusion/\n",
    "!mv layers.py stable_diffusion/\n",
    "!mv stable_diffusion.py stable_diffusion/\n",
    "!mv constants.py stable_diffusion/\n",
    "!mv encoder.h5 models/\n",
    "!mv classifier.h5 models/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;31mRuntimeError\u001B[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;31mRuntimeError\u001B[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;31mRuntimeError\u001B[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;31mRuntimeError\u001B[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/keras/__init__.py:21\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mDetailed documentation and user guides are available at\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m[keras.io](https://keras.io).\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/keras/models/__init__.py:18\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Functional\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/keras/engine/functional.py:24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdtensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layout_map \u001B[38;5;28;01mas\u001B[39;00m layout_map_lib\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_typing\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/__init__.py:42\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:21\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03m@@UNKNOWN_CARDINALITY\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:419\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"API for using the tf.data service.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;124;03m  job of ParameterServerStrategy).\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 419\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data_service_pb2\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_server_lib\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:22\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:34\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m   arrays.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:37\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trace_type\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m doc_controls\n\u001B[0;32m---> 37\u001B[0m _np_bfloat16 \u001B[38;5;241m=\u001B[39m \u001B[43m_pywrap_bfloat16\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_bfloat16_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m _np_float8_e4m3fn \u001B[38;5;241m=\u001B[39m _pywrap_float8\u001B[38;5;241m.\u001B[39mTF_float8_e4m3fn_type()\n\u001B[1;32m     39\u001B[0m _np_float8_e5m2 \u001B[38;5;241m=\u001B[39m _pywrap_float8\u001B[38;5;241m.\u001B[39mTF_float8_e5m2_type()\n",
      "\u001B[0;31mTypeError\u001B[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from stable_diffusion.layers import PaddedConv2D, apply_seq\n",
    "from stable_diffusion.diffusion_model import ResBlock, Downsample, Upsample\n",
    "import math\n",
    "from stable_diffusion.constants import _ALPHAS_CUMPROD, PYTORCH_CKPT_MAPPING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_cifar_10():\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    n_classes = 10\n",
    "    X_train = (X_train / 127.5) -1\n",
    "    X_test = (X_test / 127.5) -1\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = load_model(\"models/encoder.h5\")\n",
    "classifier = load_model(\"models/classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UNetModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        print(\"UNetModel init\")\n",
    "        super().__init__()\n",
    "        self.time_embed = [\n",
    "            keras.layers.Dense(1280),\n",
    "            keras.activations.swish,\n",
    "            keras.layers.Dense(1280),\n",
    "        ]\n",
    "        self.input_blocks = [\n",
    "            [PaddedConv2D(320, kernel_size=3, padding=1)],\n",
    "            [ResBlock(320, 320)],\n",
    "            [ResBlock(320, 320)],\n",
    "            [Downsample(320)],\n",
    "            [ResBlock(320, 640)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(640, 640)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            #[Downsample(640)],\n",
    "            #[ResBlock(640, 1280)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            #[ResBlock(1280, 1280)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            #[Downsample(1280)],\n",
    "            #[ResBlock(1280, 1280)],\n",
    "            #[ResBlock(1280, 1280)],\n",
    "        ]\n",
    "        self.middle_block = [\n",
    "            ResBlock(640, 640),\n",
    "            #Conv2D(1280, kernel_size=8, padding=\"same\"),\n",
    "            ResBlock(640, 640),\n",
    "        ]\n",
    "        self.output_blocks = [\n",
    "            #[ResBlock(2560, 1280)],\n",
    "            #[ResBlock(2560, 1280)],\n",
    "            #[ResBlock(2560, 1280), Upsample(1280)],\n",
    "            #[ResBlock(2560, 1280)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            #[ResBlock(2560, 1280)], #, Conv2D(1280, kernel_size=8, padding=\"same\")],\n",
    "            #[\n",
    "            #    ResBlock(1920, 1280),\n",
    "            #    #Conv2D(1280, kernel_size=8, padding=\"same\"),\n",
    "            #    Upsample(1280),\n",
    "            #],\n",
    "            [ResBlock(1920, 640)], #, Conv2D(640, kernel_size=8, padding=\"same\")],  # 6\n",
    "            [ResBlock(1280, 640)], #, Conv2D(640, kernel_size=8, padding=\"same\")],\n",
    "            [\n",
    "                ResBlock(960, 640),\n",
    "                #Conv2D(640, kernel_size=8, padding=\"same\"),\n",
    "                Upsample(640),\n",
    "            ],\n",
    "            [ResBlock(960, 320)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(640, 320)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "            [ResBlock(640, 320)], #, Conv2D(320, kernel_size=8, padding=\"same\")],\n",
    "        ]\n",
    "        self.out = [\n",
    "            keras.layers.GroupNormalization(epsilon=1e-5),\n",
    "            keras.activations.swish,\n",
    "            PaddedConv2D(4, kernel_size=3, padding=1),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, t_emb = inputs\n",
    "        emb = apply_seq(t_emb, self.time_embed)\n",
    "\n",
    "        def apply(x, layer):\n",
    "            return layer([x, emb]) if isinstance(layer, ResBlock) else layer(x)\n",
    "\n",
    "        saved_inputs = []\n",
    "        for b in self.input_blocks:\n",
    "            for layer in b:\n",
    "                x = apply(x, layer)\n",
    "            saved_inputs.append(x)\n",
    "\n",
    "        for layer in self.middle_block:\n",
    "            x = apply(x, layer)\n",
    "\n",
    "        for b in self.output_blocks:\n",
    "            x = tf.concat([x, saved_inputs.pop()], axis=-1)\n",
    "            for layer in b:\n",
    "                x = apply(x, layer)\n",
    "\n",
    "        return apply_seq(x, self.out)\n",
    "\n",
    "    def initialize(self, params, input_latent=None, batch_size=64):\n",
    "        timesteps = np.arange(1, params['num_steps']+ 1)\n",
    "        input_lat_noise_t = timesteps[int(len(timesteps)* params[\"input_latent_strength\"])]\n",
    "        latent, alphas, alphas_prev = self.get_starting_parameters(\n",
    "            timesteps, batch_size, input_latent=input_latent, input_lat_noise_t=input_lat_noise_t\n",
    "        )\n",
    "        timesteps = timesteps[: int(len(timesteps)*params[\"input_latent_strength\"])]\n",
    "        return latent, alphas, alphas_prev, timesteps\n",
    "\n",
    "\n",
    "    def get_x_prev(self, x, e_t, a_t, a_prev, temperature):\n",
    "        sigma_t = 0\n",
    "        sqrt_one_minus_at = math.sqrt(1 - a_t)\n",
    "        pred_x0 = x - sqrt_one_minus_at * e_t / math.sqrt(a_t)\n",
    "\n",
    "        # Direction pointing to x_t\n",
    "        dir_xt = math.sqrt(1.0 - a_prev - sigma_t**2) * e_t\n",
    "        #noise = sigma_t * tf.random.normal(x.shape, seed=seed) * temperature\n",
    "        x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt\n",
    "        return x_prev\n",
    "\n",
    "\n",
    "    def get_model_output(self, latent, timestep, batch_size):\n",
    "        timesteps = tf.convert_to_tensor([timestep], dtype=tf.float32)\n",
    "        t_emb = self.timestep_embedding(timesteps)\n",
    "        t_emb = tf.repeat(t_emb, repeats=batch_size, axis=0)\n",
    "        latent = self.call([latent, t_emb])\n",
    "        return latent\n",
    "\n",
    "\n",
    "    def timestep_embedding(self, timesteps, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = np.exp(\n",
    "            -math.log(max_period) * np.arange(0, half, dtype=\"float32\") / half\n",
    "        )\n",
    "        args = np.array(timesteps) * freqs\n",
    "        embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
    "        return tf.convert_to_tensor(embedding.reshape(1, -1), dtype=self.dtype)\n",
    "\n",
    "\n",
    "\n",
    "    # for model with input latent\n",
    "\n",
    "    def add_noise(self, x, t, noise=None):\n",
    "        if len(x.shape) == 3:\n",
    "            x = tf.expand_dims(x, axis=0)\n",
    "        batch_size, w, h, c = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "        if noise is None:\n",
    "            noise = tf.random.normal((batch_size, w, h, c), dtype=tf.float32)\n",
    "        sqrt_alpha_prod = tf.cast(_ALPHAS_CUMPROD[t] ** 0.5, tf.float32)\n",
    "        sqrt_one_minus_alpha_prod = (1 - _ALPHAS_CUMPROD[t]) ** 0.5\n",
    "\n",
    "        return sqrt_alpha_prod * x + sqrt_one_minus_alpha_prod * noise\n",
    "\n",
    "    def get_starting_parameters(self, timesteps, batch_size,  input_latent=None, input_lat_noise_t=None):\n",
    "        n_h = self.img_height // 8\n",
    "        n_w = self.img_width // 8\n",
    "        alphas = [_ALPHAS_CUMPROD[t] for t in timesteps]\n",
    "        alphas_prev = [1.0] + alphas[:-1]\n",
    "        if input_latent is None:\n",
    "            latent = tf.random.normal((batch_size, n_h, n_w, 4))\n",
    "        else:\n",
    "            input_latent = tf.cast(input_latent, self.dtype)\n",
    "            #latent = tf.repeat(input_latent , batch_size , axis=0)\n",
    "            latent = self.add_noise(input_latent, input_lat_noise_t)\n",
    "        return latent, alphas, alphas_prev\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model at the same time'''\n",
    "\n",
    "def generate(cls=classifier, input_latent=None, train=True, coeff=1.0):\n",
    "\n",
    "    batch_size = params['batch_size'] if train else 64\n",
    "    latent, alphas, alphas_prev, timesteps = model.initialize(params, input_latent, batch_size)\n",
    "\n",
    "\n",
    "    for index, timestep in reversed(list(enumerate(timesteps))):\n",
    "        if train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                e_t = model.get_model_output(\n",
    "                    latent,\n",
    "                    timestep,\n",
    "                    batch_size,\n",
    "                )\n",
    "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "                latent = model.gen.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
    "\n",
    "                pred = cls(latent)\n",
    "                #loss based on confidence\n",
    "                #ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
    "                loss = coeff*tf.keras.losses.categorical_crossentropy(pred, pred)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"]).apply_gradients(zip(grads, model.trainable_variables))\n",
    "        else:\n",
    "            e_t = model.get_model_output(\n",
    "                latent,\n",
    "                timestep,\n",
    "                batch_size,\n",
    "            )\n",
    "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "            latent = model.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
    "\n",
    "    return latent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "def run_gen_epoch():\n",
    "\n",
    "\n",
    "    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\n",
    "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
    "        if data.shape[0] != batch_size: break\n",
    "        agent.state[\"sample_amt\"] += data.shape[0]\n",
    "\n",
    "        agent.state[\"data\"] = data\n",
    "        agent.state[\"target\"] = target\n",
    "        agent.state[\"i_example\"] = i\n",
    "\n",
    "        data = agent.state[\"data\"]\n",
    "        latent = agent.gen.encoder(data)\n",
    "        mem_x = None\n",
    "\n",
    "        for it in range(agent.params[\"gen_iters\"]):\n",
    "            generate(agent, input_latent=latent)\n",
    "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "\n",
    "        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\n",
    "        loss = []\n",
    "        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\n",
    "\n",
    "            mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\n",
    "            mem_pred = agent.cls(mem_x)\n",
    "            mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
    "            loss.append(np.mean(mem_loss))\n",
    "\n",
    "        print(\"Loss on generate: \",  np.mean(mem_loss))\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = UNetModel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_steps\": 100,\n",
    "    \"input_latent_strength\":0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"batch_size\": 256,\n",
    "    \"gen_lr\": 2e-5,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_cifar_10()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(params[\"n_epoch\"]):\n",
    "    loss = []\n",
    "    for i in range(0, X_train.shape[0], params[\"batch_size\"]):\n",
    "        X_batch = X_train[i:i+params[\"batch_size\"]]\n",
    "        y_batch = y_train[i:i+params[\"batch_size\"]]\n",
    "        latent = encoder(X_batch)\n",
    "        mem_x = generate(input_latent=latent)\n",
    "        mem_pred = model.cls(mem_x)\n",
    "        mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
    "        loss.append(np.mean(mem_loss))\n",
    "    print(\"Loss on generate: \",  np.mean(loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
