{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/experiment/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pveLpmUzEF5A",
    "outputId": "13d8eb0a-4a83-4434-8ad6-130d54aecfe5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-05-15 14:05:41--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment [following]\n",
      "--2023-05-15 14:05:41--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘experiment.zip.1’\n",
      "\n",
      "experiment.zip.1        [ <=>                ]   1.62M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-05-15 14:05:41 (11.1 MB/s) - ‘experiment.zip.1’ saved [1696824]\n",
      "\n",
      "Archive:  experiment.zip\n",
      "81a3470d8644a0f0af548380128accc41c1ffbf4\n",
      "replace continual-learning-ait-experiment/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
     ]
    }
   ],
   "source": [
    "'''Download the files '''\n",
    "'''Only for colab'''\n",
    "\n",
    "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
    "!unzip experiment.zip\n",
    "#!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
    "!cd continual-learning-ait-experiment\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/continual-learning-ait-experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vjfVuEmXECoL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from keras.metrics import Accuracy\n",
    "\n",
    "#import classifier\n",
    "from generator import Generator\n",
    "from classifier import Classifier\n",
    "import utils\n",
    "from data_preparation import load_dataset, CLDataLoader, RealFakeConditionalDataset\n",
    "\n",
    "import gc\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zzYWiWfZ8kVi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TaH3bi-5crqD",
    "outputId": "c765824a-e5d7-438a-9f73-c8350a0dbd06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dpt_train, dpt_test = load_dataset('cifar-10', n_classes_first_task=4, n_classes_other_task=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yIESUurDeOCR"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = CLDataLoader(dpt_train, batch_size , train=True)\n",
    "test_loader = CLDataLoader(dpt_test, batch_size, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define parameters and agent"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3Jl-cBYs8kVj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GY9MAk581iYQ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    #general\n",
    "    \"n_runs\": 1,\n",
    "    \"n_tasks\": 3,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_shape\": (32, 32, 3),\n",
    "    \"embedding_shape\": (6, 6, 8),\n",
    "    \"samples_per_task\": 10000,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"eval_batch_size\": 1,\n",
    "    \"print_every\": 1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    #classifier\n",
    "    \"cls_iters\": 1,\n",
    "    \"cls_lr\": 1e-2,\n",
    "    \"cls_epochs\": 5,\n",
    "\n",
    "    #generator\n",
    "    \"gen_epochs\": 1,\n",
    "    \"num_steps\": 3,\n",
    "    \"gen_lr\": 2e-4,\n",
    "    \"gen_iters\": 1,\n",
    "    \"input_latent_strength\": 0.9,\n",
    "    \"temperature\": 0.9,\n",
    "\n",
    "    #mir\n",
    "    \"n_mem\": 2,\n",
    "    \"mir_iters\": 3,\n",
    "    \"reuse_samples\": True,\n",
    "    \"mem_coeff\": 0.12,\n",
    "    \"z_size\": 10,\n",
    "    \n",
    "    \"gen_ent_coeff\": 0.5,\n",
    "    \"gen_div_coeff\": 0.5,\n",
    "    \"gen_shell_coeff\": 0.5,\n",
    "    \"cls_xent_coeff\": 0.5,\n",
    "    \"cls_ent_coeff\": 0.5,\n",
    "    \"cls_div_coeff\": 0.5,\n",
    "    \"cls_shell_coeff\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OmitFSMM2WJn"
   },
   "outputs": [],
   "source": [
    "'''Agent to handle models, parameters and states'''\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, hparams):\n",
    "    self.params = hparams\n",
    "    self.state = dict()\n",
    "\n",
    "    self.classifier = None\n",
    "    self.generator = None\n",
    "    self.encoder = None\n",
    "    self.encoder_classifier = None\n",
    "\n",
    "    self.eval = accuracy_score\n",
    "\n",
    "\n",
    "  def set_models(\n",
    "          self,\n",
    "          _generator=None,\n",
    "          _classifier=None,\n",
    "    ):\n",
    "    cls = _classifier #classifier\n",
    "    gen = _generator  #generator\n",
    "\n",
    "    self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"])\n",
    "    #self.optimizer_gen = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"])\n",
    "\n",
    "    #encoder pipeline\n",
    "    #data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    #enc_output = enc(data_input)\n",
    "    #self.encoder = keras.Model(inputs=data_input, outputs=enc_output)\n",
    "\n",
    "    # classifier pipeline\n",
    "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    cls_output = cls(data_input)\n",
    "    self.classifier = keras.Model(inputs=data_input, outputs=cls_output)\n",
    "    self.classifier.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # encoder - classifier pipeline\n",
    "    #data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    #enc_output = enc(data_input)\n",
    "    #enc_cls_output = cls(enc_output)\n",
    "    #self.encoder_classifier = keras.Model(inputs= data_input, outputs = enc_cls_output)\n",
    "    #self.encoder_classifier.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # generator pipeline\n",
    "    self.generator = gen\n",
    "    #latent_input = keras.Input(shape=self.params[\"embedding_shape\"], name=\"latent\")\n",
    "    #gen_output = gen(latent_input)\n",
    "    #self.generator = keras.Model(inputs = latent_input, outputs = gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OPqiS9VV8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model at the same time'''\n",
    "\n",
    "def generate(\n",
    "        agent,\n",
    "        boosting, time_min, time_max,\n",
    "        dg_weight_1st_order, dg_weight_2nd_order,\n",
    "        batch_size,\n",
    "        class_idx=None,\n",
    "):\n",
    "    # Pick latents and labels.\n",
    "    latents = torch.randn([batch_size, agent.generator.net.img_channels, agent.params.img_resolution, agent.params.img_resolution], device=agent.params['device'])\n",
    "\n",
    "    class_labels = torch.eye(agent.params['n_classes'], device=agent.params['device'])[torch.randint(agent.params['classes_learned'], size=[batch_size], device=agent.params['device'])]\n",
    "    if class_idx is not None:\n",
    "        class_labels[:, :] = 0\n",
    "        class_labels[:, class_idx] = 1\n",
    "\n",
    "    # Generate images.\n",
    "    images = agent.generator.sample(boosting, time_min, time_max, dg_weight_1st_order, dg_weight_2nd_order, latents, class_labels, randn_like=torch.randn_like)\n",
    "\n",
    "    return images, class_labels\n"
   ],
   "metadata": {
    "id": "2BgWNzhg8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector for classifier'''\n",
    "\n",
    "def retrieve_gen_for_cls(agent):\n",
    "\n",
    "    print(\"Retrieving latent vector for classifier...\")\n",
    "\n",
    "    latent = agent.encoder(agent.state[\"data\"])\n",
    "    virtual_cls = Classifier()\n",
    "    virtual_cls = utils.get_next_step_cls(\n",
    "        agent.cls,\n",
    "        virtual_cls,\n",
    "        latent,\n",
    "        agent.state[\"target\"]\n",
    "    )\n",
    "\n",
    "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    final_latent = None\n",
    "\n",
    "    for i in range(agent.params[\"n_mem\"]):\n",
    "\n",
    "        generated = generate(agent)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                tape.watch(generated)\n",
    "\n",
    "                y_pre = agent.classifier(generated)\n",
    "                y_virtual = virtual_cls(generated)\n",
    "\n",
    "                # maximise the interference:\n",
    "                XENT = tf.constant(0.)\n",
    "                if params[\"cls_xent_coeff\"] > 0.:\n",
    "                    XENT = tf.keras.losses.categorical_crossentropy(y_virtual, y_pre)\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = tf.constant(0.)\n",
    "                if params[\"cls_ent_coeff\"] > 0.:\n",
    "                    ENT = tf.keras.losses.categorical_crossentropy(y_pre, y_pre)\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = tf.constant(0.)\n",
    "                if params[\"cls_div_coeff\"] > 0.:\n",
    "                    for found_generated in range(i):\n",
    "                        DIV += tf.keras.losses.MSE(\n",
    "                            generated,\n",
    "                            final_latent[found_generated * generated.shape[0]:found_generated * generated.shape[0] + generated.shape[0]]\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = tf.constant(0.)\n",
    "                if params[\"cls_shell_coeff\"] > 0.:\n",
    "                    SHELL = tf.keras.losses.MSE(\n",
    "                        tf.norm(generated, axis=1),\n",
    "                        tf.ones_like(tf.norm(generated, axis=1))*np.sqrt(params[\"z_size\"])\n",
    "                    )\n",
    "\n",
    "                XENT, ENT, DIV, SHELL = \\\n",
    "                    tf.reduce_mean(XENT), \\\n",
    "                        tf.reduce_mean(ENT), \\\n",
    "                        tf.reduce_mean(DIV), \\\n",
    "                        tf.reduce_mean(SHELL)\n",
    "\n",
    "                gain = params[\"cls_xent_coeff\"] * XENT + \\\n",
    "                       -params[\"cls_ent_coeff\"] * ENT + \\\n",
    "                       params[\"cls_div_coeff\"] * DIV + \\\n",
    "                       -params[\"cls_shell_coeff\"] * SHELL\n",
    "\n",
    "            gen_grad = tape.gradient(gain, generated)\n",
    "            if gen_grad is not None:\n",
    "                generated = (generated + 1 * gen_grad)\n",
    "\n",
    "        if final_latent is None:\n",
    "            final_latent = generated.numpy().copy()\n",
    "        else:\n",
    "            final_latent = np.concatenate([final_latent, generated.numpy().copy()])\n",
    "\n",
    "    tf.stop_gradient(final_latent)\n",
    "\n",
    "    mir_worked = not np.isnan(final_latent).any()\n",
    "    mem_x = final_latent if mir_worked else generate(agent, train=False)\n",
    "    mem_y = agent.classifier(mem_x).numpy()\n",
    "\n",
    "    return mem_x, mem_y, mir_worked"
   ],
   "metadata": {
    "id": "OlENSPig8kVl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector for generator'''\n",
    "\n",
    "\n",
    "def retrieve_gen_for_gen(agent):\n",
    "\n",
    "    print(\"Retrieving latent vector for generator...\")\n",
    "\n",
    "    latent = agent.encoder(agent.state[\"data\"])\n",
    "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    final_latent = None\n",
    "\n",
    "    for i in range(params[\"n_mem\"]):\n",
    "\n",
    "        generated = generate(agent)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                tape.watch(generated)\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = tf.constant(0.)\n",
    "                if params[\"gen_ent_coeff\"]>0.:\n",
    "                    y_pre = agent.classifier(generated)\n",
    "                    y_pre_true = utils.get_one_hot_predictions(y_pre)\n",
    "                    ENT = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pre_true, y_pre))\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = tf.constant(0.)\n",
    "                if params[\"gen_div_coeff\"]>0.:\n",
    "                    for found_generated in range(i):\n",
    "                        DIV += tf.reduce_mean(tf.math.squared_difference(\n",
    "                            generated,\n",
    "                            final_latent[found_generated * generated.shape[0]:found_generated * generated.shape[0] + generated.shape[0]])\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = tf.constant(0.)\n",
    "                if params[\"gen_shell_coeff\"]>0.:\n",
    "                    SHELL = tf.reduce_mean(tf.math.squared_difference(\n",
    "                        tf.norm(generated, ord=2, axis=1),\n",
    "                        tf.ones_like(tf.norm(generated, ord=2, axis=1))*np.sqrt(params[\"z_size\"])))\n",
    "\n",
    "\n",
    "                gain =params[\"gen_div_coeff\"] * DIV + \\\n",
    "                      -params[\"gen_ent_coeff\"] * ENT + \\\n",
    "                       -params[\"gen_shell_coeff\"] * SHELL\n",
    "\n",
    "            grad = tape.gradient(gain, generated)\n",
    "            generated = (generated + grad)\n",
    "\n",
    "        if final_latent is None:\n",
    "            final_latent = tf.identity(generated)\n",
    "        else:\n",
    "            final_latent = tf.concat([final_latent, generated], axis=0)\n",
    "\n",
    "\n",
    "    tf.stop_gradient(final_latent)\n",
    "\n",
    "    mir_worked = not np.isnan(final_latent).any()\n",
    "    mem_x = final_latent if mir_worked else generate(agent, train=False)\n",
    "\n",
    "    return mem_x, mir_worked"
   ],
   "metadata": {
    "id": "5ho68Omg8kVl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lM8aEv6833SS"
   },
   "outputs": [],
   "source": [
    "'''Train the generator unit'''\n",
    "\n",
    "def train_generator(agent):\n",
    "    scaler = lambda x: 2. * x - 1.\n",
    "    optimizer = torch.optim.Adam(agent.generator.discriminator.parameters(), lr=agent.params[\"gen_lr\"], weight_decay=1e-7)\n",
    "    loss = torch.nn.BCELoss()\n",
    "    # Training\n",
    "    outs = []\n",
    "    cors = []\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels, cond = data\n",
    "        cond = cond.to(agent.params[\"device\"])\n",
    "        inputs = inputs.to(agent.params[\"device\"])\n",
    "        labels = labels.to(agent.params[\"device\"])\n",
    "        inputs = scaler(inputs)\n",
    "\n",
    "        # Data perturbation\n",
    "        t, _ = utils.vpsde.get_diffusion_time(inputs.shape[0], inputs.device)\n",
    "        mean, std = utils.vpsde.marginal_prob(t)\n",
    "        z = torch.randn_like(inputs)\n",
    "        perturbed_inputs = mean[:, None, None, None] * inputs + std[:, None, None, None] * z\n",
    "\n",
    "        # Forward\n",
    "        with torch.no_grad():\n",
    "            pretrained_feature = agent.generator.encoder(perturbed_inputs, timesteps=t, feature=True)\n",
    "        label_prediction = agent.generator.discriminator(pretrained_feature, t, sigmoid=True, condition=cond).view(-1)\n",
    "\n",
    "        # Backward\n",
    "        out = loss(label_prediction, labels)\n",
    "        out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        cor = ((label_prediction > 0.5).float() == labels).float().mean()\n",
    "        outs.append(out.item())\n",
    "        cors.append(cor.item())\n",
    "        return outs, cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AXTbt9sT8gHE"
   },
   "outputs": [],
   "source": [
    "'''Train the encoder and the classifier unit'''\n",
    "\n",
    "def train_classifier(agent):\n",
    "\n",
    "    data = agent.state[\"data\"]\n",
    "    target = agent.state[\"target\"]\n",
    "    mem_x, mem_y = None, None\n",
    "\n",
    "    for it in range(agent.params[\"cls_iters\"]):\n",
    "        history = agent.classifier.fit(data, target, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=0)\n",
    "        #if agent.state[\"task\"] > 0:\n",
    "         #   if it == 0 or not agent.params[\"reuse_samples\"]:\n",
    "          #      mem_x, mem_y, mir_worked = retrieve_gen_for_cls(agent)\n",
    "           #     agent.state[\"mir_tries\"] += 1\n",
    "            #    if mir_worked:\n",
    "             #       agent.state[\"mir_success\"] += 1\n",
    "\n",
    "            #if mem_x is not None:\n",
    "             #   mem_history = agent.classifier.fit(mem_x, utils.get_one_hot_predictions(mem_y), batch_size=agent.params[\"batch_size\"], epochs=1, verbose=1)\n",
    "\n",
    "             #   agent.state[\"epoch_eval\"][\"retr_cls_loss\"].append(mem_history.history[\"loss\"][0])\n",
    "            #    agent.state[\"epoch_eval\"][\"retr_cls_accuracy\"].append(mem_history.history[\"accuracy\"][0])\n",
    "        agent.state[\"epoch_eval\"][\"cls_loss\"].append(history.history[\"loss\"][0])\n",
    "        agent.state[\"epoch_eval\"][\"cls_acc\"].append(history.history[\"accuracy\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "'''Run an epoch'''\n",
    "\n",
    "def run_cls_epoch(agent):\n",
    "\n",
    "    print(f\"Running Task {agent.state['task']}, Epoch: {agent.state['epoch']} on Classifier\")\n",
    "\n",
    "    agent.state[\"epoch_eval\"][\"cls_loss\"] = []\n",
    "    agent.state[\"epoch_eval\"][\"cls_acc\"] = []\n",
    "    agent.state[\"epoch_eval\"][\"retr_cls_loss\"] = []\n",
    "    agent.state[\"epoch_eval\"][\"retr_cls_accuracy\"] = []\n",
    "\n",
    "    agent.state[\"sample_amt\"] = 0\n",
    "    for i, (data, target) in enumerate(agent.state[\"tr_loader\"]):\n",
    "        agent.state[\"data\"] = data\n",
    "        agent.state[\"target\"] = target\n",
    "        train_classifier(agent)\n",
    "\n",
    "    '''Evaluate the models in epoch'''\n",
    "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "        print(f\"    Classifier loss: {np.mean(agent.state['epoch_eval']['cls_loss'])}\"\n",
    "              f\"    Classifier accuracy: {np.mean(agent.state['epoch_eval']['cls_acc'])}\")"
   ],
   "metadata": {
    "id": "xmJWM9GCzym_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''Run an epoch'''\n",
    "\n",
    "def run_gen_epoch(agent):\n",
    "  \n",
    "    print(f\"Running Task {agent.state['task']}, Epoch: {agent.state['epoch']} on Generator\")\n",
    "    \n",
    "    agent.state[\"epoch_eval\"][\"gen_loss\"] = []\n",
    "    agent.state[\"epoch_eval\"][\"retr_gen_loss\"] = []\n",
    "    agent.state[\"epoch_eval\"][\"retr_gen_accuracy\"] = []\n",
    "\n",
    "    agent.state[\"sample_amt\"] = 0\n",
    "    real_loader = agent.state[\"tr_loader\"]\n",
    "\n",
    "    for i, (X_real, y_real) in enumerate(real_loader):\n",
    "        X_gen, y_gen = generate(agent, 0 , 0.01, 1.0, 0, 0, agent.params[\"batch_size\"])\n",
    "\n",
    "        train_data = np.concatenate((X_real, X_gen))\n",
    "        train_label = torch.zeros(train_data.shape[0])\n",
    "        train_label[:X_real.shape[0]] = 1.\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        condition_label = np.concatenate((y_real, y_gen))\n",
    "        train_dataset = RealFakeConditionalDataset(train_data, train_label, condition_label, transform)\n",
    "\n",
    "        agent.state[\"real_fake_loader\"]  = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        outs, cors = train_generator(agent)\n",
    "        print(f\"{i}-th batch BCE loss: {np.mean(outs)}, correction rate: {np.mean(cors)}\")\n",
    "\n",
    "    #'''Evaluate the models in epoch'''\n",
    "    #if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "    #    print(f\"    Generator loss: {np.mean(agent.state['epoch_eval']['gen_loss'])}\")\n",
    "              #f\"    Loss on gen retrieve for cls: {np.mean(agent.state['epoch_eval']['retr_cls_loss'])}\"\n",
    "              #f\"    Loss on gen retrieve for gen: {np.mean(agent.state['epoch_eval']['retr_gen_loss'])}\")"
   ],
   "metadata": {
    "id": "K48EQq7VGuI-"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0SyPNSep_K0t"
   },
   "outputs": [],
   "source": [
    "'''Run a task'''\n",
    "\n",
    "def run_task(agent):\n",
    "\n",
    "    agent.state[\"mir_tries\"], agent.state[\"mir_success\"] = 0, 0\n",
    "    agent.state[\"epoch_eval\"] = dict()\n",
    "\n",
    "    for epoch in range(agent.params[\"cls_epochs\"]):\n",
    "        agent.state[\"epoch\"] = epoch\n",
    "        run_cls_epoch(agent)\n",
    "\n",
    "    for epoch in range(agent.params[\"gen_epochs\"]):\n",
    "        agent.state[\"epoch\"] = epoch\n",
    "        run_gen_epoch(agent)\n",
    "\n",
    "    #'''Evaluate forgetting'''\n",
    "    #if (agent.state['task']) > 0:\n",
    "    #  print(\"Evaluate Task: \", agent.state[\"task\"])\n",
    "    #for i in range(agent.state[\"task\"]):\n",
    "    #    task_loss = []\n",
    "    #    task_eval = []\n",
    "    #    for data, target in test_loader[i]:\n",
    "    #        logits = agent.encoder_classifier(data)\n",
    "    #        pred = np.argmax(logits, axis=1)\n",
    "    #        y = np.argmax(target, axis=1)\n",
    "    #        eval = agent.eval(y, pred)\n",
    "    #        task_eval.append(eval)\n",
    "#            loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    " ##           task_loss.append(np.mean(loss))\n",
    "  #      print(f\"    Task {agent.state['task']} forgetting on task {i} : \"\n",
    "  #            f\"        Loss: {np.mean(task_loss)}\"\n",
    "   #           f\"        ACC: {np.mean(task_eval)}\")\n",
    "#\n",
    " #   #print(\"MIR success rate: \", agent.state[\"mir_success\"] / agent.state[\"mir_tries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhbA5MCFCi4r"
   },
   "outputs": [],
   "source": [
    "'''Run the experiment'''\n",
    "\n",
    "def run(agent):\n",
    "  agent.state[\"classes_learned\"] = []\n",
    "  for r in range(agent.params[\"n_runs\"]):\n",
    "    agent.state[\"run\"] = r\n",
    "    print(f\"Run {r}\")\n",
    "    for task, (tr_loader, ts_loader) in enumerate(zip(train_loader, test_loader)):\n",
    "        agent.state[\"task\"] = task\n",
    "        agent.state[\"tr_loader\"] = tr_loader\n",
    "        agent.state[\"ts_loader\"] = ts_loader\n",
    "        run_task(agent)\n",
    "        #agent.state[\"classes_learned\"].append(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AoBLwork8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BlPHqKTaC41u",
    "outputId": "c7aa4453-b59a-414b-8cd7-9874c8ecfdd3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-f81a8609f8a0>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAgent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m agent.set_models(\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0m_classifier\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0m_generator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mGenerator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mencoder_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoder_f\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m )\n",
      "\u001B[0;32m/content/continual-learning-ait-experiment/classifier.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, KERNEL_SIZE, INPUT_SHAPE)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSequential\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mKERNEL_SIZE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mINPUT_SHAPE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m         super().__init__(\n\u001B[0m\u001B[1;32m      7\u001B[0m             \u001B[0mConv2D\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilters\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mKERNEL_SIZE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mINPUT_SHAPE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'relu'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'same'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m             \u001B[0mBatchNormalization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Sequential.__init__() takes from 1 to 3 positional arguments but 29 were given"
     ]
    }
   ],
   "source": [
    "encoder_f = '/drive/MyDrive/continual-learning-ait/checkpoints/32x32_classifier.pt'\n",
    "scorenet_f = '/drive/MyDrive/continual-learning-ait/checkpoints/edm-cifar10-32x32-cond-vp.pkl'\n",
    "\n",
    "agent = Agent(params)\n",
    "agent.set_models(\n",
    "    _classifier=Classifier(),\n",
    "    _generator=Generator(encoder_path=encoder_f),\n",
    ")\n",
    "#run(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eval"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Qqe-XGuwzynA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Evaluation'''\n",
    "for task, loader in enumerate(test_loader):\n",
    "    print(\"Task: \", task)\n",
    "    LOSS = []\n",
    "    ACC = []\n",
    "    for data, target in loader:\n",
    "      logits = agent.classifier_model(data)\n",
    "      pred = np.argmax(logits, axis=1)\n",
    "      report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "      loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "      #print(report)\n",
    "      ACC.append(report)\n",
    "      LOSS.append(loss)\n",
    "    print(\"Mean loss: \", np.mean(LOSS))\n",
    "    print(\"Mean accuracy: \", np.mean(ACC))\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "id": "-BguEZvfzynA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation, testing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "UiiKIl618kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(loader, first_n_tasks=None):\n",
    "    for task, tr_loader in enumerate(loader):\n",
    "        print(\"Task: \", task)\n",
    "        data, target = tr_loader.batch(124)\n",
    "        logits = agent.classifier_model(data)\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "        print(report)\n",
    "        print(\"Mean loss: \", np.mean(loss))"
   ],
   "metadata": {
    "id": "r_K9lbNh8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I2ZLTm-JnX8"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluation on training set:\")\n",
    "evaluate(train_loader)\n",
    "print(\"Evaluation on test set:\")\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils for development"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vOxdRLeJ8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'stable_diffusion.stable_diffusion' from '/Users/laszlofreund/PycharmProjects/continual-learning-ait/stable_diffusion/stable_diffusion.py'>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload modules\n",
    "importlib.reload()"
   ],
   "metadata": {
    "id": "LNWS0pbd8kVn",
    "outputId": "b2184f26-86ef-40da-d970-7bd8b2d283ff"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "21547"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collection\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "NCdIwGMq8kVn",
    "outputId": "2495022b-fa19-489f-ae37-79b95d3ae24a"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
