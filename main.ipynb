{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/experiment/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pveLpmUzEF5A",
    "outputId": "62665bd2-f64e-4857-8422-7a1fd23db346"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-05-08 16:08:55--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment [following]\n",
      "--2023-05-08 16:08:55--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.121.10\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.121.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘experiment.zip’\n",
      "\n",
      "experiment.zip          [ <=>                ] 758.34K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2023-05-08 16:08:56 (8.95 MB/s) - ‘experiment.zip’ saved [776540]\n",
      "\n",
      "Archive:  experiment.zip\n",
      "42cdb330f22efaaa980ae0b0fd340081b3729d1e\n",
      "   creating: continual-learning-ait-experiment/\n",
      "  inflating: continual-learning-ait-experiment/README.md  \n",
      "  inflating: continual-learning-ait-experiment/classifier.py  \n",
      "  inflating: continual-learning-ait-experiment/data_preparation.ipynb  \n",
      "  inflating: continual-learning-ait-experiment/data_preparation.py  \n",
      "  inflating: continual-learning-ait-experiment/img.png  \n",
      "  inflating: continual-learning-ait-experiment/main.ipynb  \n",
      "   creating: continual-learning-ait-experiment/stable_diffusion/\n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/autoencoder_kl.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/constants.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/diffusion_model.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/layers.py  \n",
      "  inflating: continual-learning-ait-experiment/stable_diffusion/stable_diffusion.py  \n",
      "  inflating: continual-learning-ait-experiment/utils.py  \n",
      "rm: cannot remove 'stable_diffusion': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "'''Download the files '''\n",
    "'''Only for colab'''\n",
    "\n",
    "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
    "!unzip experiment.zip\n",
    "!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
    "\n",
    "!rm -r stable_diffusion\n",
    "!mkdir stable_diffusion\n",
    "!mv diffusion_model.py stable_diffusion/\n",
    "!mv autoencoder_kl.py stable_diffusion/\n",
    "!mv layers.py stable_diffusion/\n",
    "!mv stable_diffusion.py stable_diffusion/\n",
    "!mv constants.py stable_diffusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vjfVuEmXECoL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import classification_report\n",
    "#from keras.metrics import Accuracy\n",
    "\n",
    "import classifier\n",
    "from stable_diffusion import stable_diffusion\n",
    "import utils\n",
    "from data_preparation import load_dataset, CLDataLoader\n",
    "\n",
    "import gc\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zzYWiWfZ8kVi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TaH3bi-5crqD",
    "outputId": "fc0e09e4-ef09-46c7-beaa-7ee77449496c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dpt_train, dpt_test = load_dataset('cifar-10', n_classes_first_task=4, n_classes_other_task=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yIESUurDeOCR"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = CLDataLoader(dpt_train, batch_size , train=True)\n",
    "test_loader = CLDataLoader(dpt_test, batch_size, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define parameters and agent"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3Jl-cBYs8kVj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GY9MAk581iYQ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    #general\n",
    "    \"n_runs\": 1,\n",
    "    \"n_tasks\": 10,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_shape\": (32,32, 3),\n",
    "    \"embedding_shape\": (4, 4, 4),\n",
    "    \"samples_per_task\": 10000,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"eval_batch_size\": 1,\n",
    "    \"print_every\": 1,\n",
    "\n",
    "    #classifier\n",
    "    \"cls_epochs\": 10,\n",
    "    \"cls_iters\": 5,\n",
    "    \"cls_hiddens\": 32,\n",
    "    \"cls_lr\": 0.03,\n",
    "\n",
    "    #generator\n",
    "    \"gen_epochs\": 5,\n",
    "    \"num_steps\": 2,\n",
    "    \"gen_iters\": 1,\n",
    "    \"input_latent_strength\": 0.75,\n",
    "    \"gen_lr\": 0.01,\n",
    "    \"temperature\": 1,\n",
    "\n",
    "    #mir\n",
    "    \"reuse_samples\": True,\n",
    "    \"cls_mir_gen\": 1,\n",
    "    \"gen_mir_gen\": 1,\n",
    "    \"mem_coeff\": 0.12,\n",
    "    \"n_mem\": 2,\n",
    "    \"z_size\": 10,\n",
    "    \"mir_iters\": 3,\n",
    "    \"gen_kl_coeff\": 0.5,\n",
    "    \"gen_rec_coeff\": 0.5,\n",
    "    \"gen_ent_coeff\": 0.5,\n",
    "    \"gen_div_coeff\": 0.5,\n",
    "    \"gen_shell_coeff\": 0.5,\n",
    "    \"cls_xent_coeff\": 0.5,\n",
    "    \"cls_ent_coeff\": 0.5,\n",
    "    \"cls_div_coeff\": 0.5,\n",
    "    \"cls_shell_coeff\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OmitFSMM2WJn"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import ParamSpec\n",
    "'''Agent to handle models, parameters and states'''\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, hparams):\n",
    "    self.cls = None\n",
    "    self.opt = None\n",
    "    self.opt_gen = None\n",
    "    self.gen = None\n",
    "    self.params = hparams\n",
    "    self.state = dict()\n",
    "    self.encoder = None\n",
    "    self.classifier_model = None\n",
    "    self.eval = accuracy_score\n",
    "    #self.decoder = None\n",
    "\n",
    "  def set_models(self, generator=None, classifier=None):\n",
    "    self.cls = classifier #classifier\n",
    "    self.gen = generator  #generator\n",
    "    self.encoder = generator.encoder #encoder\n",
    "    #self.decoder = gen.decoder\n",
    "    self.opt = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"])\n",
    "    self.opt_gen = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"])\n",
    "\n",
    "    # encoder - classifier pipeline\n",
    "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    encoder_output = self.encoder(data_input)\n",
    "    cls_encoder_output = self.cls(encoder_output)\n",
    "    self.classifier_model = keras.Model(inputs=data_input, outputs=cls_encoder_output)\n",
    "    self.classifier_model.compile(optimizer=self.opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # classifier pipeline\n",
    "    latent_input = keras.Input(shape=self.params[\"embedding_shape\"], name=\"latent\")\n",
    "    cls_latent_output = self.cls(latent_input)\n",
    "    self.cls_model = keras.Model(inputs=latent_input, outputs=cls_latent_output)\n",
    "    self.cls_model.compile(optimizer=self.opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "  def set_models_from_file(self, generator):\n",
    "\n",
    "    self.cls = keras.models.load_model('classifier.h5') #classifier\n",
    "    self.gen = generator  #generator\n",
    "    self.encoder = keras.models.load_model('encoder.h5') #encoder\n",
    "    #self.decoder = gen.decoder\n",
    "    self.opt = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"])\n",
    "    self.opt_gen = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"])\n",
    "\n",
    "    # encoder - classifier pipeline\n",
    "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    encoder_output = self.encoder(data_input)\n",
    "    cls_encoder_output = self.cls(encoder_output)\n",
    "    self.classifier_model = keras.Model(inputs=data_input, outputs=cls_encoder_output)\n",
    "    self.classifier_model.compile(optimizer=self.opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # classifier pipeline\n",
    "    latent_input = keras.Input(shape=self.params[\"embedding_shape\"], name=\"latent\")\n",
    "    cls_latent_output = self.cls(latent_input)\n",
    "    self.cls_model = keras.Model(inputs=latent_input, outputs=cls_latent_output)\n",
    "    self.cls_model.compile(optimizer=self.opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "  def set_params(self, params):\n",
    "    self.params = params"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OPqiS9VV8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model at the same time'''\n",
    "\n",
    "def generate(agent, cls=None, input_latent=None, train=True, coeff=1.0):\n",
    "\n",
    "    if cls is None:\n",
    "        cls = agent.cls\n",
    "    batch_size = agent.params['batch_size'] if train else 64\n",
    "    latent, alphas, alphas_prev, timesteps = agent.gen.initialize(agent.params, input_latent, batch_size)\n",
    "\n",
    "\n",
    "    for index, timestep in reversed(list(enumerate(timesteps))):\n",
    "        if train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                e_t = agent.gen.get_model_output(\n",
    "                    latent,\n",
    "                    timestep,\n",
    "                    batch_size,\n",
    "                )\n",
    "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "                latent = agent.gen.get_x_prev(latent, e_t,  a_t, a_prev, agent.params[\"temperature\"])\n",
    "\n",
    "                pred = cls(latent)\n",
    "                #loss based on confidence\n",
    "                #ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
    "                loss = coeff*tf.keras.losses.categorical_crossentropy(pred, pred)\n",
    "            grads = tape.gradient(loss, agent.gen.diffusion_model.trainable_variables)\n",
    "            agent.opt_gen.apply_gradients(zip(grads, agent.gen.diffusion_model.trainable_variables))\n",
    "        else:\n",
    "            e_t = agent.gen.get_model_output(\n",
    "                latent,\n",
    "                timestep,\n",
    "                agent.params['batch_size'],\n",
    "            )\n",
    "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "            latent = agent.gen.get_x_prev(latent, e_t,  a_t, a_prev, agent.params[\"temperature\"])\n",
    "\n",
    "\n",
    "\n",
    "    return latent"
   ],
   "metadata": {
    "id": "2BgWNzhg8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector for classifier'''\n",
    "\n",
    "def retrieve_gen_for_cls(agent):\n",
    "\n",
    "    print(\"Retrieving latent vector for classifier...\")\n",
    "\n",
    "    latent = agent.gen.encoder(agent.state[\"data\"])\n",
    "    virtual_cls = classifier.classifier(agent.params)\n",
    "    virtual_cls = utils.get_next_step_cls(\n",
    "        agent.cls,\n",
    "        virtual_cls,\n",
    "        latent,\n",
    "        agent.state[\"target\"]\n",
    "    )\n",
    "\n",
    "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    z_new_max = None\n",
    "\n",
    "    for i in range(agent.params[\"n_mem\"]):\n",
    "\n",
    "        z_new = generate(agent, input_latent=latent, train=False, coeff=0.1)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                tape.watch(z_new)\n",
    "\n",
    "                #z_new = tf.cast(z_new, tf.float64)\n",
    "                y_pre = agent.cls(z_new)\n",
    "                y_virtual = virtual_cls(z_new)\n",
    "\n",
    "                # maximise the interference:\n",
    "                XENT = tf.constant(0.)\n",
    "                if params[\"cls_xent_coeff\"] > 0.:\n",
    "                    XENT = tf.keras.losses.categorical_crossentropy(y_virtual, y_pre)\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = tf.constant(0.)\n",
    "                if params[\"cls_ent_coeff\"] > 0.:\n",
    "                    ENT = tf.keras.losses.categorical_crossentropy(y_pre, y_pre)\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = tf.constant(0.)\n",
    "                if params[\"cls_div_coeff\"] > 0.:\n",
    "                    for found_z_i in range(i):\n",
    "                        DIV += tf.keras.losses.MSE(\n",
    "                            z_new,\n",
    "                            z_new_max[found_z_i * z_new.shape[0]:found_z_i * z_new.shape[0] + z_new.shape[0]]\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = tf.constant(0.)\n",
    "                if params[\"cls_shell_coeff\"] > 0.:\n",
    "                    SHELL = tf.keras.losses.MSE(\n",
    "                        tf.norm(z_new, axis=1),\n",
    "                        tf.ones_like(tf.norm(z_new, axis=1))*np.sqrt(params[\"z_size\"])\n",
    "                    )\n",
    "\n",
    "                XENT, ENT, DIV, SHELL = \\\n",
    "                    tf.reduce_mean(XENT), \\\n",
    "                        tf.reduce_mean(ENT), \\\n",
    "                        tf.reduce_mean(DIV), \\\n",
    "                        tf.reduce_mean(SHELL)\n",
    "\n",
    "                gain = params[\"cls_xent_coeff\"] * XENT + \\\n",
    "                       -params[\"cls_ent_coeff\"] * ENT + \\\n",
    "                       params[\"cls_div_coeff\"] * DIV + \\\n",
    "                       -params[\"cls_shell_coeff\"] * SHELL\n",
    "\n",
    "            z_g = tape.gradient(gain, z_new)\n",
    "            if z_g is not None:\n",
    "                z_new = (z_new + 1 * z_g)\n",
    "\n",
    "        if z_new_max is None:\n",
    "            z_new_max = z_new.numpy().copy()\n",
    "        else:\n",
    "            z_new_max = np.concatenate([z_new_max, z_new.numpy().copy()])\n",
    "\n",
    "    tf.stop_gradient(z_new_max)\n",
    "\n",
    "    if np.isnan(z_new_max).any():\n",
    "        mir_worked = 0\n",
    "        mem_x = generate(agent, train=False)\n",
    "    else:\n",
    "        mem_x = z_new_max\n",
    "        mir_worked = 1\n",
    "\n",
    "    mem_y = agent.cls(mem_x).numpy()\n",
    "\n",
    "    return mem_x, mem_y, mir_worked"
   ],
   "metadata": {
    "id": "OlENSPig8kVl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector for generator'''\n",
    "#TODO: vmi más loss is (maximise interference)\n",
    "\n",
    "def retrieve_gen_for_gen(agent):\n",
    "\n",
    "    print(\"Retrieving latent vector for generator...\")\n",
    "\n",
    "    latent = agent.gen.encoder(agent.state[\"data\"])\n",
    "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    z_new_max = None\n",
    "\n",
    "    for i in range(params[\"n_mem\"]):\n",
    "\n",
    "        z_new = generate(agent, input_latent=latent, train=False, coeff=0.1)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                tape.watch(z_new)\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = tf.constant(0.)\n",
    "                if params[\"gen_ent_coeff\"]>0.:\n",
    "                    y_pre = agent.cls(z_new)\n",
    "                    ENT = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pre, y_pre))\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = tf.constant(0.)\n",
    "                if params[\"gen_div_coeff\"]>0.:\n",
    "                    for found_z_i in range(i):\n",
    "                        DIV += tf.reduce_mean(tf.math.squared_difference(\n",
    "                            z_new,\n",
    "                            z_new_max[found_z_i * z_new.shape[0]:found_z_i * z_new.shape[0] + z_new.shape[0]])\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = tf.constant(0.)\n",
    "                if params[\"gen_shell_coeff\"]>0.:\n",
    "                    SHELL = tf.reduce_mean(tf.math.squared_difference(\n",
    "                        tf.norm(z_new, ord=2, axis=1),\n",
    "                        tf.ones_like(tf.norm(z_new, ord=2, axis=1))*np.sqrt(params[\"z_size\"])))\n",
    "\n",
    "\n",
    "                gain =params[\"gen_div_coeff\"] * DIV + \\\n",
    "                      -params[\"gen_ent_coeff\"] * ENT + \\\n",
    "                       -params[\"gen_shell_coeff\"] * SHELL\n",
    "\n",
    "            z_g = tape.gradient(gain, z_new)\n",
    "            z_new = (z_new + z_g)\n",
    "\n",
    "        if z_new_max is None:\n",
    "            z_new_max = tf.identity(z_new)\n",
    "        else:\n",
    "            z_new_max = tf.concat([z_new_max, z_new], axis=0)\n",
    "\n",
    "\n",
    "    tf.stop_gradient(z_new_max)\n",
    "\n",
    "    if np.isnan(z_new_max).any():\n",
    "        mir_worked = 0\n",
    "        mem_x = generate(agent, train=False)\n",
    "    else:\n",
    "        mem_x = z_new_max\n",
    "        mir_worked = 1\n",
    "\n",
    "    return mem_x, mir_worked"
   ],
   "metadata": {
    "id": "5ho68Omg8kVl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lM8aEv6833SS"
   },
   "outputs": [],
   "source": [
    "'''Train the generator unit'''\n",
    "\n",
    "def train_generator(agent):\n",
    "\n",
    "    data = agent.state[\"data\"]\n",
    "    latent = agent.gen.encoder(data)\n",
    "    mem_x = None\n",
    "\n",
    "    for it in range(agent.params[\"gen_iters\"]):\n",
    "        generate(agent, input_latent=latent)\n",
    "\n",
    "        #if agent.state[\"task\"] > 0:\n",
    "        #    if it == 0 or not agent.params[\"reuse_samples\"]:\n",
    "        #        mem_x, mir_worked = retrieve_gen_for_gen(agent)\n",
    "        #\n",
    "        #        agent.state[\"mir_tries\"] += 1\n",
    "        #        if mir_worked:\n",
    "        #            agent.state[\"mir_success\"] += 1\n",
    "\n",
    "        # TODO\n",
    "        #if mem_x is not None:\n",
    "        #  if len(mem_x.shape) == 3:\n",
    "        #    mem_x = tf.expand__dims(mem_x, axis=-1)\n",
    "        #  generate(agent, input_latent=mem_x, coeff=agent.params[\"mem_coeff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AXTbt9sT8gHE"
   },
   "outputs": [],
   "source": [
    "'''Train the encoder and the classifier unit'''\n",
    "\n",
    "def train_classifier(agent):\n",
    "\n",
    "    data = agent.state[\"data\"]\n",
    "    target = agent.state[\"target\"]\n",
    "    mem_x, mem_y = None, None\n",
    "\n",
    "    for it in range(agent.params[\"cls_iters\"]):\n",
    "        agent.classifier_model.fit(data, target, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=0)\n",
    "        #if agent.state[\"task\"] > 0:\n",
    "        #    if it == 0 or not agent.params[\"reuse_samples\"]:\n",
    "        #        mem_x, mem_y, mir_worked = retrieve_gen_for_cls(agent)\n",
    "        #        agent.state[\"mir_tries\"] += 1\n",
    "        #        if mir_worked:\n",
    "        #            agent.state[\"mir_success\"] += 1\n",
    "\n",
    "        #    if mem_x is not None:\n",
    "        #        agent.cls_model.fit(mem_x, mem_y, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0A88lN-h9hdV"
   },
   "outputs": [],
   "source": [
    "'''Run an epoch'''\n",
    "\n",
    "def run_cls_epoch(agent):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    agent.state[\"sample_amt\"] = 0\n",
    "\n",
    "    print(\"Running epoch on classifier \", agent.state[\"epoch\"])\n",
    "\n",
    "    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\n",
    "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
    "        agent.state[\"sample_amt\"] += data.shape[0]\n",
    "\n",
    "        agent.state[\"data\"] = data\n",
    "        agent.state[\"target\"] = target\n",
    "        agent.state[\"i_example\"] = i\n",
    "\n",
    "        train_classifier(agent)\n",
    "\n",
    "\n",
    "    '''Evaluate the models in epoch'''\n",
    "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "\n",
    "        print(\"\\nEvaluate classifier on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\n",
    "\n",
    "        accuracy = []\n",
    "        losses = []\n",
    "\n",
    "        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\n",
    "\n",
    "          '''Evaluate the classifier'''\n",
    "          logits = agent.classifier_model(data)\n",
    "          pred = np.argmax(logits, axis=1)\n",
    "          report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "          loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "          accuracy.append(report)\n",
    "          losses.append(np.mean(loss))\n",
    "\n",
    "\n",
    "        print(\"Mean accuracy: \", np.mean(accuracy))\n",
    "        print(\"Mean loss: \", np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''Run an epoch'''\n",
    "\n",
    "def run_gen_epoch(agent):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    agent.state[\"sample_amt\"] = 0\n",
    "\n",
    "    print(\"Running generator epoch \", agent.state[\"epoch\"])\n",
    "\n",
    "    for i, (data, target) in tqdm(enumerate(agent.state[\"tr_loader\"])):\n",
    "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
    "        if data.shape[0] != batch_size: break\n",
    "        agent.state[\"sample_amt\"] += data.shape[0]\n",
    "\n",
    "        agent.state[\"data\"] = data\n",
    "        agent.state[\"target\"] = target\n",
    "        agent.state[\"i_example\"] = i\n",
    "\n",
    "        train_generator(agent)\n",
    "\n",
    "    '''Evaluate the models in epoch'''\n",
    "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
    "\n",
    "        print(\"\\nEvaluate generator on Task: \", agent.state[\"task\"], \" Epoch: \", agent.state[\"epoch\"])\n",
    "        loss = []\n",
    "        for i, (data, target) in tqdm(enumerate(agent.state[\"ts_loader\"])):\n",
    "\n",
    "          '''Evaluate the generator'''\n",
    "          mem_x = generate(agent, input_latent=agent.encoder(data), train=False)\n",
    "          mem_pred = agent.cls(mem_x)\n",
    "          mem_loss = tf.keras.losses.categorical_crossentropy(mem_pred, mem_pred)\n",
    "          loss.append(np.mean(mem_loss))\n",
    "\n",
    "        print(\"Loss on generate: \",  np.mean(mem_loss))\n",
    "\n",
    "          #mem_x_cls, mem_y, mir_worked_cls = retrieve_gen_for_cls(agent)\n",
    "          #mem_loss_cls = tf.keras.losses.categorical_crossentropy(mem_y, mem_y)\n",
    "          #print(\"Loss on retrieve for cls: \",  np.mean(mem_loss_cls))\n",
    "          #print(\"MIR worked on retrieve for cls: \", mir_worked_cls)\n",
    "\n",
    "          #mem_x_gen, mir_worked_gen = retrieve_gen_for_gen(agent)\n",
    "          #em_pred_gen = agent.cls(mem_x_gen)\n",
    "          #mem_loss_gen = tf.keras.losses.categorical_crossentropy(mem_pred_gen, mem_pred_gen)\n",
    "          #print(\"Loss on retrieve for gen: \",  np.mean(mem_loss_gen))\n",
    "          #print(\"MIR worked on retrieve for gen: \", mir_worked_gen)"
   ],
   "metadata": {
    "id": "34IEmO3Wn_Lk"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0SyPNSep_K0t"
   },
   "outputs": [],
   "source": [
    "'''Run a task'''\n",
    "\n",
    "def run_task(agent):\n",
    "  \n",
    "    print(\"Running task \", agent.state[\"task\"])\n",
    "\n",
    "    agent.state[\"mir_tries\"], agent.state[\"mir_success\"] = 0, 0\n",
    "\n",
    "    for epoch in range(agent.params[\"cls_epochs\"]):\n",
    "        agent.state[\"epoch\"] = epoch\n",
    "        run_cls_epoch(agent)\n",
    "      \n",
    "    for epoch in range(agent.params[\"gen_epochs\"]):\n",
    "        agent.state[\"epoch\"] = epoch\n",
    "        run_gen_epoch(agent)\n",
    "\n",
    "    '''Evaluate forgetting'''\n",
    "    print(\"Task: \", agent.state[\"task\"])\n",
    "    accuracy = []\n",
    "    losses = []\n",
    "    for i in range(agent.state[\"task\"]-1):\n",
    "        print(\"Task forgetting on task \", i+1)\n",
    "        for data, target in agent.state[\"ts_loader\"][i]:\n",
    "          logits = agent.classifier_model(data)\n",
    "          pred = np.argmax(logits, axis=1)\n",
    "          report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "          loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "          accuracy.append(report)\n",
    "          losses.append(np.mean(loss))\n",
    "\n",
    "        print(\"Mean accuracy: \", np.mean(accuracy))\n",
    "        print(\"Mean loss: \", np.mean(losses))\n",
    "\n",
    "    #print(\"MIR success rate: \", agent.state[\"mir_success\"] / agent.state[\"mir_tries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VhbA5MCFCi4r"
   },
   "outputs": [],
   "source": [
    "'''Run the experiment'''\n",
    "\n",
    "def run(agent):\n",
    "\n",
    "  agent.set_models(\n",
    "      classifier=classifier.classifier(agent.params),\n",
    "      generator=stable_diffusion.StableDiffusion(img_height=32, img_width=32, download_weights=True))\n",
    "\n",
    "  for task, tr_loader in enumerate(train_loader):\n",
    "    agent.state[\"task\"] = task\n",
    "    agent.state[\"tr_loader\"] = tr_loader\n",
    "    run_task(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing for development"
   ],
   "metadata": {
    "collapsed": false,
    "id": "1ltdwQCB8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier init\n",
      "StableDiffusion init\n",
      "UNetModel init\n",
      "Encoder init\n"
     ]
    }
   ],
   "source": [
    "tasks_to_test = 3\n",
    "\n",
    "agent = Agent(params)\n",
    "agent.set_models(\n",
    "    classifier=classifier.classifier(agent.params),\n",
    "    generator=stable_diffusion.StableDiffusion(img_height=32, img_width=32, download_weights=False))"
   ],
   "metadata": {
    "id": "MnyM145b8kVn",
    "outputId": "a1c06caa-9014-4d8a-9581-b1c0b91d8519",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent.set_params(params)\n",
    "for task, (tr_loader, ts_loader) in enumerate(zip(train_loader[:tasks_to_test],test_loader[:tasks_to_test])):\n",
    "    agent.state[\"task\"] = task\n",
    "    agent.state[\"tr_loader\"] = tr_loader\n",
    "    agent.state[\"ts_loader\"] = ts_loader\n",
    "    run_task(agent)"
   ],
   "metadata": {
    "id": "bvKaDQ-xRaSw",
    "outputId": "e0132437-dcf4-40ae-c78e-adf4ff7e2fd0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "execution_count": 53,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task  0\n",
      "Running epoch on classifier  0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [05:05,  5.18s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.56it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.7877089665653496\n",
      "Mean loss:  0.54069996\n",
      "Running epoch on classifier  1\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [05:03,  5.15s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  1\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.67it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.756126519756839\n",
      "Mean loss:  0.6417045\n",
      "Running epoch on classifier  2\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [05:10,  5.27s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  2\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.45it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.7916983282674772\n",
      "Mean loss:  0.54620624\n",
      "Running epoch on classifier  3\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [05:08,  5.22s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  3\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.68it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.803713905775076\n",
      "Mean loss:  0.4950199\n",
      "Running epoch on classifier  4\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [05:07,  5.21s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  4\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.57it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.8168218085106383\n",
      "Mean loss:  0.5342148\n",
      "Running epoch on classifier  5\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [04:59,  5.07s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  5\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.29it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.3333966565349544\n",
      "Mean loss:  1.133703\n",
      "Running epoch on classifier  6\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [04:52,  4.96s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  6\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.41it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.3333966565349544\n",
      "Mean loss:  1.1128514\n",
      "Running epoch on classifier  7\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [04:49,  4.90s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  7\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:08,  5.41it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.333254179331307\n",
      "Mean loss:  1.1083696\n",
      "Running epoch on classifier  8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "59it [04:54,  5.00s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "47it [00:08,  5.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean accuracy:  0.333254179331307\n",
      "Mean loss:  1.1065516\n",
      "Running epoch on classifier  9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "59it [04:59,  5.08s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluate classifier on Task:  0  Epoch:  9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "47it [00:08,  5.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean accuracy:  0.333254179331307\n",
      "Mean loss:  1.1046727\n",
      "Running generator epoch  0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "58it [01:01,  1.06s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluate generator on Task:  0  Epoch:  0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-53-af1f9e6ad3cd>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"tr_loader\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtr_loader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"ts_loader\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mts_loader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mrun_task\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-19-dcda6f3bead7>\u001B[0m in \u001B[0;36mrun_task\u001B[0;34m(agent)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"gen_epochs\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"epoch\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mrun_gen_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;34m'''Evaluate forgetting'''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-18-04a9b8db5ab4>\u001B[0m in \u001B[0;36mrun_gen_epoch\u001B[0;34m(agent)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m           \u001B[0;34m'''Evaluate the generator'''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m           \u001B[0mmem_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_latent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m           \u001B[0mmem_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmem_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m           \u001B[0mmem_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcategorical_crossentropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmem_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmem_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-e82939e306f6>\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(agent, cls, input_latent, train, coeff)\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt_gen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiffusion_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m             e_t = agent.gen.get_model_output(\n\u001B[0m\u001B[1;32m     30\u001B[0m                 \u001B[0mlatent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m                 \u001B[0mtimestep\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/stable_diffusion/stable_diffusion.py\u001B[0m in \u001B[0;36mget_model_output\u001B[0;34m(self, latent, timestep, batch_size)\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0mt_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimestep_embedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimesteps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0mt_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt_emb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrepeats\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mlatent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiffusion_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlatent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt_emb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlatent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/stable_diffusion/diffusion_model.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0memb_out\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0memb_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0memb_out\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mskip_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Exception encountered when calling layer 'res_block_108' (type ResBlock).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer 'res_block_108' (type ResBlock):\n  • inputs=['tf.Tensor(shape=(64, 4, 4, 320), dtype=float32)', 'tf.Tensor(shape=(256, 1280), dtype=float32)']"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "agent.cls.save('classifier.h5')\n",
    "agent.encoder.save('encoder.h5')"
   ],
   "metadata": {
    "id": "OhTpuhB3Z1ke"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "''' Evaluation'''\n",
    "for task, loader in enumerate(test_loader[:tasks_to_test]):\n",
    "    print(\"Task: \", task)\n",
    "    LOSS = []\n",
    "    ACC = []\n",
    "    for data, target in loader:\n",
    "      logits = agent.classifier_model(data)\n",
    "      pred = np.argmax(logits, axis=1)\n",
    "      report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "      loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "      #print(report)\n",
    "      ACC.append(report)\n",
    "      LOSS.append(loss)\n",
    "    print(\"Mean loss: \", np.mean(LOSS))\n",
    "    print(\"Mean accuracy: \", np.mean(ACC))\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "id": "MlEAyeVoFfWy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AoBLwork8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlPHqKTaC41u"
   },
   "outputs": [],
   "source": [
    "agent = Agent(params)\n",
    "for r in range(agent.params[\"n_runs\"]):\n",
    "  agent.state[\"run\"] = r\n",
    "  run(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation, testing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "UiiKIl618kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(loader, first_n_tasks=None):\n",
    "    for task, tr_loader in enumerate(loader):\n",
    "        print(\"Task: \", task)\n",
    "        data, target = tr_loader.batch(124)\n",
    "        logits = agent.classifier_model(data)\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        report = agent.eval(np.argmax(target, axis=1), pred)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
    "        print(report)\n",
    "        print(\"Mean loss: \", np.mean(loss))"
   ],
   "metadata": {
    "id": "r_K9lbNh8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I2ZLTm-JnX8"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluation on training set:\")\n",
    "evaluate(train_loader)\n",
    "print(\"Evaluation on test set:\")\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils for development"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vOxdRLeJ8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'stable_diffusion.stable_diffusion' from '/Users/laszlofreund/PycharmProjects/continual-learning-ait/stable_diffusion/stable_diffusion.py'>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload modules\n",
    "importlib.reload(stable_diffusion)"
   ],
   "metadata": {
    "id": "LNWS0pbd8kVn",
    "outputId": "b2184f26-86ef-40da-d970-7bd8b2d283ff"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "21547"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collection\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "NCdIwGMq8kVn",
    "outputId": "2495022b-fa19-489f-ae37-79b95d3ae24a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "YvU86CThdnOY"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
