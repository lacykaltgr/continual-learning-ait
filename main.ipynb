{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pveLpmUzEF5A",
    "outputId": "f64c860c-89fc-4c0c-a53f-877e679e343f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-05-16 23:12:39--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main [following]\n",
      "--2023-05-16 23:12:39--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main\n",
      "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
      "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1803102 (1.7M) [application/zip]\n",
      "Saving to: ‘main.zip’\n",
      "\n",
      "main.zip            100%[===================>]   1.72M  1.22MB/s    in 1.4s    \n",
      "\n",
      "2023-05-16 23:12:41 (1.22 MB/s) - ‘main.zip’ saved [1803102/1803102]\n",
      "\n",
      "Archive:  main.zip\n",
      "4861ac23fcd29f49a6d3986cc09d5dcec72bb6b0\n",
      "   creating: continual-learning-ait-main/\n",
      "  inflating: continual-learning-ait-main/.gitattributes  \n",
      "  inflating: continual-learning-ait-main/README.md  \n",
      "  inflating: continual-learning-ait-main/classifier.py  \n",
      "  inflating: continual-learning-ait-main/data_preparation.py  \n",
      "   creating: continual-learning-ait-main/dnnlib/\n",
      "  inflating: continual-learning-ait-main/dnnlib/__init__.py  \n",
      "  inflating: continual-learning-ait-main/dnnlib/util.py  \n",
      "  inflating: continual-learning-ait-main/generator.py  \n",
      "   creating: continual-learning-ait-main/guided_diffusion/\n",
      "  inflating: continual-learning-ait-main/guided_diffusion/__init__.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/dist_util.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/fp16_util.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/gaussian_diffusion.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/logger.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/losses.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/nn.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/resample.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/respace.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/script_util.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/train_util.py  \n",
      "  inflating: continual-learning-ait-main/guided_diffusion/unet.py  \n",
      "  inflating: continual-learning-ait-main/img.png  \n",
      "  inflating: continual-learning-ait-main/main.ipynb  \n",
      "   creating: continual-learning-ait-main/models/\n",
      "  inflating: continual-learning-ait-main/models/classifier.h5  \n",
      "  inflating: continual-learning-ait-main/models/encoder.h5  \n",
      "   creating: continual-learning-ait-main/notebooks/\n",
      "  inflating: continual-learning-ait-main/notebooks/classifier.ipynb  \n",
      "  inflating: continual-learning-ait-main/notebooks/data_preparation.ipynb  \n",
      "  inflating: continual-learning-ait-main/notebooks/generator.ipynb  \n",
      "   creating: continual-learning-ait-main/torch_utils/\n",
      "  inflating: continual-learning-ait-main/torch_utils/__init__.py  \n",
      "  inflating: continual-learning-ait-main/torch_utils/distributed.py  \n",
      "  inflating: continual-learning-ait-main/torch_utils/misc.py  \n",
      "  inflating: continual-learning-ait-main/torch_utils/persistence.py  \n",
      "  inflating: continual-learning-ait-main/torch_utils/training_stats.py  \n",
      "  inflating: continual-learning-ait-main/utils.py  \n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "'''Download the files '''\n",
    "'''Only for colab'''\n",
    "useColab = True\n",
    "useDrive = True\n",
    "\n",
    "if useColab:\n",
    "    !wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
    "    !unzip main.zip\n",
    "    #!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;\n",
    "    !cd continual-learning-ait-main\n",
    "\n",
    "    import sys\n",
    "    sys.path.append('/content/continual-learning-ait-main')\n",
    "\n",
    "    if useDrive:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        encoder_f = '/drive/MyDrive/continual-learning-ait/checkpoints/32x32_classifier.pt'\n",
    "        scorenet_f = 'drive/MyDrive/continual-learning-ait/checkpoints/edm-cifar10-32x32-cond-vp.pkl'\n",
    "    #else:\n",
    "        #!wget https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n",
    "\n",
    "       #!wget https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/32x32_classifier.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vjfVuEmXECoL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import classification_report\n",
    "#from keras.metrics import Accuracy\n",
    "\n",
    "from generator import Generator\n",
    "from classifier import Classifier\n",
    "import utils\n",
    "from data_preparation import load_dataset, RealFakeConditionalDataLoader\n",
    "\n",
    "import gc\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zzYWiWfZ8kVi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TaH3bi-5crqD",
    "outputId": "cbfa9371-fd0c-4817-8690-a6cddfd2ed64",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dpt_train, dpt_test = load_dataset('cifar-10', n_classes_first_task=4, n_classes_other_task=3)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def disp(image):\n",
    "    image = (image + 1)* 127.5\n",
    "\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "wN4zXqramJOX"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define parameters and agent"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3Jl-cBYs8kVj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GY9MAk581iYQ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    #general\n",
    "    \"n_runs\": 1,\n",
    "    \"n_tasks\": 3,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_shape\": (32, 32, 3),\n",
    "    \"batch_size\": 256,\n",
    "    \"print_every\": 1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"img_resolution\": 32,\n",
    "    \"classes_learned\": [4, 7, 10],\n",
    "\n",
    "    #classifier\n",
    "    \"cls_iters\": 1,\n",
    "    \"cls_lr\": 1e-2,\n",
    "    \"cls_epochs\": 15,\n",
    "\n",
    "    #generator\n",
    "    \"gen_epochs\": 3,\n",
    "    \"gen_lr\": 5e-3,\n",
    "    \"gen_scaler\": lambda x: 2. * x - 1.,\n",
    "\n",
    "    #mir\n",
    "    \"n_mem\": 1,\n",
    "    \"mir_iters\": 1,\n",
    "    \"reuse_samples\": False,\n",
    "    \"mem_coeff\": 0.12,\n",
    "\n",
    "    \"z_size\": 10,\n",
    "    \"xent_coeff\": 0.25,\n",
    "    \"ent_coeff\": 0.25,\n",
    "    \"div_coeff\": 0.25,\n",
    "    \"shell_coeff\": 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OmitFSMM2WJn"
   },
   "outputs": [],
   "source": [
    "'''Agent to handle models, parameters and states'''\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, hparams):\n",
    "    self.params = hparams\n",
    "    self.state = dict()\n",
    "    self.eval = dict()\n",
    "    self.score = accuracy_score\n",
    "\n",
    "    self.state[\"tasks_learned\"] = 0\n",
    "    self.eval[\"cls_loss\"] = []\n",
    "    self.eval[\"cls_acc\"] = []\n",
    "    self.eval[\"discriminator_loss\"] = []\n",
    "    self.eval[\"correction_rate\"] = []\n",
    "\n",
    "    self.classifier = None\n",
    "    self.generator = None\n",
    "    self.optimizer = None\n",
    "    self.optimizer_gen = None\n",
    "    self.loss = None\n",
    "    self.loss_gen = None\n",
    "\n",
    "\n",
    "\n",
    "  def set_models(\n",
    "          self,\n",
    "          _generator=None,\n",
    "          _classifier=None,\n",
    "    ):\n",
    "    cls = _classifier #classifier\n",
    "    gen = _generator  #generator\n",
    "\n",
    "    # losses\n",
    "    self.loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    self.loss_gen = torch.nn.BCELoss()\n",
    "\n",
    "    # optimizers\n",
    "    self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"])\n",
    "    self.optimizer_gen = torch.optim.Adam(gen.discriminator.parameters(), lr=agent.params[\"gen_lr\"], weight_decay=1e-7)\n",
    "\n",
    "    # classifier pipeline\n",
    "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
    "    cls_output = cls(data_input)\n",
    "    self.classifier = keras.Model(inputs=data_input, outputs=cls_output)\n",
    "    self.classifier.compile(optimizer=self.optimizer, loss=self.loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    # generator pipeline\n",
    "    self.generator = gen"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OPqiS9VV8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model at the same time'''\n",
    "\n",
    "def generate(\n",
    "        agent,\n",
    "        num_samples,\n",
    "        dg_weight_1st_order=1,\n",
    "        dg_weight_2nd_order=0,\n",
    "        boosting = 1,\n",
    "        time_min= 0.01, time_max= 1,\n",
    "        class_idx=None,\n",
    "):\n",
    "    batch_size = agent.params[\"batch_size\"]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    generated_images = []\n",
    "    class_labels = []\n",
    "    for batch in range(num_batches):\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_latents = torch.randn(\n",
    "            [end_idx - start_idx, agent.generator.net.img_channels, agent.params[\"img_resolution\"], agent.params[\"img_resolution\"]],\n",
    "            device=agent.params['device']\n",
    "        )\n",
    "\n",
    "        batch_class_labels = torch.eye(agent.params['n_classes'], device=agent.params['device'])[\n",
    "            torch.randint(agent.params['classes_learned'][agent.state[\"tasks_learned\"]-1] -1, size=[end_idx - start_idx], device=agent.params['device'])\n",
    "        ]\n",
    "        if class_idx is not None:\n",
    "            batch_class_labels[:, :] = 0\n",
    "            batch_class_labels[:, class_idx] = 1\n",
    "\n",
    "\n",
    "        batch_images = agent.generator.sample(\n",
    "            boosting, time_min, time_max, dg_weight_1st_order, dg_weight_2nd_order, batch_latents, batch_class_labels\n",
    "        )\n",
    "        generated_images.append(batch_images)\n",
    "        class_labels.append(batch_class_labels)\n",
    "\n",
    "    images = torch.cat(generated_images, dim=0)\n",
    "    class_labels = torch.cat(class_labels, dim=0)\n",
    "    \n",
    "    '''\n",
    "    latents = torch.randn([num_samples, agent.generator.net.img_channels, agent.params[\"img_resolution\"], agent.params[\"img_resolution\"]], device=agent.params['device'])\n",
    "    \n",
    "    class_labels = torch.eye(agent.params['n_classes'], device=agent.params['device'])[torch.randint(agent.params['classes_learned'][agent.state[\"tasks_learned\"]]-1, size=[num_samples], device=agent.params['device'])]\n",
    "    if class_idx is not None:\n",
    "        class_labels[:, :] = 0\n",
    "        class_labels[:, class_idx] = 1\n",
    "\n",
    "\n",
    "    images = agent.generator.sample(boosting, time_min, time_max, dg_weight_1st_order, dg_weight_2nd_order, latents, class_labels)\n",
    "    '''\n",
    "\n",
    "    # turn (3 32 32) into (32 32 3)\n",
    "    images = torch.transpose(images, 1, 3)\n",
    "    images = torch.transpose(images, 1, 2)\n",
    "\n",
    "    images, class_labels = \\\n",
    "        tf.convert_to_tensor(images.cpu().numpy()), \\\n",
    "            tf.convert_to_tensor(class_labels.cpu().numpy())\n",
    "    torch.cuda.empty_cache()\n",
    "    return images, class_labels\n"
   ],
   "metadata": {
    "id": "2BgWNzhg8kVk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector for classifier'''\n",
    "\n",
    "def retrieve_mir(agent):\n",
    "\n",
    "    virtual_cls = Classifier()\n",
    "    virtual_cls = utils.get_next_step_cls(\n",
    "        agent.classifier,\n",
    "        virtual_cls,\n",
    "        agent.state[\"task_train_samples\"],\n",
    "        agent.state[\"task_train_targets\"]\n",
    "    )\n",
    "\n",
    "    final_generated = None\n",
    "    final_labels = None\n",
    "    for i in range(agent.params[\"n_mem\"]):\n",
    "\n",
    "        generated, labels = generate(agent, agent.params[\"batch_size\"]*20)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                tape.watch(generated)\n",
    "\n",
    "                y_pre = agent.classifier(generated)\n",
    "                y_virtual = virtual_cls(generated)\n",
    "\n",
    "                # maximise the interference:\n",
    "                XENT = tf.constant(0.)\n",
    "                if params[\"xent_coeff\"] > 0.:\n",
    "                    XENT = tf.keras.losses.categorical_crossentropy(y_virtual, y_pre)\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = tf.constant(0.)\n",
    "                if params[\"ent_coeff\"] > 0.:\n",
    "                    ENT = tf.keras.losses.categorical_crossentropy(y_pre, y_pre)\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = tf.constant(0.)\n",
    "                if params[\"div_coeff\"] > 0.:\n",
    "                    for found_generated in range(i):\n",
    "                        DIV += tf.cast(tf.keras.losses.MSE(\n",
    "                            generated,\n",
    "                            final_generated[found_generated * generated.shape[0]:found_generated * generated.shape[0] + generated.shape[0]]\n",
    "                        ) / i, tf.float32)\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = tf.constant(0.)\n",
    "                if params[\"shell_coeff\"] > 0.:\n",
    "                    SHELL = tf.keras.losses.MSE(\n",
    "                        tf.norm(generated, axis=1),\n",
    "                        tf.ones_like(tf.norm(generated, axis=1))*np.sqrt(params[\"z_size\"])\n",
    "                    )\n",
    "\n",
    "                XENT, ENT, DIV, SHELL = \\\n",
    "                    tf.cast(tf.reduce_mean(XENT), dtype=tf.float64), \\\n",
    "                    tf.cast(tf.reduce_mean(ENT), dtype=tf.float64), \\\n",
    "                    tf.cast(tf.reduce_mean(DIV), dtype=tf.float64), \\\n",
    "                    tf.cast(tf.reduce_mean(SHELL), dtype=tf.float64)\n",
    "\n",
    "                gain = params[\"xent_coeff\"] * XENT + \\\n",
    "                       -params[\"ent_coeff\"] * ENT + \\\n",
    "                       params[\"div_coeff\"] * DIV + \\\n",
    "                       -params[\"shell_coeff\"] * SHELL\n",
    "\n",
    "            gen_grad = tape.gradient(gain, generated)\n",
    "            if gen_grad is not None:\n",
    "                generated = (generated + 1 * gen_grad)\n",
    "\n",
    "        if final_generated is None:\n",
    "            final_generated = generated.numpy().copy()\n",
    "            final_labels = labels.numpy().copy()\n",
    "        else:\n",
    "            final_generated = np.concatenate([final_generated, generated.numpy().copy()])\n",
    "            final_labels = np.concatenate([final_labels, labels.numpy().copy()])\n",
    "\n",
    "    tf.stop_gradient(final_generated)\n",
    "\n",
    "    return final_generated, final_labels"
   ],
   "metadata": {
    "id": "OlENSPig8kVl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "'''Run an epoch'''\n",
    "\n",
    "def train_classifier(agent):\n",
    "\n",
    "    if (agent.state[\"task\"]>0):\n",
    "        mem_x, mem_y = generate(agent, agent.params[\"batch_size\"]*30)\n",
    "        print(\"Sample of retrieved images:\")\n",
    "        disp(mem_x[0])\n",
    "        print(mem_y[0], \"\\n\")\n",
    "        train_data = np.concatenate((agent.state[\"task_train_samples\"], mem_x))\n",
    "        train_target = np.concatenate((agent.state[\"task_train_targets\"], mem_y))\n",
    "    else:\n",
    "        train_data, train_target = agent.state[\"task_train_samples\"], agent.state[\"task_train_targets\"]\n",
    "\n",
    "      \n",
    "    classifier_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_target)).shuffle(len(train_data)).batch(agent.params[\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    history = agent.classifier.fit(classifier_dataset, batch_size=agent.params[\"batch_size\"], epochs=agent.params[\"cls_epochs\"], verbose=1)\n",
    "\n",
    "    agent.eval[\"cls_loss\"].append(history.history[\"loss\"])\n",
    "    agent.eval[\"cls_acc\"].append(history.history[\"accuracy\"])\n",
    "\n",
    "    '''Evaluate the models in epoch'''\n",
    "    print(f\"Classifier loss: {np.mean(agent.eval['cls_loss'][-1])}\"\n",
    "          f\"    Classifier accuracy: {np.mean(agent.eval['cls_acc'][-1])}\")"
   ],
   "metadata": {
    "id": "xmJWM9GCzym_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''Train the discriminator unit'''\n",
    "\n",
    "def train_generator(agent):\n",
    "\n",
    "    NUM_SAMPLES = int(len(agent.state[\"task_train_samples\"])/2)\n",
    "    real_samples, real_labels = \\\n",
    "      agent.state[\"task_train_samples\"][:NUM_SAMPLES], agent.state[\"task_train_targets\"][:NUM_SAMPLES]\n",
    "    fake_samples, fake_labels = generate(\n",
    "        agent, NUM_SAMPLES, dg_weight_1st_order=0, dg_weight_2nd_order=0, boosting=0)\n",
    "\n",
    "    train_data = np.concatenate((real_samples, fake_samples))\n",
    "    train_label = torch.zeros(train_data.shape[0])\n",
    "    train_label[:real_samples.shape[0]] = 1.\n",
    "    condition_label = np.concatenate((real_labels, fake_labels))\n",
    "\n",
    "    real_fake_loader = RealFakeConditionalDataLoader(train_data, train_label, condition_label)\n",
    "    for epoch in range(agent.params[\"gen_epochs\"]):\n",
    "        outs = []\n",
    "        cors = []\n",
    "        for data in real_fake_loader:\n",
    "            agent.optimizer_gen.zero_grad()\n",
    "\n",
    "            inputs, labels, cond = data\n",
    "            cond = cond.to(agent.params[\"device\"])\n",
    "            inputs = inputs.to(agent.params[\"device\"])\n",
    "            labels = labels.to(agent.params[\"device\"])\n",
    "            inputs = agent.params[\"gen_scaler\"](inputs)\n",
    "\n",
    "            # Data perturbation\n",
    "            t, _ = agent.generator.vpsde.get_diffusion_time(inputs.shape[0], inputs.device)\n",
    "            mean, std = agent.generator.vpsde.marginal_prob(t)\n",
    "            z = torch.randn_like(inputs)\n",
    "            perturbed_inputs = mean[:, None, None, None] * inputs + std[:, None, None, None] * z\n",
    "\n",
    "            # Forward\n",
    "            with torch.no_grad():\n",
    "                pretrained_feature = agent.generator.encoder(perturbed_inputs, timesteps=t, feature=True).type(torch.cuda.FloatTensor)\n",
    "            label_prediction = agent.generator.discriminator(pretrained_feature, t, sigmoid=True, condition=cond).view(-1)\n",
    "\n",
    "            # Backward\n",
    "            out = agent.loss_gen(label_prediction, labels)\n",
    "            out.backward()\n",
    "            agent.optimizer_gen.step()\n",
    "\n",
    "            # Report\n",
    "            cor = ((label_prediction > 0.5).float() == labels).float().mean()\n",
    "            outs.append(out.item())\n",
    "            cors.append(cor.item())\n",
    "            torch.cuda.empty_cache()\n",
    "        agent.eval[\"correction_rate\"][-1].append(np.mean(cors))\n",
    "        agent.eval[\"discriminator_loss\"][-1].append(np.mean(outs))\n",
    "        \n",
    "\n",
    "    '''Evaluate the models in epoch'''\n",
    "    print(f\"Discriminator loss: {np.mean(agent.eval['discriminator_loss'][-1])}\"\n",
    "          f\"    Correction rate: {np.mean(agent.eval['correction_rate'][-1])}\")"
   ],
   "metadata": {
    "id": "K48EQq7VGuI-"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0SyPNSep_K0t"
   },
   "outputs": [],
   "source": [
    "'''Run a task'''\n",
    "\n",
    "def run_task(agent):\n",
    "\n",
    "    print(f\"\\n#############\\n\"\n",
    "          f\"  TASK {agent.state['task']}\\n\"\n",
    "          f\"#############\\n\")\n",
    "\n",
    "    agent.eval[\"discriminator_loss\"].append([])\n",
    "    agent.eval[\"correction_rate\"].append([])\n",
    "  \n",
    "    train_classifier(agent)\n",
    "\n",
    "    agent.state[\"tasks_learned\"] += 1\n",
    "    print(\"\\n\")\n",
    "\n",
    "    train_generator(agent)\n",
    "\n",
    "    '''Evaluate forgetting'''\n",
    "    print(\"\\nEvaluate Task: \", agent.state[\"task\"])\n",
    "    for i in range(agent.state[\"task\"]+1):\n",
    "        data, target = dpt_test[i]\n",
    "        logits = agent.classifier.predict(data, batch_size=256)\n",
    "        #loss\n",
    "        loss = agent.loss(logits, target)\n",
    "        #accuracy\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        y = np.argmax(target, axis=1)\n",
    "        accuracy = agent.score(y, pred)\n",
    "        print(f\"    Forgetting after Task {agent.state['task']} on task {i} : \"\n",
    "              f\"        Loss: {np.mean(loss)}\"\n",
    "              f\"        ACC: {np.mean(accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VhbA5MCFCi4r"
   },
   "outputs": [],
   "source": [
    "'''Run the experiment'''\n",
    "\n",
    "def run(agent):\n",
    "    for r in range(agent.params[\"n_runs\"]):\n",
    "        agent.state[\"run\"] = r\n",
    "        for task, (tr_loader, ts_loader) in enumerate(zip(dpt_train, dpt_test)):\n",
    "            agent.state[\"task\"] = task\n",
    "            agent.state[\"task_train_samples\"], agent.state[\"task_train_targets\"]  = tr_loader\n",
    "            agent.state[\"task_test_samples\"], agent.state[\"task_test_targets\"] = ts_loader\n",
    "            run_task(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AoBLwork8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BlPHqKTaC41u"
   },
   "outputs": [],
   "source": [
    "agent = Agent(params)\n",
    "agent.set_models(\n",
    "    _classifier=Classifier(),\n",
    "    _generator=Generator(encoder_path=encoder_f, scorenet_path=scorenet_f, device=agent.params[\"device\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "run(agent)"
   ],
   "metadata": {
    "id": "jL7eBmbM33Nw",
    "outputId": "af0bdb38-28a6-4e81-e22d-1ad3e0eac928",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "#############\n",
      "  TASK 0\n",
      "#############\n",
      "\n",
      "Epoch 1/15\n",
      "79/79 [==============================] - 2s 12ms/step - loss: 1.0399 - accuracy: 0.6119\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.6576 - accuracy: 0.7388\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.5682 - accuracy: 0.7750\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.4893 - accuracy: 0.8129\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.4194 - accuracy: 0.8396\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3833 - accuracy: 0.8565\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3549 - accuracy: 0.8662\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3207 - accuracy: 0.8796\n",
      "Epoch 9/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2876 - accuracy: 0.8917\n",
      "Epoch 10/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2791 - accuracy: 0.8958\n",
      "Epoch 11/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2422 - accuracy: 0.9100\n",
      "Epoch 12/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2612 - accuracy: 0.9039\n",
      "Epoch 13/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2087 - accuracy: 0.9237\n",
      "Epoch 14/15\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.1880 - accuracy: 0.9312\n",
      "Epoch 15/15\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2003 - accuracy: 0.9247\n",
      "Classifier loss: 0.39336377680301665    Classifier accuracy: 0.8507633368174236\n",
      "\n",
      "\n",
      "Discriminator loss: 0.5964191314719972    Correction rate: 0.7085084035283044\n",
      "\n",
      "Evaluate Task:  0\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 0 on task 0 :         Loss: 2.1177115440368652        ACC: 0.887\n",
      "\n",
      "#############\n",
      "  TASK 1\n",
      "#############\n",
      "\n",
      "Epoch 1/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.6715 - accuracy: 0.3839\n",
      "Epoch 2/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.0419 - accuracy: 0.5991\n",
      "Epoch 3/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.8126 - accuracy: 0.7203\n",
      "Epoch 4/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.6763 - accuracy: 0.7743\n",
      "Epoch 5/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5836 - accuracy: 0.8088\n",
      "Epoch 6/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5074 - accuracy: 0.8299\n",
      "Epoch 7/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.4554 - accuracy: 0.8504\n",
      "Epoch 8/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.3828 - accuracy: 0.8741\n",
      "Epoch 9/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.3600 - accuracy: 0.8828\n",
      "Epoch 10/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.3307 - accuracy: 0.8902\n",
      "Epoch 11/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.2975 - accuracy: 0.9047\n",
      "Epoch 12/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.2846 - accuracy: 0.9071\n",
      "Epoch 13/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.2676 - accuracy: 0.9124\n",
      "Epoch 14/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.2467 - accuracy: 0.9186\n",
      "Epoch 15/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.2379 - accuracy: 0.9213\n",
      "Classifier loss: 0.5437598983446758    Classifier accuracy: 0.8118547916412353\n",
      "\n",
      "\n",
      "Discriminator loss: 0.6470563821494579    Correction rate: 0.5840569299956163\n",
      "\n",
      "Evaluate Task:  1\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 1 on task 0 :         Loss: 6.708626747131348        ACC: 0.5925\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 1 on task 1 :         Loss: 1.9520771503448486        ACC: 0.903\n",
      "\n",
      "#############\n",
      "  TASK 2\n",
      "#############\n",
      "\n",
      "Epoch 1/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.2958 - accuracy: 0.3319\n",
      "Epoch 2/15\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.2625 - accuracy: 0.5648\n",
      "Epoch 3/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.0808 - accuracy: 0.6191\n",
      "Epoch 4/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.9649 - accuracy: 0.6560\n",
      "Epoch 5/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.8749 - accuracy: 0.6860\n",
      "Epoch 6/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.7943 - accuracy: 0.7195\n",
      "Epoch 7/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.7321 - accuracy: 0.7425\n",
      "Epoch 8/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.6682 - accuracy: 0.7607\n",
      "Epoch 9/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.6239 - accuracy: 0.7773\n",
      "Epoch 10/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5927 - accuracy: 0.7892\n",
      "Epoch 11/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5626 - accuracy: 0.7947\n",
      "Epoch 12/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5433 - accuracy: 0.8035\n",
      "Epoch 13/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.5187 - accuracy: 0.8169\n",
      "Epoch 14/15\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 0.4840 - accuracy: 0.8281\n",
      "Epoch 15/15\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.4567 - accuracy: 0.8385\n",
      "Classifier loss: 0.8303534011046092    Classifier accuracy: 0.7152469078699748\n",
      "\n",
      "\n",
      "Discriminator loss: 0.659410406381656    Correction rate: 0.6140293494249002\n",
      "\n",
      "Evaluate Task:  2\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 2 on task 0 :         Loss: 7.551288604736328        ACC: 0.58575\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 2 on task 1 :         Loss: 11.527849197387695        ACC: 0.359\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "    Forgetting after Task 2 on task 2 :         Loss: 1.4050257205963135        ACC: 0.9386666666666666\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training analysis"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vOxdRLeJ8kVn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_history(data, title=\"Plot\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(data, label=title)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#analysis per task\n",
    "for task in range(agent.params['n_tasks']):\n",
    "    plot_history(agent.eval[\"cls_loss\"][task], title=f\"Task {task} Classifier Loss\")\n",
    "    plot_history(agent.eval[\"cls_acc\"][task], title=f\"Task {task} Classifier Accuracy\")\n",
    "    plot_history(agent.eval[\"discriminator_loss\"][task], title=f\"Task {task} Discriminator Loss\")\n",
    "    plot_history(agent.eval[\"correction_rate\"][task], title=f\"Task {task} Correction Rate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#full analysis\n",
    "plot_history(np.concatenate(agent.eval[\"cls_loss\"]), title=f\"Classifier Loss\")\n",
    "plot_history(np.concatenate(agent.eval[\"cls_acc\"]), title=f\"Classifier Accuracy\")\n",
    "plot_history(np.concatenate(agent.eval[\"discriminator_loss\"]), title=f\"Discriminator Loss\")\n",
    "plot_history(np.concatenate(agent.eval[\"correction_rate\"]), title=f\"Correction Rate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(10):\n",
    "    images.append(generate(agent, 2, dg_weight_1st_order=0, dg_weight_2nd_order=0, boosting=0, class_idx=i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    disp(image[0])\n",
    "    print(image[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation, testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from data_preparation import load_cifar_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test, eval_name=\"\"):\n",
    "    y_pred = agent.classifier.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    print(f\"Classification Report on {eval_name}:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes))\n",
    "    print(f\"Confusion Matrix on {eval_name}:\")\n",
    "    confusion_matrix_plot(y_true_classes, y_pred_classes)\n",
    "    print(f\"Accuracy on {eval_name}: \", agent.score(y_true_classes, y_pred_classes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_eval():\n",
    "    (_, _), (X_test, Y_test) = load_cifar_10()\n",
    "    evaluate(X_test, Y_test, eval_name=\"TEST SET\")\n",
    "\n",
    "    for task, (x_test, y_test) in enumerate(dpt_test):\n",
    "        evaluate(x_test, y_test, eval_name=f\"TASK {task}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
