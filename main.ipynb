{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pveLpmUzEF5A",
    "outputId": "9d696bb4-268a-40f1-e28c-81c589c79419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-11 14:57:07--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main [following]\n",
      "--2023-04-11 14:57:07--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘main.zip’\n",
      "\n",
      "main.zip                [ <=>                ] 154.08K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-04-11 14:57:07 (3.06 MB/s) - ‘main.zip’ saved [157773]\n",
      "\n",
      "Archive:  main.zip\n",
      "9d267e6b95e57ad02d2e58c6e1d1f16dcffdefcb\n",
      "   creating: continual-learning-ait-main/\n",
      "  inflating: continual-learning-ait-main/LICENSE  \n",
      "  inflating: continual-learning-ait-main/README.md  \n",
      "  inflating: continual-learning-ait-main/classifier.py  \n",
      "  inflating: continual-learning-ait-main/data_preparation.ipynb  \n",
      "  inflating: continual-learning-ait-main/data_preparation.py  \n",
      "  inflating: continual-learning-ait-main/distributions.py  \n",
      "  inflating: continual-learning-ait-main/loss.py  \n",
      "  inflating: continual-learning-ait-main/main.ipynb  \n",
      "  inflating: continual-learning-ait-main/main.py  \n",
      "  inflating: continual-learning-ait-main/mir.py  \n",
      "  inflating: continual-learning-ait-main/requirements_m1.txt  \n",
      "   creating: continual-learning-ait-main/stable_diffusion/\n",
      "  inflating: continual-learning-ait-main/stable_diffusion/autoencoder_kl.py  \n",
      "  inflating: continual-learning-ait-main/stable_diffusion/diffusion_model.py  \n",
      "  inflating: continual-learning-ait-main/stable_diffusion/layers.py  \n",
      "  inflating: continual-learning-ait-main/stable_diffusion/stable_diffusion.py  \n",
      "  inflating: continual-learning-ait-main/utils.py  \n"
     ]
    }
   ],
   "source": [
    "import stable_diffusion.stable_diffusion\n",
    "# Download the files\n",
    "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
    "!unzip main.zip\n",
    "!find continual-learning-ait-main -type f ! -name \"main.ipynb\" ! -name \"main.py\" -exec cp {} . \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vjfVuEmXECoL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laszlofreund/miniconda/envs/cl/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "import classifier\n",
    "from stable_diffusion import stable_diffusion\n",
    "from utils import get_grad_vector, get_future_step_parameters\n",
    "from data_preparation import load_dataset, CLDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaH3bi-5crqD",
    "outputId": "e0efc0f3-ec94-496a-e91d-2f3b7bbacabb"
   },
   "outputs": [],
   "source": [
    "dpt_train, dpt_test = load_dataset('cifar-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yIESUurDeOCR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    }
   ],
   "source": [
    "train_loader = CLDataLoader(dpt_train, 64, train=True)\n",
    "test_loader = CLDataLoader(dpt_train, 64, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GY9MAk581iYQ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"device\": 'cuda:0' if tf.config.list_physical_devices('GPU') else 'cpu',\n",
    "    \"n_runs\": 1,\n",
    "    \"n_tasks\": 10,\n",
    "    \"n_epochs\": 100,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_shape\": (32,32, 3),\n",
    "    \"embedding_shape\": (4,4,4),\n",
    "    \"samples_per_task\": 100,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_steps\": 25,\n",
    "\n",
    "    \"gen_depth\": 6,\n",
    "    \"gen_steps\": 25,\n",
    "    \"input_latent_strength\":0.5,\n",
    "    \"gen_temperature\":1,\n",
    "    \"cls_mir_gen\": 1,\n",
    "    \"gen_mir_gen\": 1,\n",
    "    \"cls_iters\": 100,\n",
    "    \"cls_hiddens\": 32,\n",
    "    \"gen_iters\": 10,\n",
    "    \"loss_fn\": \"mse\",\n",
    "\n",
    "    \"lr\": 0.001,\n",
    "    \"warmup\": 0,\n",
    "    \"max_beta\": 1,\n",
    "    \"reuse_samples\": True,\n",
    "    \"print_every\": 5,\n",
    "    \"mem_coeff\": 0.12,\n",
    "    \"n_mem\": 10,\n",
    "    \"mir_init_prior\": 10,\n",
    "    \"z_size\": 10,\n",
    "    \"mir_iters\": 100,\n",
    "    \"gen_kl_coeff\": 0.5,\n",
    "    \"gen_rec_coeff\": 0.5,\n",
    "    \"gen_ent_coeff\": 0.5,\n",
    "    \"gen_div_coeff\": 0.5,\n",
    "    \"gen_shell_coeff\": 0.5,\n",
    "    \"cls_xent_coeff\": 0.5,\n",
    "    \"cls_ent_coeff\": 0.5,\n",
    "    \"cls_div_coeff\": 0.5,\n",
    "    \"cls_shell_coeff\": 0.5,\n",
    "    \"temperature\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OmitFSMM2WJn"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self, hparams):\n",
    "    self.cls = None\n",
    "    self.opt = None\n",
    "    self.opt_gen = None\n",
    "    self.gen = None\n",
    "    self.params = hparams\n",
    "    self.state = dict()\n",
    "    self.encoder = None\n",
    "    #self.decoder = None\n",
    "\n",
    "  def set_models(self, generator=None, classifier=None):\n",
    "    self.cls = classifier\n",
    "    self.gen = generator\n",
    "    self.encoder = generator.encoder\n",
    "    #self.decoder = gen.decoder\n",
    "    self.opt = tf.keras.optimizers.legacy.SGD(learning_rate=params[\"lr\"])\n",
    "    self.opt_gen = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "'''Generate samples and train the diffusion model the same time'''\n",
    "\n",
    "def generate(agent, classifier=None, input_latent=None, train=True, coeff=1.0):\n",
    "\n",
    "    if classifier is None:\n",
    "        classifier = agent.cls\n",
    "\n",
    "    latent, alphas, alphas_prev, timesteps = agent.gen.initialize(agent.params, input_latent)\n",
    "\n",
    "    progbar = tqdm(list(enumerate(timesteps))[::-1])\n",
    "    for index, timestep in progbar:\n",
    "        progbar.set_description(f'{index:3d} {timestep:3d}')\n",
    "        if train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                e_t = agent.gen.get_model_output(\n",
    "                    latent,\n",
    "                    timestep,\n",
    "                    agent.params['batch_size'],\n",
    "                )\n",
    "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "                latent = agent.gen.get_x_prev(latent, e_t,  a_t, a_prev, agent.params[\"temperature\"])\n",
    "\n",
    "                pred = classifier(latent)\n",
    "                #loss based on confidence\n",
    "                #ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
    "                loss = coeff*tf.keras.losses.categorical_crossentropy(pred, pred)\n",
    "                print(\"Generator loss: \", loss)\n",
    "            grads = tape.gradient(loss, agent.gen.diffusion_model.trainable_variables)\n",
    "            agent.opt_gen.apply_gradients(zip(grads, agent.gen.diffusion_model.trainable_variables))\n",
    "        else:\n",
    "            e_t = agent.gen.get_model_output(\n",
    "                latent,\n",
    "                timestep,\n",
    "                agent.params['batch_size'],\n",
    "            )\n",
    "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "            latent = agent.gen.get_x_prev(latent, e_t,  a_t, a_prev, agent.params[\"temperature\"])\n",
    "\n",
    "    return latent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "'''Retrive maximally interferred latent vector'''\n",
    "#TODO: gradient tape\n",
    "\n",
    "def retrieve_gen_for_cls(agent, latent):\n",
    "\n",
    "    grad_vector = get_grad_vector(agent.cls.trainable_variables, agent.cls.grad_dims)\n",
    "\n",
    "    virtual_cls = get_future_step_parameters(agent.cls, grad_vector, agent.cls.grad_dims, params)\n",
    "\n",
    "    mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    z_new_max = None\n",
    "\n",
    "    for i in range(agent.params[\"n_mem\"]):\n",
    "\n",
    "        z_new = generate(agent, input_latent=mean_latent, train=False, coeff=0.1)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                z_new = tf.cast(z_new, tf.float64)\n",
    "                y_pre = agent.cls(z_new)\n",
    "                y_virtual = virtual_cls(z_new)\n",
    "\n",
    "                # maximise the interference:\n",
    "                XENT = 0\n",
    "                if params[\"cls_xent_coeff\"] > 0.:\n",
    "                    XENT = tf.keras.losses.categorical_crossentropy(y_virtual, y_pre)\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = 0\n",
    "                if params[\"cls_ent_coeff\"] > 0.:\n",
    "                    ENT = tf.keras.losses.categorical_crossentropy(y_pre, y_pre)\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = 0\n",
    "                if params[\"cls_div_coeff\"] > 0.:\n",
    "                    for found_z_i in range(i):\n",
    "                        DIV += tf.keras.losses.MSE(\n",
    "                            z_new,\n",
    "                            z_new_max[found_z_i * z_new.shape[0]:found_z_i * z_new.shape[0] + z_new.shape[0]]\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = 0\n",
    "                if params[\"cls_shell_coeff\"] > 0.:\n",
    "                    SHELL = tf.keras.losses.MSE(\n",
    "                        tf.norm(z_new, axis=1),\n",
    "                        tf.ones_like(tf.norm(z_new, axis=1))*np.sqrt(params[\"z_size\"])\n",
    "                    )\n",
    "\n",
    "                XENT, ENT, DIV, SHELL = tf.cast(XENT, tf.float32), tf.cast(ENT, tf.float32), tf.cast(DIV, tf.float32), tf.cast(SHELL, tf.float32)\n",
    "\n",
    "                gain = params[\"cls_xent_coeff\"] * XENT + \\\n",
    "                       -params[\"cls_ent_coeff\"] * ENT + \\\n",
    "                       params[\"cls_div_coeff\"] * DIV + \\\n",
    "                       -params[\"cls_shell_coeff\"] * SHELL\n",
    "\n",
    "            z_g = tape.gradient(gain, z_new)\n",
    "            if z_g is not None:\n",
    "                z_new = (z_new + 1 * z_g)\n",
    "\n",
    "        if z_new_max is None:\n",
    "            z_new_max = z_new.numpy().copy()\n",
    "        else:\n",
    "            z_new_max = np.concatenate([z_new_max, z_new.numpy().copy()])\n",
    "\n",
    "    tf.stop_gradient(z_new_max)\n",
    "\n",
    "    if np.isnan(z_new_max).any():\n",
    "        mir_worked = 0\n",
    "        mem_x = generate(agent, train=False)\n",
    "    else:\n",
    "        mem_x = z_new_max\n",
    "        mir_worked = 1\n",
    "\n",
    "    mem_y = tf.nn.softmax(agent.cls(mem_x), axis=1).numpy().copy()\n",
    "\n",
    "    return mem_x, mem_y, mir_worked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#TODO: vmi más loss is\n",
    "\n",
    "def retrieve_gen_for_gen(agent, latent):\n",
    "\n",
    "    mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
    "    z_new_max = None\n",
    "\n",
    "\n",
    "    for i in range(params[\"n_mem\"]):\n",
    "\n",
    "        z_new = generate(agent, input_latent=mean_latent, train=False, coeff=0.1)\n",
    "\n",
    "        for j in range(params[\"mir_iters\"]):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                # the predictions from the two models should be confident\n",
    "                ENT = 0\n",
    "                if params[\"gen_ent_coeff\"]>0.:\n",
    "                    y_pre = agent.cls(z_new)\n",
    "                    ENT = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_pre, logits=y_pre))\n",
    "\n",
    "                # the new-found samples should be different from each others\n",
    "                DIV = 0\n",
    "                if params[\"gen_div_coeff\"]>0.:\n",
    "                    for found_z_i in range(i):\n",
    "                        DIV += tf.reduce_mean(tf.math.squared_difference(\n",
    "                            z_new,\n",
    "                            z_new_max[found_z_i * z_new.shape[0]:found_z_i * z_new.shape[0] + z_new.shape[0]])\n",
    "                        ) / i\n",
    "\n",
    "                # (NEW) stay on gaussian shell loss:\n",
    "                SHELL = 0\n",
    "                if params[\"gen_shell_coeff\"]>0.:\n",
    "                    SHELL = tf.reduce_mean(tf.math.squared_difference(\n",
    "                        tf.norm(z_new, ord=2, axis=1),\n",
    "                        tf.ones_like(tf.norm(z_new, ord=2, axis=1))*np.sqrt(params[\"z_size\"])))\n",
    "\n",
    "\n",
    "                gain =params[\"gen_div_coeff\"] * DIV + \\\n",
    "                      -params[\"gen_ent_coeff\"] * ENT + \\\n",
    "                       -params[\"gen_shell_coeff\"] * SHELL\n",
    "\n",
    "            z_g = tape.gradient(gain, z_new)\n",
    "            z_new = (z_new + 1 * z_g)\n",
    "\n",
    "        if z_new_max is None:\n",
    "            z_new_max = tf.identity(z_new)\n",
    "        else:\n",
    "            z_new_max = tf.concat([z_new_max, z_new], axis=0)\n",
    "\n",
    "\n",
    "    tf.stop_gradient(z_new_max)\n",
    "\n",
    "    if np.isnan(z_new_max).any():\n",
    "        mir_worked = 0\n",
    "        mem_x = generate(agent, train=False)\n",
    "    else:\n",
    "        mem_x = z_new_max\n",
    "        mir_worked = 1\n",
    "\n",
    "    return mem_x, mir_worked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lM8aEv6833SS"
   },
   "outputs": [],
   "source": [
    "'''Train the generator unit'''\n",
    "\n",
    "def train_generator(agent):\n",
    "\n",
    "    data = agent.state[\"data\"]\n",
    "    latent = agent.gen.encoder(data)\n",
    "    mem_x = None\n",
    "\n",
    "    for it in range(agent.params[\"gen_iters\"]):\n",
    "        generate(agent, input_latent=latent)\n",
    "\n",
    "        if agent.state[\"task\"] > 0:\n",
    "            if it == 0 or not agent.params[\"reuse_samples\"]:\n",
    "                print(data.shape)\n",
    "                mem_x, mir_worked = retrieve_gen_for_gen(agent, latent)\n",
    "\n",
    "                agent.state[\"mir_tries\"] += 1\n",
    "                if mir_worked:\n",
    "                    agent.state[\"mir_success\"] += 1\n",
    "                    # keep for logging later\n",
    "                    gen_x, gen_mem_x = data, mem_x\n",
    "\n",
    "        if mem_x is not None:\n",
    "            generate(agent, input_latent=mem_x, coeff=agent.params[\"mem_coeff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AXTbt9sT8gHE"
   },
   "outputs": [],
   "source": [
    "'''Train the encoder and the classifier unit'''\n",
    "\n",
    "def train_classifier(agent):\n",
    "    data = agent.state[\"data\"]\n",
    "    target = np.argmax(agent.state[\"target\"], axis=1)\n",
    "    mem_x, mem_y = None, None\n",
    "\n",
    "    data_input = keras.Input(shape=agent.params[\"input_shape\"], name=\"image\")\n",
    "    encoder_output = agent.encoder(data_input)\n",
    "    cls_output = agent.cls(encoder_output)\n",
    "    classifier_model = keras.Model(inputs=data_input, outputs=cls_output)\n",
    "    classifier_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    for it in range(agent.params[\"cls_iters\"]):\n",
    "        classifier_model.fit(data, target, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=0)\n",
    "        if agent.state[\"task\"] > 0:\n",
    "            if it == 0 or not agent.params[\"reuse_samples\"]:\n",
    "                latent = agent.gen.encoder(data)\n",
    "                mem_x, mem_y, mir_worked = retrieve_gen_for_cls(agent, latent)\n",
    "                agent.state[\"mir_tries\"] += 1\n",
    "                if mir_worked:\n",
    "                    agent.state[\"mir_success\"] += 1\n",
    "                    # keep for logging later\n",
    "                    cls_x, cls_mem_x = latent, mem_x\n",
    "            if mem_x is not None:\n",
    "                classifier_model.fit(mem_x, mem_y, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A88lN-h9hdV"
   },
   "outputs": [],
   "source": [
    "def run_epoch(agent):\n",
    "  for i, (data, target) in enumerate(agent.state[\"tr_loader\"]):\n",
    "    print(data.shape)\n",
    "    #print(target.shape)\n",
    "    if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
    "    agent.state[\"sample_amt\"] += data.shape[0]\n",
    "\n",
    "    agent.state[\"data\"] = data\n",
    "    agent.state[\"target\"] = target\n",
    "    agent.state[\"i_example\"] = i\n",
    "\n",
    "    agent.state[\"beta\"] = min([(agent.state[\"sample_amt\"]) / max([agent.params[\"warmup\"], 1.]), agent.params[\"max_beta\"]])\n",
    "\n",
    "    train_classifier(agent)\n",
    "    train_generator(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0SyPNSep_K0t"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from keras.models import model_from_config\n",
    "\n",
    "def run_task(agent):\n",
    "  agent.state[\"sample_amt\"] = 0\n",
    "\n",
    "  for epoch in range(agent.params[\"n_epochs\"]):\n",
    "    run_epoch(agent)\n",
    "\n",
    "\n",
    "  #evaluation\n",
    "  #with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "    ''' logging\n",
    "\n",
    "    # save some training reconstructions:\n",
    "    recon_path_ = os.path.join(recon_path, f'task{agent.state[\"task\"]}.png')\n",
    "    # TODO: data\n",
    "    recons = tf.concat([data.to('cpu'), x_mean.to('cpu')])\n",
    "    save_image(recons, recon_path_, nrow=agent.params[\"batch_size\"])\n",
    "\n",
    "    # save some pretty images:\n",
    "    gen_images = agent.gen.generate(25).to('cpu')\n",
    "    sample_path_ = os.path.join(sample_path, f'task{agent.state[\"task\"]}.png')\n",
    "    save_image(gen_images, sample_path_, nrow=5)\n",
    "\n",
    "\n",
    "    # save some MIR samples:\n",
    "    if agent.state[\"task\"] > 0:\n",
    "        mir_images = tf.concat([cls_x.to('cpu'), cls_mem_x.to('cpu')])\n",
    "        mir_path_ = os.path.join(mir_path, f'cls_task{task}.png')\n",
    "        save_image(mir_images, mir_path_, nrow=10)\n",
    "\n",
    "        mir_images = tf.concat([gen_x.to('cpu'), gen_mem_x.to('cpu')])\n",
    "        mir_path_ = os.path.join(mir_path, f'gen_task{task}.png')\n",
    "        save_image(mir_images, mir_path_, nrow=10)\n",
    "\n",
    "\n",
    "\n",
    "    for task_t, te_loader in enumerate(test_loader):\n",
    "        if task_t > agent.state[\"task\"]: break\n",
    "        LOG_temp = get_temp_logger(None, ['gen_loss', 'cls_loss', 'acc'])\n",
    "\n",
    " \n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            logits = agent.cls.predict(data)\n",
    "\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='mean')(target, logits)\n",
    "            pred = logits.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            LOG_temp['acc'] += [pred.eq(target.view_as(pred)).sum().item() / pred.size(0)]\n",
    "            LOG_temp['cls_loss'] += [loss.item()]\n",
    "\n",
    "            x_mean, z_mu, z_var, ldj, z0, zk = agent.gen.generate(data)\n",
    "            gen_loss, rec, kl, bpd = calculate_loss(x_mean, data, z_mu, z_var, z0,\n",
    "                    zk, ldj, agent.params[\"input_size\"], agent.params[\"loss_fn\"], beta=agent.state[\"beta\"])\n",
    "            LOG_temp['gen_loss'] += [gen_loss.item()]\n",
    "\n",
    "\n",
    "\n",
    "        logging_per_task(wandb, LOG, run, mode, 'acc', task, task_t,\n",
    "                  np.round(np.mean(LOG_temp['acc']), 2))\n",
    "        logging_per_task(wandb, LOG, run, mode, 'cls_loss', task, task_t,\n",
    "                  np.round(np.mean(LOG_temp['cls_loss']), 2))\n",
    "        logging_per_task(wandb, LOG, run, mode, 'gen_loss', task, task_t,\n",
    "                  np.round(np.mean(LOG_temp['gen_loss']), 2))\n",
    "\n",
    "    print(f'\\n{mode}:')\n",
    "    print(LOG[run][mode]['acc'])\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VhbA5MCFCi4r"
   },
   "outputs": [],
   "source": [
    "def run(agent):\n",
    "  agent.state[\"mir_tries\"], agent.state[\"mir_success\"] = 0, 0\n",
    "\n",
    "  agent.set_models(\n",
    "      classifier=classifier.classifier(agent.params),\n",
    "      generator=stable_diffusion.StableDiffusion(img_height=32, img_width=32))\n",
    "\n",
    "  for task, tr_loader in enumerate(train_loader):\n",
    "    agent.state[\"task\"] = task\n",
    "    agent.state[\"tr_loader\"] = tr_loader\n",
    "    run_task(agent)\n",
    "\n",
    "    ''' eval\n",
    "\n",
    "      # accuracy\n",
    "      final_accs = LOG[agent.state[\"run\"]]['acc'][:, agent.state[\"task\"]]\n",
    "      logging_per_task(wandb, LOG, run, mode, 'final_acc', task, value=np.round(np.mean(final_accs),2))\n",
    "\n",
    "      # forgetting\n",
    "      best_acc = np.max(LOG[run][mode]['acc'], 1)\n",
    "      final_forgets = best_acc - LOG[run][mode]['acc'][:, task]\n",
    "      logging_per_task(wandb, LOG, run, mode, 'final_forget', task, value=np.round(np.mean(final_forgets[:-1]),2))\n",
    "\n",
    "      # VAE loss\n",
    "      final_elbos = LOG[run][mode]['gen_loss'][:, task]\n",
    "      logging_per_task(wandb, LOG, run, mode, 'final_elbo', task, value=np.round(np.mean(final_elbos), 2))\n",
    "\n",
    "      print(f'\\n{mode}:')\n",
    "      print(f'final accuracy: {final_accs}')\n",
    "      print(f'average: {LOG[run][mode][\"final_acc\"]}')\n",
    "      print(f'final forgetting: {final_forgets}')\n",
    "      print(f'average: {LOG[run][mode][\"final_forget\"]}')\n",
    "      print(f'final VAE loss: {final_elbos}')\n",
    "      print(f'average: {LOG[run][mode][\"final_elbo\"]}\\n')\n",
    "\n",
    "      try:\n",
    "          mir_worked_frac = mir_success/ (mir_tries)\n",
    "          logging_per_task(wandb, LOG, run, mode, 'final_mir_worked_frac', task, mir_worked_frac)\n",
    "          print('mir worked \\n', mir_worked_frac)\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'stable_diffusion.stable_diffusion' from '/Users/laszlofreund/PycharmProjects/continual-learning-ait/stable_diffusion/stable_diffusion.py'>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "#importlib.reload(classifier)\n",
    "importlib.reload(stable_diffusion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlPHqKTaC41u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h (None, 4, 4, 320)\n",
      "emb (None, 320)\n",
      "h (None, 4, 4, 320)\n",
      "emb (None, 320)\n",
      "h (None, 2, 2, 640)\n",
      "emb (None, 640)\n",
      "h (None, 2, 2, 640)\n",
      "emb (None, 640)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 1, 1, 1280)\n",
      "emb (None, 1280)\n",
      "h (None, 2, 2, 640)\n",
      "emb (None, 640)\n",
      "h (None, 2, 2, 640)\n",
      "emb (None, 640)\n",
      "h (None, 2, 2, 640)\n",
      "emb (None, 640)\n",
      "h (None, 4, 4, 320)\n",
      "emb (None, 320)\n",
      "h (None, 4, 4, 320)\n",
      "emb (None, 320)\n",
      "h (None, 4, 4, 320)\n",
      "emb (None, 320)\n",
      "(64, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 18:53:55.705402: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      " 11  12:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "Generator loss:  tf.Tensor(\n",
      "[ -4.45275    -5.586276   -4.9296036  -8.510807   -5.4032583  -6.5243263\n",
      "  -8.680038   -7.384837  -11.651019   -2.74195    -4.871292   -4.452985\n",
      "  -8.10217    -4.6582546  -5.7108917  -5.455018   -3.7294602  -5.3113413\n",
      "  -6.531668   -5.508216   -8.867166  -11.183591   -4.8973145  -4.5599065\n",
      "  -4.6262465  -7.665973   -6.432064   -6.42487    -3.3591285  -6.4161077\n",
      "  -7.5616703  -7.5428066  -4.1183147  -5.513752   -6.592759   -7.4683127\n",
      "  -4.0348234  -8.39825    -3.7158322   8.409157   -9.005091   -5.292802\n",
      "  -5.699345   -4.0450854  -4.2961817  -7.0072546  -5.297821   -7.1830645\n",
      "  -6.2865663  -4.567163   -5.956134   -5.040764   -5.1878734  -5.2540073\n",
      "  -5.2864056  -4.7698035  -7.0220165  -5.499574   -5.2686224  -3.9915156\n",
      "  -6.112562   -4.9187317  -5.5491195  -7.180644 ], shape=(64,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10  11:   8%|▊         | 1/12 [00:08<01:33,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "Generator loss:  tf.Tensor(\n",
      "[ -4.6866074  -5.823448   -5.085408   -8.677839   -5.6463385  -6.7656965\n",
      "  -8.909419   -7.530851  -11.960329   -2.8714485  -5.0729084  -4.6722603\n",
      "  -8.218417   -4.8162284  -5.976713   -5.6841965  -3.867021   -5.5487895\n",
      "  -6.717581   -5.7398276  -9.105291  -11.205311   -5.1211143  -4.8057494\n",
      "  -4.867646   -7.687046   -6.6853485  -6.679387   -3.5454514  -6.6238194\n",
      "  -7.865712   -7.7798347  -4.335488   -5.7879543  -6.744604   -7.625166\n",
      "  -4.175152   -8.643961   -3.9052577   8.544486   -9.288034   -5.5232906\n",
      "  -5.961605   -4.2552586  -4.499798   -7.198548   -5.5482326  -7.3729973\n",
      "  -6.5456715  -4.7160664  -6.1947303  -5.3043227  -5.424507   -5.4132013\n",
      "  -5.5467296  -4.8920965  -7.309134   -5.680517   -5.5192947  -4.2151957\n",
      "  -6.275129   -5.1762304  -5.804977   -7.461323 ], shape=(64,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9  10:  17%|█▋        | 2/12 [00:12<00:59,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 1, 1, 1280)\n",
      "emb (64, 1280)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 2, 2, 640)\n",
      "emb (64, 640)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "h (64, 4, 4, 320)\n",
      "emb (64, 320)\n",
      "Generator loss:  tf.Tensor(\n",
      "[ -4.712513   -5.859616   -5.115799   -8.699662   -5.6882076  -6.806996\n",
      "  -8.909665   -7.5601287 -12.042876   -2.8969302  -5.091666   -4.7083187\n",
      "  -8.233217   -4.8488736  -6.0188026  -5.7157516  -3.8974576  -5.5788608\n",
      "  -6.7323484  -5.776719   -9.097996  -11.171671   -5.1556873  -4.838076\n",
      "  -4.916125   -7.645625   -6.7157125  -6.713606   -3.5851028  -6.6458654\n",
      "  -7.9027896  -7.807622   -4.370075   -5.835843   -6.784475   -7.6673975\n",
      "  -4.190624   -8.659309   -3.9387693   8.624746   -9.36226    -5.561622\n",
      "  -5.9850082  -4.293839   -4.5362453  -7.2468367  -5.583334   -7.4189954\n",
      "  -6.5810547  -4.740861   -6.2283206  -5.352255   -5.4496017  -5.414366\n",
      "  -5.5950165  -4.898138   -7.371068   -5.730108   -5.551201   -4.258796\n",
      "  -6.3070807  -5.1918364  -5.848813   -7.5098796], shape=(64,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(params)\n",
    "for r in range(agent.params[\"n_runs\"]):\n",
    "  agent.state[\"run\"] = r\n",
    "  run(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I2ZLTm-JnX8"
   },
   "outputs": [],
   "source": [
    "''' final eval\n",
    "\n",
    "n_runs = agent.params[\"n_runs\"]\n",
    "\n",
    "final_accs = [LOG[x]['final_acc'] for x in range(n_runs)]\n",
    "final_acc_avg = np.mean(final_accs)\n",
    "final_acc_se = np.std(final_accs) / np.sqrt(n_runs)\n",
    "\n",
    "# forgetting\n",
    "final_forgets = [LOG[x]['final_forget'] for x in range(n_runs)]\n",
    "final_forget_avg = np.mean(final_forgets)\n",
    "final_forget_se = np.std(final_forgets) / np.sqrt(n_runs)\n",
    "\n",
    "# VAE loss\n",
    "final_elbos = [LOG[x]['final_elbo'] for x in range(n_runs)]\n",
    "final_elbo_avg = np.mean(final_elbos)\n",
    "final_elbo_se = np.std(final_elbos) / np.sqrt(n_runs)\n",
    "\n",
    "# MIR worked\n",
    "try:\n",
    "    final_mir_worked_frac = [LOG[x]['final_mir_worked_frac'] for x in range(n_runs)]\n",
    "    final_mir_worked_avg = np.mean(final_mir_worked_frac)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f'\\nFinal Accuracy: {final_acc_avg:.3f} +/- {final_acc_se:.3f}')\n",
    "print(f'\\nFinal Forget: {final_forget_avg:.3f} +/- {final_forget_se:.3f}')\n",
    "print(f'\\nFinal ELBO: {final_elbo_avg:.3f} +/- {final_elbo_se:.3f}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
