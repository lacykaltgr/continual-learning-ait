{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Download the files\n",
        "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
        "!unzip main.zip\n",
        "!find continual-learning-ait-main -type f ! -name \"main.ipynb\" ! -name \"main.py\" -exec cp {} . \\;"
      ],
      "metadata": {
        "id": "pveLpmUzEF5A",
        "outputId": "9d696bb4-268a-40f1-e28c-81c589c79419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 14:57:07--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main [following]\n",
            "--2023-04-11 14:57:07--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [ <=>                ] 154.08K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-04-11 14:57:07 (3.06 MB/s) - ‘main.zip’ saved [157773]\n",
            "\n",
            "Archive:  main.zip\n",
            "9d267e6b95e57ad02d2e58c6e1d1f16dcffdefcb\n",
            "   creating: continual-learning-ait-main/\n",
            "  inflating: continual-learning-ait-main/LICENSE  \n",
            "  inflating: continual-learning-ait-main/README.md  \n",
            "  inflating: continual-learning-ait-main/classifier.py  \n",
            "  inflating: continual-learning-ait-main/data_preparation.ipynb  \n",
            "  inflating: continual-learning-ait-main/data_preparation.py  \n",
            "  inflating: continual-learning-ait-main/distributions.py  \n",
            "  inflating: continual-learning-ait-main/loss.py  \n",
            "  inflating: continual-learning-ait-main/main.ipynb  \n",
            "  inflating: continual-learning-ait-main/main.py  \n",
            "  inflating: continual-learning-ait-main/mir.py  \n",
            "  inflating: continual-learning-ait-main/requirements_m1.txt  \n",
            "   creating: continual-learning-ait-main/stable_diffusion/\n",
            "  inflating: continual-learning-ait-main/stable_diffusion/autoencoder_kl.py  \n",
            "  inflating: continual-learning-ait-main/stable_diffusion/diffusion_model.py  \n",
            "  inflating: continual-learning-ait-main/stable_diffusion/layers.py  \n",
            "  inflating: continual-learning-ait-main/stable_diffusion/stable_diffusion.py  \n",
            "  inflating: continual-learning-ait-main/utils.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": true
        },
        "id": "vjfVuEmXECoL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from data_preparation import load_dataset\n",
        "from data_preparation import CLDataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uVyESBnu1CNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpt_train, dpt_test = load_dataset('cifar-100')"
      ],
      "metadata": {
        "id": "TaH3bi-5crqD",
        "outputId": "e0efc0f3-ec94-496a-e91d-2f3b7bbacabb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = CLDataLoader(dpt_train, 64, train=True)\n",
        "test_loader = CLDataLoader(dpt_train, 64, train=False)"
      ],
      "metadata": {
        "id": "yIESUurDeOCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "result_dir = 'results/'\n",
        "\n",
        "result_path = os.path.join(result_dir)\n",
        "if not os.path.exists(result_path): os.mkdir(result_path)\n",
        "sample_path = os.path.join(*[result_dir, 'samples'])\n",
        "if not os.path.exists(sample_path): os.mkdir(sample_path)\n",
        "recon_path = os.path.join(*[result_dir, 'reconstructions'])\n",
        "if not os.path.exists(recon_path): os.mkdir(recon_path)\n",
        "mir_path = os.path.join(*[result_dir, 'mir'])\n",
        "if not os.path.exists(mir_path): os.mkdir(mir_path)"
      ],
      "metadata": {
        "id": "gAZGeLzm02VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"device\": 'cuda:0' if tf.test.is_gpu_available() else 'cpu',\n",
        "    \"n_runs\": 1,\n",
        "    \"n_tasks\": 10,\n",
        "    \"n_epochs\": 100,\n",
        "    \"n_classes\": 10,\n",
        "    \"input_size\": (3,32,32),\n",
        "    #\"samples_per_task\": 100,\n",
        "    \"batch_size\": 64,\n",
        "\n",
        "    \"gen_depth\": 6,\n",
        "    \"cls_mir_gen\": 1,\n",
        "    \"gen_mir_gen\": 1,\n",
        "    \"cls_iters\": 100,\n",
        "    \"gen_iters\": 10,\n",
        "    \"loss_fn\": \"mse\",\n",
        "\n",
        "    \"lr\": 0.001,\n",
        "    \"warmup\": 0,\n",
        "    \"max_beta\": 1,\n",
        "    \"reuse_samples\": True,\n",
        "    \"print_every\": 5,\n",
        "    \"mem_coeff\": 0.12,\n",
        "    \"n_mem\": 10,\n",
        "    \"mir_init_prior\": 10,\n",
        "    \"z_size\": 10,\n",
        "    \"mir_iters\": 100,\n",
        "    \"gen_kl_coeff\": 0.5,\n",
        "    \"gen_rec_coeff\": 0.5,\n",
        "    \"gen_ent_coeff\": 0.5,\n",
        "    \"gen_div_coeff\": 0.5,\n",
        "    \"gen_shell_coeff\": 0.5\n",
        "}"
      ],
      "metadata": {
        "id": "GY9MAk581iYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, params):\n",
        "    self.params = params\n",
        "    self.state = dict()\n",
        "    self.prev_cls = None\n",
        "    self.prev_gen = None\n",
        "\n",
        "  def set_gen(self, gen):\n",
        "    self.gen = gen\n",
        "    self.opt_gen = tf.keras.optimizers.Adam(gen.parameters())\n",
        "  \n",
        "  def set_cls(self, cls):\n",
        "    self.cls = cls\n",
        "    self.opt = tf.keras.optimizers.SGD(cls.parameters(), lr=params[\"lr\"])"
      ],
      "metadata": {
        "id": "OmitFSMM2WJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mir\n",
        "from utils import get_logger, get_temp_logger, logging_per_task, naive_cross_entropy_loss\n",
        "from classifier import ResNet18, classifier\n",
        "from loss import calculate_loss\n",
        "from stable_diffusion.stable_diffusion import StableDiffusion"
      ],
      "metadata": {
        "id": "Fq8mfvFg66j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(agent):\n",
        "  data = agent.state[\"data\"]\n",
        "  beta = agent.state[\"beta\"]\n",
        "  task = agent.state[\"task\"]\n",
        "\n",
        "  for it in range(agent.params[\"gen_iters\"]):\n",
        "\n",
        "      x_mean, z_mu, z_var, ldj, z0, zk = agent.gen(data)\n",
        "      gen_loss, rec, kl, _ = calculate_loss(x_mean, data, z_mu, z_var, z0, zk, ldj, agent.params[\"input_size\"], agent.params[\"loss_fn\"], beta=beta)\n",
        "\n",
        "      tot_gen_loss = 0 + gen_loss\n",
        "\n",
        "      if task > 0:\n",
        "\n",
        "          if it == 0 or not agent.params[\"reuse_samples\"]:\n",
        "              mem_x, mir_worked = mir.retrieve_gen_for_gen(agent.params, data, agent.gen, agent.prev_gen, agent.prev_cls)\n",
        "\n",
        "              agent.state[\"mir_tries\"] += 1\n",
        "              if mir_worked:\n",
        "                  agent.state[\"mir_success\"] += 1\n",
        "                  # keep for logging later\n",
        "                  gen_x, gen_mem_x = data, mem_x\n",
        "\n",
        "          mem_x_mean, z_mu, z_var, ldj, z0, zk = agent.gen(mem_x)\n",
        "          mem_gen_loss, mem_rec, mem_kl, _ = calculate_loss(mem_x_mean, mem_x, z_mu, z_var, z0, zk, ldj, agent.params[\"input_size\"], agent.params[\"loss_fn\"], beta=beta)\n",
        "\n",
        "          tot_gen_loss += agent.params[\"mem_coeff\"] * mem_gen_loss\n",
        "\n",
        "      agent.opt_gen.zero_grad()\n",
        "      tot_gen_loss.backward()\n",
        "      agent.opt_gen.step()\n",
        "\n",
        "      \n",
        "\n",
        "  if agent.state[\"i_example\"] % agent.params[\"print_every\"] == 0:\n",
        "      print(f'current VAE loss = {gen_loss.item():.4f} (rec: {rec.item():.4f} + beta: {beta:.2f} * kl: {kl.item():.2f}')\n",
        "      if task > 0:\n",
        "          print(f'memory VAE loss = {mem_gen_loss.item():.4f} (rec: { mem_rec.item():.4f} + beta: {beta:.2f} * kl: {mem_kl.item():.2f})')\n"
      ],
      "metadata": {
        "id": "lM8aEv6833SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(agent):\n",
        "  data = agent.state[\"data\"]\n",
        "  target = agent.state[\"target\"]\n",
        "  beta = agent.state[\"beta\"]\n",
        "  task = agent.state[\"task\"]\n",
        "\n",
        "  for it in range(agent.params[\"cls_iters\"]):\n",
        "\n",
        "      logits = agent.cls(data)\n",
        "      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='mean')\n",
        "      cls_loss = loss_fn(target, logits)\n",
        "      tot_cls_loss = 0 + cls_loss\n",
        "\n",
        "      if task > 0:\n",
        "\n",
        "          if it == 0 or not agent.params[\"reuse_samples\"]:\n",
        "              mem_x, mem_y, mir_worked = mir.retrieve_gen_for_cls(agent.params, data, agent.cls, agent.prev_cls, agent.prev_gen)\n",
        "              mir_tries += 1\n",
        "              if mir_worked:\n",
        "                  mir_success += 1\n",
        "                  # keep for logging later\n",
        "                  cls_x, cls_mem_x = data, mem_x\n",
        "\n",
        "          mem_logits = agent.cls(mem_x)\n",
        "\n",
        "          mem_cls_loss = naive_cross_entropy_loss(mem_logits, mem_y)\n",
        "\n",
        "          tot_cls_loss += agent.params[\"mem_coeff\"] * mem_cls_loss\n",
        "\n",
        "      agent.opt.zero_grad()\n",
        "      tot_cls_loss.backward()\n",
        "      agent.opt.step()\n",
        "\n",
        "\n",
        "\n",
        "  if agent.state[\"i_example\"] % agent.params[\"print_every\"] == 0:\n",
        "      pred = logits.argmax(dim=1, keepdim=True)\n",
        "      acc = pred.eq(target.view_as(pred)).sum().item() / pred.size(0)\n",
        "      print(f'current training accuracy: {acc:.2f}')\n",
        "      if agent.state[\"task\"] > 0:\n",
        "          pred = mem_logits.argmax(dim=1, keepdim=True)\n",
        "          mem_y = mem_y.argmax(dim=1, keepdim=True)\n",
        "          acc = pred.eq(mem_y.view_as(pred)).sum().item() / pred.size(0)\n",
        "          print(f'memory training accuracy: {acc:.2f}')"
      ],
      "metadata": {
        "id": "AXTbt9sT8gHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(agent):\n",
        "  for i, (data, target) in enumerate(agent.state[\"tr_loader\"]):\n",
        "\n",
        "    if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
        "    agent.state[\"sample_amt\"] += data.size(0)\n",
        "\n",
        "    agent.state[\"data\"] = data.to(agent.params[\"device\"])\n",
        "    agent.state[\"target\"] = target.to(agent.params[\"device\"])\n",
        "    agent.state[\"i_example\"] = i\n",
        "\n",
        "    agent.state[\"beta\"] = min([(agent.state[\"sample_amt\"]) / max([agent.params[\"warmup\"], 1.]), agent.params[\"max_beta\"]])\n",
        "\n",
        "    train_generator(agent)\n",
        "    train_classifier(agent)"
      ],
      "metadata": {
        "id": "0A88lN-h9hdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def run_task(agent):\n",
        "  agent.cls = agent.cls.train()\n",
        "  agent.gen = agent.gen.train()\n",
        "\n",
        "  agent.state[\"sample_amt\"] = 0\n",
        "\n",
        "  for epoch in range(agent.params[\"n_epochs\"]):\n",
        "    run_epoch(agent)\n",
        "\n",
        "\n",
        "  #evaluation\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "    agent.cls = agent.cls.eval()\n",
        "    agent.prev_cls = deepcopy(agent.cls)\n",
        "\n",
        "    agent.gen = agent.gen.eval()\n",
        "    agent.prev_gen = deepcopy(agent.gen)\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "    # save some training reconstructions:\n",
        "    recon_path_ = os.path.join(recon_path, f'task{agent.state[\"task\"]}.png')\n",
        "    # TODO: data\n",
        "    recons = tf.concat([data.to('cpu'), x_mean.to('cpu')])\n",
        "    save_image(recons, recon_path_, nrow=agent.params[\"batch_size\"])\n",
        "\n",
        "    # save some pretty images:\n",
        "    gen_images = agent.gen.generate(25).to('cpu')\n",
        "    sample_path_ = os.path.join(sample_path, f'task{agent.state[\"task\"]}.png')\n",
        "    save_image(gen_images, sample_path_, nrow=5)\n",
        "\n",
        "\n",
        "    # save some MIR samples:\n",
        "    if agent.state[\"task\"] > 0:\n",
        "        mir_images = tf.concat([cls_x.to('cpu'), cls_mem_x.to('cpu')])\n",
        "        mir_path_ = os.path.join(mir_path, f'cls_task{task}.png')\n",
        "        save_image(mir_images, mir_path_, nrow=10)\n",
        "\n",
        "        mir_images = tf.concat([gen_x.to('cpu'), gen_mem_x.to('cpu')])\n",
        "        mir_path_ = os.path.join(mir_path, f'gen_task{task}.png')\n",
        "        save_image(mir_images, mir_path_, nrow=10)\n",
        "\n",
        "    '''\n",
        "\n",
        "    for task_t, te_loader in enumerate(test_loader):\n",
        "        if task_t > agent.state[\"task\"]: break\n",
        "        LOG_temp = get_temp_logger(None, ['gen_loss', 'cls_loss', 'acc'])\n",
        "\n",
        " \n",
        "        for i, (data, target) in enumerate(test_loader):\n",
        "\n",
        "            data, target = data.to(agent.params[\"device\"]), target.to(agent.params[\"device\"])\n",
        "\n",
        "            logits = agent.cls(data)\n",
        "\n",
        "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='mean')(target, logits)\n",
        "            pred = logits.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            LOG_temp['acc'] += [pred.eq(target.view_as(pred)).sum().item() / pred.size(0)]\n",
        "            LOG_temp['cls_loss'] += [loss.item()]\n",
        "\n",
        "            x_mean, z_mu, z_var, ldj, z0, zk = agent.gen(data)\n",
        "            gen_loss, rec, kl, bpd = calculate_loss(x_mean, data, z_mu, z_var, z0,\n",
        "                    zk, ldj, agent.params[\"input_size\"], agent.params[\"loss_fn\"], beta=agent.state[\"beta\"])\n",
        "            LOG_temp['gen_loss'] += [gen_loss.item()]\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "        logging_per_task(wandb, LOG, run, mode, 'acc', task, task_t,\n",
        "                  np.round(np.mean(LOG_temp['acc']), 2))\n",
        "        logging_per_task(wandb, LOG, run, mode, 'cls_loss', task, task_t,\n",
        "                  np.round(np.mean(LOG_temp['cls_loss']), 2))\n",
        "        logging_per_task(wandb, LOG, run, mode, 'gen_loss', task, task_t,\n",
        "                  np.round(np.mean(LOG_temp['gen_loss']), 2))\n",
        "\n",
        "    print(f'\\n{mode}:')\n",
        "    print(LOG[run][mode]['acc'])\n",
        "\n",
        "    '''"
      ],
      "metadata": {
        "id": "0SyPNSep_K0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(agent):\n",
        "  agent.state[\"mir_tries\"], agent.state[\"mir_success\"] = 0, 0\n",
        "\n",
        "  agent.set_cls(ResNet18(agent.params[\"n_classes\"], nf=20, input_size=agent.params[\"input_size\"]))\n",
        "  agent.set_gen(StableDiffusion())\n",
        "\n",
        "  for task, tr_loader in enumerate(train_loader):\n",
        "    agent.state[\"task\"] = task\n",
        "    agent.state[\"tr_loader\"] = tr_loader\n",
        "    run_task(agent)\n",
        "\n",
        "'''\n",
        "\n",
        "  # accuracy\n",
        "  final_accs = LOG[agent.state[\"run\"]]['acc'][:, agent.state[\"task\"]]\n",
        "  logging_per_task(wandb, LOG, run, mode, 'final_acc', task, value=np.round(np.mean(final_accs),2))\n",
        "\n",
        "  # forgetting\n",
        "  best_acc = np.max(LOG[run][mode]['acc'], 1)\n",
        "  final_forgets = best_acc - LOG[run][mode]['acc'][:, task]\n",
        "  logging_per_task(wandb, LOG, run, mode, 'final_forget', task, value=np.round(np.mean(final_forgets[:-1]),2))\n",
        "\n",
        "  # VAE loss\n",
        "  final_elbos = LOG[run][mode]['gen_loss'][:, task]\n",
        "  logging_per_task(wandb, LOG, run, mode, 'final_elbo', task, value=np.round(np.mean(final_elbos), 2))\n",
        "\n",
        "  print(f'\\n{mode}:')\n",
        "  print(f'final accuracy: {final_accs}')\n",
        "  print(f'average: {LOG[run][mode][\"final_acc\"]}')\n",
        "  print(f'final forgetting: {final_forgets}')\n",
        "  print(f'average: {LOG[run][mode][\"final_forget\"]}')\n",
        "  print(f'final VAE loss: {final_elbos}')\n",
        "  print(f'average: {LOG[run][mode][\"final_elbo\"]}\\n')\n",
        "\n",
        "  try:\n",
        "      mir_worked_frac = mir_success/ (mir_tries)\n",
        "      logging_per_task(wandb, LOG, run, mode, 'final_mir_worked_frac', task, mir_worked_frac)\n",
        "      print('mir worked \\n', mir_worked_frac)\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "VhbA5MCFCi4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(params)\n",
        "for r in range(agent.params[\"n_runs\"]):\n",
        "  agent.state[\"run\"] = r\n",
        "  run(agent)"
      ],
      "metadata": {
        "id": "BlPHqKTaC41u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "n_runs = agent.params[\"n_runs\"]\n",
        "\n",
        "final_accs = [LOG[x]['final_acc'] for x in range(n_runs)]\n",
        "final_acc_avg = np.mean(final_accs)\n",
        "final_acc_se = np.std(final_accs) / np.sqrt(n_runs)\n",
        "\n",
        "# forgetting\n",
        "final_forgets = [LOG[x]['final_forget'] for x in range(n_runs)]\n",
        "final_forget_avg = np.mean(final_forgets)\n",
        "final_forget_se = np.std(final_forgets) / np.sqrt(n_runs)\n",
        "\n",
        "# VAE loss\n",
        "final_elbos = [LOG[x]['final_elbo'] for x in range(n_runs)]\n",
        "final_elbo_avg = np.mean(final_elbos)\n",
        "final_elbo_se = np.std(final_elbos) / np.sqrt(n_runs)\n",
        "\n",
        "# MIR worked\n",
        "try:\n",
        "    final_mir_worked_frac = [LOG[x]['final_mir_worked_frac'] for x in range(n_runs)]\n",
        "    final_mir_worked_avg = np.mean(final_mir_worked_frac)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(f'\\nFinal Accuracy: {final_acc_avg:.3f} +/- {final_acc_se:.3f}')\n",
        "print(f'\\nFinal Forget: {final_forget_avg:.3f} +/- {final_forget_se:.3f}')\n",
        "print(f'\\nFinal ELBO: {final_elbo_avg:.3f} +/- {final_elbo_se:.3f}')\n",
        "'''"
      ],
      "metadata": {
        "id": "4I2ZLTm-JnX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}