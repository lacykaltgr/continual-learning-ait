{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lacykaltgr/continual-learning-ait/blob/experiment/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pveLpmUzEF5A",
        "outputId": "a98b7794-e908-4e72-9bca-4c2f5a04a5c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-14 21:28:01--  https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment [following]\n",
            "--2023-05-14 21:28:01--  https://codeload.github.com/lacykaltgr/continual-learning-ait/zip/refs/heads/experiment\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘experiment.zip’\n",
            "\n",
            "experiment.zip          [ <=>                ]   1.63M  9.76MB/s    in 0.2s    \n",
            "\n",
            "2023-05-14 21:28:02 (9.76 MB/s) - ‘experiment.zip’ saved [1711070]\n",
            "\n",
            "Archive:  experiment.zip\n",
            "d33f65be4069546f218c818c0db66dc7f257010e\n",
            "   creating: continual-learning-ait-experiment/\n",
            "  inflating: continual-learning-ait-experiment/README.md  \n",
            "  inflating: continual-learning-ait-experiment/classifier.py  \n",
            "  inflating: continual-learning-ait-experiment/classifier_lib.py  \n",
            "  inflating: continual-learning-ait-experiment/constants.py  \n",
            "  inflating: continual-learning-ait-experiment/data_preparation.py  \n",
            "  inflating: continual-learning-ait-experiment/generate.py  \n",
            "  inflating: continual-learning-ait-experiment/generator.py  \n",
            "   creating: continual-learning-ait-experiment/guided_diffusion/\n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/__init__.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/dist_util.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/fp16_util.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/gaussian_diffusion.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/logger.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/losses.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/nn.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/resample.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/respace.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/script_util.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/train_util.py  \n",
            "  inflating: continual-learning-ait-experiment/guided_diffusion/unet.py  \n",
            "  inflating: continual-learning-ait-experiment/img.png  \n",
            "  inflating: continual-learning-ait-experiment/main.ipynb  \n",
            "   creating: continual-learning-ait-experiment/models/\n",
            "  inflating: continual-learning-ait-experiment/models/classifier.h5  \n",
            "  inflating: continual-learning-ait-experiment/models/encoder.h5  \n",
            "   creating: continual-learning-ait-experiment/notebooks/\n",
            "  inflating: continual-learning-ait-experiment/notebooks/classifier.ipynb  \n",
            "  inflating: continual-learning-ait-experiment/notebooks/data_preparation.ipynb  \n",
            "  inflating: continual-learning-ait-experiment/notebooks/generator.ipynb  \n",
            "   creating: continual-learning-ait-experiment/torch_utils/\n",
            "  inflating: continual-learning-ait-experiment/torch_utils/__init__.py  \n",
            "  inflating: continual-learning-ait-experiment/torch_utils/distributed.py  \n",
            "  inflating: continual-learning-ait-experiment/torch_utils/misc.py  \n",
            "  inflating: continual-learning-ait-experiment/torch_utils/persistence.py  \n",
            "  inflating: continual-learning-ait-experiment/torch_utils/training_stats.py  \n",
            "  inflating: continual-learning-ait-experiment/train.py  \n",
            "  inflating: continual-learning-ait-experiment/utils.py  \n"
          ]
        }
      ],
      "source": [
        "'''Download the files '''\n",
        "'''Only for colab'''\n",
        "\n",
        "!wget https://github.com/lacykaltgr/continual-learning-ait/archive/refs/heads/experiment.zip\n",
        "!unzip experiment.zip\n",
        "!find continual-learning-ait-experiment -type f ! -name \"main.ipynb\" -exec cp {} . \\;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjfVuEmXECoL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "#from keras.metrics import Accuracy\n",
        "\n",
        "#import classifier\n",
        "#from generator import Generator\n",
        "#from classifier import Encoder, Classifier\n",
        "import utils\n",
        "from data_preparation import load_dataset, CLDataLoader\n",
        "\n",
        "import gc\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "collapsed": false,
        "id": "zzYWiWfZ8kVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaH3bi-5crqD"
      },
      "outputs": [],
      "source": [
        "dpt_train, dpt_test = load_dataset('cifar-10', n_classes_first_task=4, n_classes_other_task=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIESUurDeOCR",
        "outputId": "07cc28a9-d1b1-4edd-af50-9c62e7ab2342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M2\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_loader = CLDataLoader(dpt_train, batch_size , train=True)\n",
        "test_loader = CLDataLoader(dpt_train, batch_size, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define parameters and agent"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3Jl-cBYs8kVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY9MAk581iYQ"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    #general\n",
        "    \"n_runs\": 1,\n",
        "    \"n_tasks\": 3,\n",
        "    \"n_classes\": 10,\n",
        "    \"input_shape\": (32, 32, 3),\n",
        "    \"embedding_shape\": (6, 6, 8),\n",
        "    \"samples_per_task\": 10000,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"print_every\": 1,\n",
        "\n",
        "    #classifier\n",
        "    \"cls_iters\": 1,\n",
        "    \"cls_lr\": 1e-2,\n",
        "    \"cls_epochs\": 5,\n",
        "\n",
        "    #generator\n",
        "    \"gen_epochs\": 1,\n",
        "    \"num_steps\": 3,\n",
        "    \"gen_lr\": 2e-4,\n",
        "    \"gen_iters\": 1,\n",
        "    \"input_latent_strength\": 0.9,\n",
        "    \"temperature\": 0.9,\n",
        "\n",
        "    #mir\n",
        "    \"n_mem\": 2,\n",
        "    \"mir_iters\": 3,\n",
        "    \"reuse_samples\": True,\n",
        "    \"mem_coeff\": 0.12,\n",
        "    \"z_size\": 10,\n",
        "    \n",
        "    \"gen_ent_coeff\": 0.5,\n",
        "    \"gen_div_coeff\": 0.5,\n",
        "    \"gen_shell_coeff\": 0.5,\n",
        "    \"cls_xent_coeff\": 0.5,\n",
        "    \"cls_ent_coeff\": 0.5,\n",
        "    \"cls_div_coeff\": 0.5,\n",
        "    \"cls_shell_coeff\": 0.5,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmitFSMM2WJn"
      },
      "outputs": [],
      "source": [
        "'''Agent to handle models, parameters and states'''\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, hparams):\n",
        "    self.params = hparams\n",
        "    self.state = dict()\n",
        "\n",
        "    self.classifier = None\n",
        "    self.generator = None\n",
        "    self.encoder = None\n",
        "    self.encoder_classifier = None\n",
        "\n",
        "    self.eval = accuracy_score\n",
        "\n",
        "\n",
        "  def set_models(\n",
        "          self,\n",
        "          _generator=None,\n",
        "          _classifier=None,\n",
        "          _encoder  = None,\n",
        "    ):\n",
        "    cls = _classifier #classifier\n",
        "    gen = _generator  #generator\n",
        "    enc = _encoder #encoder\n",
        "\n",
        "    self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"cls_lr\"])\n",
        "    self.optimizer_gen = tf.keras.optimizers.legacy.Adam(learning_rate=params[\"gen_lr\"])\n",
        "\n",
        "    #encoder pipeline\n",
        "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
        "    enc_output = enc(data_input)\n",
        "    self.encoder = keras.Model(inputs=data_input, outputs=enc_output)\n",
        "\n",
        "    # classifier pipeline\n",
        "    latent_input = keras.Input(shape=self.params[\"embedding_shape\"], name=\"latent\")\n",
        "    cls_output = cls(latent_input)\n",
        "    self.classifier = keras.Model(inputs=latent_input, outputs=cls_output)\n",
        "    self.classifier.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # encoder - classifier pipeline\n",
        "    data_input = keras.Input(shape=self.params[\"input_shape\"], name=\"image\")\n",
        "    enc_output = enc(data_input)\n",
        "    enc_cls_output = cls(enc_output)\n",
        "    self.encoder_classifier = keras.Model(inputs= data_input, outputs = enc_cls_output)\n",
        "    self.encoder_classifier.compile(optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    # generator pipeline\n",
        "    self.generator = gen\n",
        "    #latent_input = keras.Input(shape=self.params[\"embedding_shape\"], name=\"latent\")\n",
        "    #gen_output = gen(latent_input)\n",
        "    #self.generator = keras.Model(inputs = latent_input, outputs = gen_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "OPqiS9VV8kVk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''Generate samples and train the diffusion model at the same time'''\n",
        "\n",
        "#plusz lehetne itt még kritérium hogy ne menjen olyan messze az alaptól --- similarity loss\n",
        "#plusz még lehetne talán egy discriminator is, hogy valós reprezentációkat tanuljon meg\n",
        "\n",
        "\n",
        "def generate(agent, cls=None, input_latent=None, train=True, coeff=1.0):\n",
        "\n",
        "    if cls is None:\n",
        "      cls = agent.classifier\n",
        "\n",
        "    batch_size = input_latent.shape[0]\n",
        "    latent, alphas, alphas_prev, timesteps = agent.generator.initialize(params, input_latent, batch_size)\n",
        "\n",
        "\n",
        "    for index, timestep in reversed(list(enumerate(timesteps))):\n",
        "        if train:\n",
        "            with tf.GradientTape() as tape:\n",
        "                e_t = agent.generator.get_model_output(\n",
        "                    latent,\n",
        "                    timestep,\n",
        "                    batch_size,\n",
        "                )\n",
        "                a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "                latent = agent.generator.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
        "\n",
        "                pred = cls(latent)\n",
        "                pred_true = utils.get_one_hot_predictions(pred) #ezt nem fixen kell mecsinálni\n",
        "                confidence_loss = coeff*tf.reduce_mean(tf.keras.losses.categorical_crossentropy(pred_true, pred))\n",
        "                #print(confidence_loss)\n",
        "                similarity_loss = 0.1 * tf.reduce_mean(tf.square(latent - e_t))\n",
        "                #print(similarity_loss)\n",
        "                loss = confidence_loss + similarity_loss\n",
        "                agent.state[\"epoch_eval\"][\"gen_loss\"].append(loss)\n",
        "            grads = tape.gradient(loss, agent.generator.trainable_variables)\n",
        "            agent.optimizer_gen.apply_gradients(zip(grads, agent.generator.trainable_variables))\n",
        "        else:\n",
        "            e_t = agent.generator.get_model_output(\n",
        "                latent,\n",
        "                timestep,\n",
        "                batch_size,\n",
        "            )\n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            latent = agent.generator.get_x_prev(latent, e_t,  a_t, a_prev, params[\"temperature\"])\n",
        "\n",
        "    return latent\n"
      ],
      "metadata": {
        "id": "2BgWNzhg8kVk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''Retrive maximally interferred latent vector for classifier'''\n",
        "\n",
        "def retrieve_gen_for_cls(agent):\n",
        "\n",
        "    print(\"Retrieving latent vector for classifier...\")\n",
        "\n",
        "    latent = agent.encoder(agent.state[\"data\"])\n",
        "    virtual_cls = Classifier()\n",
        "    virtual_cls = utils.get_next_step_cls(\n",
        "        agent.cls,\n",
        "        virtual_cls,\n",
        "        latent,\n",
        "        agent.state[\"target\"]\n",
        "    )\n",
        "\n",
        "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
        "    final_latent = None\n",
        "\n",
        "    for i in range(agent.params[\"n_mem\"]):\n",
        "\n",
        "        generated = generate(agent, input_latent=latent, train=False)\n",
        "\n",
        "        for j in range(params[\"mir_iters\"]):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "                tape.watch(generated)\n",
        "\n",
        "                y_pre = agent.classifier(generated)\n",
        "                y_virtual = virtual_cls(generated)\n",
        "\n",
        "                # maximise the interference:\n",
        "                XENT = tf.constant(0.)\n",
        "                if params[\"cls_xent_coeff\"] > 0.:\n",
        "                    XENT = tf.keras.losses.categorical_crossentropy(y_virtual, y_pre)\n",
        "\n",
        "                # the predictions from the two models should be confident\n",
        "                ENT = tf.constant(0.)\n",
        "                if params[\"cls_ent_coeff\"] > 0.:\n",
        "                    ENT = tf.keras.losses.categorical_crossentropy(y_pre, y_pre)\n",
        "\n",
        "                # the new-found samples should be different from each others\n",
        "                DIV = tf.constant(0.)\n",
        "                if params[\"cls_div_coeff\"] > 0.:\n",
        "                    for found_generated in range(i):\n",
        "                        DIV += tf.keras.losses.MSE(\n",
        "                            generated,\n",
        "                            final_latent[found_generated * generated.shape[0]:found_generated * generated.shape[0] + generated.shape[0]]\n",
        "                        ) / i\n",
        "\n",
        "                # (NEW) stay on gaussian shell loss:\n",
        "                SHELL = tf.constant(0.)\n",
        "                if params[\"cls_shell_coeff\"] > 0.:\n",
        "                    SHELL = tf.keras.losses.MSE(\n",
        "                        tf.norm(generated, axis=1),\n",
        "                        tf.ones_like(tf.norm(generated, axis=1))*np.sqrt(params[\"z_size\"])\n",
        "                    )\n",
        "\n",
        "                XENT, ENT, DIV, SHELL = \\\n",
        "                    tf.reduce_mean(XENT), \\\n",
        "                        tf.reduce_mean(ENT), \\\n",
        "                        tf.reduce_mean(DIV), \\\n",
        "                        tf.reduce_mean(SHELL)\n",
        "\n",
        "                gain = params[\"cls_xent_coeff\"] * XENT + \\\n",
        "                       -params[\"cls_ent_coeff\"] * ENT + \\\n",
        "                       params[\"cls_div_coeff\"] * DIV + \\\n",
        "                       -params[\"cls_shell_coeff\"] * SHELL\n",
        "\n",
        "            gen_grad = tape.gradient(gain, generated)\n",
        "            if gen_grad is not None:\n",
        "                generated = (generated + 1 * gen_grad)\n",
        "\n",
        "        if final_latent is None:\n",
        "            final_latent = generated.numpy().copy()\n",
        "        else:\n",
        "            final_latent = np.concatenate([final_latent, generated.numpy().copy()])\n",
        "\n",
        "    tf.stop_gradient(final_latent)\n",
        "\n",
        "    mir_worked = not np.isnan(final_latent).any()\n",
        "    mem_x = final_latent if mir_worked else generate(agent, train=False)\n",
        "    mem_y = agent.classifier(mem_x).numpy()\n",
        "\n",
        "    return mem_x, mem_y, mir_worked"
      ],
      "metadata": {
        "id": "OlENSPig8kVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''Retrive maximally interferred latent vector for generator'''\n",
        "\n",
        "\n",
        "def retrieve_gen_for_gen(agent):\n",
        "\n",
        "    print(\"Retrieving latent vector for generator...\")\n",
        "\n",
        "    latent = agent.encoder(agent.state[\"data\"])\n",
        "    #mean_latent = tf.cast(tf.reduce_mean(latent, axis=0), tf.float64)\n",
        "    final_latent = None\n",
        "\n",
        "    for i in range(params[\"n_mem\"]):\n",
        "\n",
        "        generated = generate(agent, input_latent=latent, train=False)\n",
        "\n",
        "        for j in range(params[\"mir_iters\"]):\n",
        "\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                tape.watch(generated)\n",
        "\n",
        "                # the predictions from the two models should be confident\n",
        "                ENT = tf.constant(0.)\n",
        "                if params[\"gen_ent_coeff\"]>0.:\n",
        "                    y_pre = agent.classifier(generated)\n",
        "                    y_pre_true = utils.get_one_hot_predictions(y_pre)\n",
        "                    ENT = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pre_true, y_pre))\n",
        "\n",
        "                # the new-found samples should be different from each others\n",
        "                DIV = tf.constant(0.)\n",
        "                if params[\"gen_div_coeff\"]>0.:\n",
        "                    for found_generated in range(i):\n",
        "                        DIV += tf.reduce_mean(tf.math.squared_difference(\n",
        "                            generated,\n",
        "                            final_latent[found_generated * generated.shape[0]:found_generated * generated.shape[0] + generated.shape[0]])\n",
        "                        ) / i\n",
        "\n",
        "                # (NEW) stay on gaussian shell loss:\n",
        "                SHELL = tf.constant(0.)\n",
        "                if params[\"gen_shell_coeff\"]>0.:\n",
        "                    SHELL = tf.reduce_mean(tf.math.squared_difference(\n",
        "                        tf.norm(generated, ord=2, axis=1),\n",
        "                        tf.ones_like(tf.norm(generated, ord=2, axis=1))*np.sqrt(params[\"z_size\"])))\n",
        "\n",
        "\n",
        "                gain =params[\"gen_div_coeff\"] * DIV + \\\n",
        "                      -params[\"gen_ent_coeff\"] * ENT + \\\n",
        "                       -params[\"gen_shell_coeff\"] * SHELL\n",
        "\n",
        "            grad = tape.gradient(gain, generated)\n",
        "            generated = (generated + grad)\n",
        "\n",
        "        if final_latent is None:\n",
        "            final_latent = tf.identity(generated)\n",
        "        else:\n",
        "            final_latent = tf.concat([final_latent, generated], axis=0)\n",
        "\n",
        "\n",
        "    tf.stop_gradient(final_latent)\n",
        "\n",
        "    mir_worked = not np.isnan(final_latent).any()\n",
        "    mem_x = final_latent if mir_worked else generate(agent, train=False)\n",
        "\n",
        "    return mem_x, mir_worked"
      ],
      "metadata": {
        "id": "5ho68Omg8kVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM8aEv6833SS"
      },
      "outputs": [],
      "source": [
        "'''Train the generator unit'''\n",
        "\n",
        "def train_generator(agent):\n",
        "\n",
        "    data = agent.state[\"data\"]\n",
        "    latent = agent.encoder(data)\n",
        "\n",
        "    mem_x = None\n",
        "    for it in range(agent.params[\"gen_iters\"]):\n",
        "        generate(agent, input_latent=latent)\n",
        "\n",
        "        if agent.state[\"task\"] > 0:\n",
        "            if it == 0 or not agent.params[\"reuse_samples\"]:\n",
        "                mem_x, mir_worked = retrieve_gen_for_gen(agent)\n",
        "\n",
        "                agent.state[\"mir_tries\"] += 1\n",
        "                if mir_worked:\n",
        "                    agent.state[\"mir_success\"] += 1\n",
        "\n",
        "        if mem_x is not None:\n",
        "          if len(mem_x.shape) == 3:\n",
        "            mem_x = tf.expand__dims(mem_x, axis=-1)\n",
        "          generate(agent, input_latent=mem_x, coeff=agent.params[\"mem_coeff\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXTbt9sT8gHE"
      },
      "outputs": [],
      "source": [
        "'''Train the encoder and the classifier unit'''\n",
        "\n",
        "def train_classifier(agent):\n",
        "\n",
        "    data = agent.state[\"data\"]\n",
        "    target = agent.state[\"target\"]\n",
        "    mem_x, mem_y = None, None\n",
        "\n",
        "    for it in range(agent.params[\"cls_iters\"]):\n",
        "        history = agent.encoder_classifier.fit(data, target, batch_size=agent.params[\"batch_size\"], epochs=1, verbose=0)\n",
        "        if agent.state[\"task\"] > 0:\n",
        "            if it == 0 or not agent.params[\"reuse_samples\"]:\n",
        "                mem_x, mem_y, mir_worked = retrieve_gen_for_cls(agent)\n",
        "                agent.state[\"mir_tries\"] += 1\n",
        "                if mir_worked:\n",
        "                    agent.state[\"mir_success\"] += 1\n",
        "\n",
        "            if mem_x is not None:\n",
        "                mem_history = agent.classifier.fit(mem_x, utils.get_one_hot_predictions(mem_y), batch_size=agent.params[\"batch_size\"], epochs=1, verbose=1)\n",
        "\n",
        "                agent.state[\"epoch_eval\"][\"retr_cls_loss\"].append(mem_history.history[\"loss\"][0])\n",
        "                agent.state[\"epoch_eval\"][\"retr_cls_accuracy\"].append(mem_history.history[\"accuracy\"][0])\n",
        "        agent.state[\"epoch_eval\"][\"cls_loss\"].append(history.history[\"loss\"][0])\n",
        "        agent.state[\"epoch_eval\"][\"cls_acc\"].append(history.history[\"accuracy\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A88lN-h9hdV"
      },
      "outputs": [],
      "source": [
        "'''Train model'''\n",
        "\n",
        "def train_model(train_function, agent):\n",
        "    agent.state[\"sample_amt\"] = 0\n",
        "    loader = agent.state[\"tr_loader\"]\n",
        "    for i, (data, target) in enumerate(loader):\n",
        "        #if agent.state[\"sample_amt\"] > agent.params[\"samples_per_task\"] > 0: break\n",
        "        agent.state[\"sample_amt\"] += data.shape[0]\n",
        "        agent.state[\"data\"] = data\n",
        "        agent.state[\"target\"] = target\n",
        "        agent.state[\"i_example\"] = i\n",
        "        train_function(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''Run an epoch'''\n",
        "\n",
        "def run_cls_epoch(agent):\n",
        "\n",
        "    print(f\"Running Task {agent.state['task']}, Epoch: {agent.state['epoch']} on Classifier\")\n",
        "\n",
        "    agent.state[\"epoch_eval\"][\"cls_loss\"] = []\n",
        "    agent.state[\"epoch_eval\"][\"cls_acc\"] = []\n",
        "    agent.state[\"epoch_eval\"][\"retr_cls_loss\"] = []\n",
        "    agent.state[\"epoch_eval\"][\"retr_cls_accuracy\"] = []\n",
        "\n",
        "    train_model(train_classifier, agent)\n",
        "\n",
        "    '''Evaluate the models in epoch'''\n",
        "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
        "        print(f\"    Classifier loss: {np.mean(agent.state['epoch_eval']['cls_loss'])}\"\n",
        "              f\"    Classifier accuracy: {np.mean(agent.state['epoch_eval']['cls_acc'])}\")"
      ],
      "metadata": {
        "id": "xmJWM9GCzym_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Run an epoch'''\n",
        "\n",
        "def run_gen_epoch(agent):\n",
        "  \n",
        "    print(f\"Running Task {agent.state['task']}, Epoch: {agent.state['epoch']} on Generator\")\n",
        "    \n",
        "    agent.state[\"epoch_eval\"][\"gen_loss\"] = []\n",
        "    agent.state[\"epoch_eval\"][\"retr_gen_loss\"] = []\n",
        "    agent.state[\"epoch_eval\"][\"retr_gen_accuracy\"] = []\n",
        "\n",
        "    train_model(train_generator, agent)\n",
        "\n",
        "    '''Evaluate the models in epoch'''\n",
        "    if agent.state[\"epoch\"] % agent.params[\"print_every\"] == 0:\n",
        "        print(f\"    Generator loss: {np.mean(agent.state['epoch_eval']['gen_loss'])}\")\n",
        "              #f\"    Loss on gen retrieve for cls: {np.mean(agent.state['epoch_eval']['retr_cls_loss'])}\"\n",
        "              #f\"    Loss on gen retrieve for gen: {np.mean(agent.state['epoch_eval']['retr_gen_loss'])}\")"
      ],
      "metadata": {
        "id": "K48EQq7VGuI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SyPNSep_K0t"
      },
      "outputs": [],
      "source": [
        "'''Run a task'''\n",
        "\n",
        "def run_task(agent):\n",
        "\n",
        "    agent.state[\"mir_tries\"], agent.state[\"mir_success\"] = 0, 0\n",
        "    agent.state[\"epoch_eval\"] = dict()\n",
        "\n",
        "    for epoch in range(agent.params[\"cls_epochs\"]):\n",
        "        agent.state[\"epoch\"] = epoch\n",
        "        run_cls_epoch(agent)\n",
        "\n",
        "    for epoch in range(agent.params[\"gen_epochs\"]):\n",
        "        agent.state[\"epoch\"] = epoch\n",
        "        run_gen_epoch(agent)\n",
        "\n",
        "    '''Evaluate forgetting'''\n",
        "    if (agent.state['task']) > 0:\n",
        "      print(\"Evaluate Task: \", agent.state[\"task\"])\n",
        "    for i in range(agent.state[\"task\"]):\n",
        "        task_loss = []\n",
        "        task_eval = []\n",
        "        for data, target in test_loader[i]:\n",
        "            logits = agent.encoder_classifier(data)\n",
        "            pred = np.argmax(logits, axis=1)\n",
        "            y = np.argmax(target, axis=1)\n",
        "            eval = agent.eval(y, pred)\n",
        "            task_eval.append(eval)\n",
        "            loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
        "            task_loss.append(np.mean(loss))\n",
        "        print(f\"    Task {agent.state['task']} forgetting on task {i} : \"\n",
        "              f\"        Loss: {np.mean(task_loss)}\"\n",
        "              f\"        ACC: {np.mean(task_eval)}\")\n",
        "\n",
        "    #print(\"MIR success rate: \", agent.state[\"mir_success\"] / agent.state[\"mir_tries\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhbA5MCFCi4r"
      },
      "outputs": [],
      "source": [
        "'''Run the experiment'''\n",
        "\n",
        "def run(agent):\n",
        "\n",
        "  agent.set_models(\n",
        "      _classifier=Classifier(),\n",
        "      _generator=Generator(img_height=32, img_width=32),\n",
        "      _encoder=Encoder(),\n",
        "  )\n",
        "\n",
        "  for task, (tr_loader, ts_loader) in enumerate(zip(train_loader, test_loader)):\n",
        "    agent.state[\"task\"] = task\n",
        "    agent.state[\"tr_loader\"] = tr_loader\n",
        "    agent.state[\"ts_loader\"] = ts_loader\n",
        "    run_task(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "AoBLwork8kVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlPHqKTaC41u",
        "outputId": "f8aad095-4b7b-4b89-b453-f155803bb049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator init\n",
            "Running Task 0, Epoch: 0 on Classifier\n",
            "    Classifier loss: 1.1025629296141155    Classifier accuracy: 0.6429527209976972\n",
            "Running Task 0, Epoch: 1 on Classifier\n",
            "    Classifier loss: 0.529264626361556    Classifier accuracy: 0.7715767167382321\n",
            "Running Task 0, Epoch: 2 on Classifier\n",
            "    Classifier loss: 0.46851174811185414    Classifier accuracy: 0.7981712757530859\n",
            "Running Task 0, Epoch: 3 on Classifier\n",
            "    Classifier loss: 0.42587326492293404    Classifier accuracy: 0.8232395739878638\n",
            "Running Task 0, Epoch: 4 on Classifier\n",
            "    Classifier loss: 0.3907396576162112    Classifier accuracy: 0.8342335244356576\n",
            "Running Task 0, Epoch: 0 on Generator\n"
          ]
        }
      ],
      "source": [
        "agent = Agent(params)\n",
        "for r in range(agent.params[\"n_runs\"]):\n",
        "  agent.state[\"run\"] = r\n",
        "  run(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Qqe-XGuwzynA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "''' Evaluation'''\n",
        "for task, loader in enumerate(test_loader):\n",
        "    print(\"Task: \", task)\n",
        "    LOSS = []\n",
        "    ACC = []\n",
        "    for data, target in loader:\n",
        "      logits = agent.classifier_model(data)\n",
        "      pred = np.argmax(logits, axis=1)\n",
        "      report = agent.eval(np.argmax(target, axis=1), pred)\n",
        "      loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
        "      #print(report)\n",
        "      ACC.append(report)\n",
        "      LOSS.append(loss)\n",
        "    print(\"Mean loss: \", np.mean(LOSS))\n",
        "    print(\"Mean accuracy: \", np.mean(ACC))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "-BguEZvfzynA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation, testing"
      ],
      "metadata": {
        "collapsed": false,
        "id": "UiiKIl618kVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(loader, first_n_tasks=None):\n",
        "    for task, tr_loader in enumerate(loader):\n",
        "        print(\"Task: \", task)\n",
        "        data, target = tr_loader.batch(124)\n",
        "        logits = agent.classifier_model(data)\n",
        "        pred = np.argmax(logits, axis=1)\n",
        "        report = agent.eval(np.argmax(target, axis=1), pred)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(target, logits)\n",
        "        print(report)\n",
        "        print(\"Mean loss: \", np.mean(loss))"
      ],
      "metadata": {
        "id": "r_K9lbNh8kVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I2ZLTm-JnX8"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluation on training set:\")\n",
        "evaluate(train_loader)\n",
        "print(\"Evaluation on test set:\")\n",
        "evaluate(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for development"
      ],
      "metadata": {
        "collapsed": false,
        "id": "vOxdRLeJ8kVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-eb6d272b8931>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reload modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_lib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier_lib' is not defined"
          ]
        }
      ],
      "source": [
        "# Reload modules\n",
        "import importlib\n",
        "importlib.reload(classifier_lib)"
      ],
      "metadata": {
        "id": "LNWS0pbd8kVn",
        "outputId": "e73bc099-3a1c-4477-bd08-bd85cc5b7623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "21547"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Garbage collection\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "NCdIwGMq8kVn",
        "outputId": "2495022b-fa19-489f-ae37-79b95d3ae24a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator"
      ],
      "metadata": {
        "collapsed": false,
        "id": "oUW-hMC5H5JC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "idUKrMRqH5JC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "def load_cifar_10():\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    n_classes = 10\n",
        "    X_train = (X_train / 127.5) -1\n",
        "    X_test = (X_test / 127.5) -1\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "metadata": {
        "id": "ac7_ze4nH5JD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_cifar_10()"
      ],
      "metadata": {
        "id": "MhGhd436H5JD",
        "outputId": "ab52c931-a936-46bd-be06-1582d8321494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ardgHNDSH5JD",
        "outputId": "beed63e0-8e73-43c0-f6d8-3253d0d930fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_f = '/drive/MyDrive/continual-learning-ait/checkpoints/32x32_classifier.pt'\n",
        "scorenet_f = '/drive/MyDrive/continual-learning-ait/checkpoints/edm-cifar10-32x32-cond-vp.pkl'"
      ],
      "metadata": {
        "id": "6FmW_At2I2hF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "import classifier_lib\n",
        "import pickle\n",
        "from train import train_generator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YvU86CThdnOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "source": [
        "encoder =       classifier_lib.load_encoder(encoder_f, 32, \"cuda\", eval=False)\n",
        "discriminator = classifier_lib.load_discriminator(None, \"cuda\", eval=False)\n",
        "\n",
        "#Load pretrained score network.\n",
        "#with open(scorenet_f, 'rb') as f:\n",
        "#    scorenet = pickle.load(f)['ema'].to(\"cpu\")"
      ],
      "metadata": {
        "id": "efIs-okfH5JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "collapsed": false,
        "id": "pxTMKPfAH5JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noise_batch(batch_size):\n",
        "    image_shape = (32, 32, 3)  # CIFAR-10 image shape\n",
        "\n",
        "    # Generate random noise between 0 and 255\n",
        "    noise_batch = np.random.randint(0, 256, size=(batch_size,) + image_shape, dtype=np.uint8)\n",
        "    \n",
        "    # Scale the pixel values to a range between 0 and 1\n",
        "    noise_batch = noise_batch /127.5 - 1\n",
        "    noise_batch = torch.tensor(noise_batch)\n",
        "    noise_batch = noise_batch.to(dtype=torch.float32)\n",
        "    return noise_batch"
      ],
      "metadata": {
        "id": "uQp3GAOaL-v5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "pTHNrRa8Nc4T"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train)\n",
        "X_train = X_train.to(device=\"cpu\", dtype=torch.float32) "
      ],
      "metadata": {
        "id": "h6MubtmWNIAU",
        "outputId": "a937439e-d199-40a1-9211-d90eda4c450a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-19aed427a4c1>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train = torch.tensor(X_train)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-th epoch BCE loss: 0.2153909057378769, correction rate: 0.90234375\n",
            "0-th epoch BCE loss: 0.22524017095565796, correction rate: 0.87890625\n",
            "0-th epoch BCE loss: 0.22636927167574564, correction rate: 0.8893229166666666\n",
            "0-th epoch BCE loss: 0.2209838517010212, correction rate: 0.888671875\n",
            "0-th epoch BCE loss: 0.22342173755168915, correction rate: 0.8875\n",
            "0-th epoch BCE loss: 0.2254129226009051, correction rate: 0.8834635416666666\n",
            "0-th epoch BCE loss: 0.22814626566001348, correction rate: 0.8816964285714286\n",
            "0-th epoch BCE loss: 0.22854527458548546, correction rate: 0.8818359375\n",
            "1-th epoch BCE loss: 0.21129852533340454, correction rate: 0.8828125\n",
            "1-th epoch BCE loss: 0.21098896861076355, correction rate: 0.8828125\n",
            "1-th epoch BCE loss: 0.2056670089562734, correction rate: 0.8854166666666666\n",
            "1-th epoch BCE loss: 0.20066765695810318, correction rate: 0.8896484375\n",
            "1-th epoch BCE loss: 0.20360890924930572, correction rate: 0.88828125\n",
            "1-th epoch BCE loss: 0.20492772509654364, correction rate: 0.8880208333333334\n",
            "1-th epoch BCE loss: 0.20108993990080698, correction rate: 0.8934151785714286\n",
            "1-th epoch BCE loss: 0.1955429892987013, correction rate: 0.89599609375\n",
            "2-th epoch BCE loss: 0.20095700025558472, correction rate: 0.87890625\n",
            "2-th epoch BCE loss: 0.19104713201522827, correction rate: 0.880859375\n",
            "2-th epoch BCE loss: 0.18387866020202637, correction rate: 0.8841145833333334\n",
            "2-th epoch BCE loss: 0.18755332753062248, correction rate: 0.8828125\n",
            "2-th epoch BCE loss: 0.18363822996616364, correction rate: 0.8890625\n",
            "2-th epoch BCE loss: 0.1807797650496165, correction rate: 0.8899739583333334\n",
            "2-th epoch BCE loss: 0.17859993236405508, correction rate: 0.8939732142857143\n",
            "2-th epoch BCE loss: 0.1802792977541685, correction rate: 0.89404296875\n",
            "3-th epoch BCE loss: 0.1857517957687378, correction rate: 0.8984375\n",
            "3-th epoch BCE loss: 0.18560396879911423, correction rate: 0.884765625\n",
            "3-th epoch BCE loss: 0.18198485175768533, correction rate: 0.8893229166666666\n",
            "3-th epoch BCE loss: 0.18351347744464874, correction rate: 0.8876953125\n",
            "3-th epoch BCE loss: 0.17926794290542603, correction rate: 0.890625\n",
            "3-th epoch BCE loss: 0.18492287894090018, correction rate: 0.890625\n",
            "3-th epoch BCE loss: 0.18557537027767726, correction rate: 0.8895089285714286\n",
            "3-th epoch BCE loss: 0.17986813932657242, correction rate: 0.8935546875\n",
            "4-th epoch BCE loss: 0.1658765971660614, correction rate: 0.91015625\n",
            "4-th epoch BCE loss: 0.15674348175525665, correction rate: 0.919921875\n",
            "4-th epoch BCE loss: 0.17740923166275024, correction rate: 0.9114583333333334\n",
            "4-th epoch BCE loss: 0.17824024707078934, correction rate: 0.9091796875\n",
            "4-th epoch BCE loss: 0.17799479961395265, correction rate: 0.90859375\n",
            "4-th epoch BCE loss: 0.18252385159333548, correction rate: 0.8977864583333334\n",
            "4-th epoch BCE loss: 0.18392435567719595, correction rate: 0.8989955357142857\n",
            "4-th epoch BCE loss: 0.1805471759289503, correction rate: 0.90185546875\n",
            "5-th epoch BCE loss: 0.1769861876964569, correction rate: 0.890625\n",
            "5-th epoch BCE loss: 0.17719992995262146, correction rate: 0.896484375\n",
            "5-th epoch BCE loss: 0.17209844291210175, correction rate: 0.8919270833333334\n",
            "5-th epoch BCE loss: 0.16649788990616798, correction rate: 0.896484375\n",
            "5-th epoch BCE loss: 0.16701934039592742, correction rate: 0.89765625\n",
            "5-th epoch BCE loss: 0.16820622235536575, correction rate: 0.8958333333333334\n",
            "5-th epoch BCE loss: 0.1646634191274643, correction rate: 0.8967633928571429\n",
            "5-th epoch BCE loss: 0.16549227572977543, correction rate: 0.89697265625\n",
            "6-th epoch BCE loss: 0.20999732613563538, correction rate: 0.87890625\n",
            "6-th epoch BCE loss: 0.17144906520843506, correction rate: 0.904296875\n",
            "6-th epoch BCE loss: 0.16245048741499582, correction rate: 0.9010416666666666\n",
            "6-th epoch BCE loss: 0.15711116418242455, correction rate: 0.90625\n",
            "6-th epoch BCE loss: 0.15478647351264954, correction rate: 0.903125\n",
            "6-th epoch BCE loss: 0.15692370384931564, correction rate: 0.90234375\n",
            "6-th epoch BCE loss: 0.15764963839735305, correction rate: 0.90234375\n",
            "6-th epoch BCE loss: 0.15705144591629505, correction rate: 0.904296875\n",
            "7-th epoch BCE loss: 0.18929050862789154, correction rate: 0.890625\n",
            "7-th epoch BCE loss: 0.17581089586019516, correction rate: 0.90625\n",
            "7-th epoch BCE loss: 0.17154361307621002, correction rate: 0.9036458333333334\n",
            "7-th epoch BCE loss: 0.16406182572245598, correction rate: 0.9052734375\n",
            "7-th epoch BCE loss: 0.16621561646461486, correction rate: 0.90234375\n",
            "7-th epoch BCE loss: 0.161202701429526, correction rate: 0.9055989583333334\n",
            "7-th epoch BCE loss: 0.1608953412090029, correction rate: 0.90234375\n",
            "7-th epoch BCE loss: 0.1605935599654913, correction rate: 0.9033203125\n",
            "8-th epoch BCE loss: 0.16715046763420105, correction rate: 0.8828125\n",
            "8-th epoch BCE loss: 0.16161944717168808, correction rate: 0.89453125\n",
            "8-th epoch BCE loss: 0.16843628386656442, correction rate: 0.88671875\n",
            "8-th epoch BCE loss: 0.16978711262345314, correction rate: 0.890625\n",
            "8-th epoch BCE loss: 0.1694062441587448, correction rate: 0.8921875\n",
            "8-th epoch BCE loss: 0.1669993201891581, correction rate: 0.892578125\n",
            "8-th epoch BCE loss: 0.17122149893215724, correction rate: 0.8883928571428571\n",
            "8-th epoch BCE loss: 0.17134780436754227, correction rate: 0.890625\n",
            "9-th epoch BCE loss: 0.166065514087677, correction rate: 0.890625\n",
            "9-th epoch BCE loss: 0.1667567938566208, correction rate: 0.896484375\n",
            "9-th epoch BCE loss: 0.1485205963253975, correction rate: 0.9075520833333334\n",
            "9-th epoch BCE loss: 0.15116873942315578, correction rate: 0.9033203125\n",
            "9-th epoch BCE loss: 0.15198191553354262, correction rate: 0.90390625\n",
            "9-th epoch BCE loss: 0.15265142048398653, correction rate: 0.904296875\n",
            "9-th epoch BCE loss: 0.1565370953508786, correction rate: 0.9017857142857143\n",
            "9-th epoch BCE loss: 0.15687322337180376, correction rate: 0.90234375\n"
          ]
        }
      ],
      "source": [
        "train_generator(X_train[:1024], y_train[:1024], generate_noise_batch(1024), y_train[1024:2048], encoder, discriminator)"
      ],
      "metadata": {
        "id": "rrE5BKoZH5JD",
        "outputId": "3d747078-6b29-45ec-a94c-401f624577b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "sjgStI_5H5JD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}